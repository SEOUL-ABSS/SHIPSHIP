{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "SHIPDETECTOR_V1.ipynb",
      "authorship_tag": "ABX9TyNnxZ4DVMgcDiFz6KJcEbxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SHIPDETECTOR_V2_BYOL-A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#         DeepShip + MBARI (Streaming VAD) â€” Ship vs Noise Binary Classifier\n",
        "#     (RAM-safe MBARI ingestion, Reservoir Sampling, Full Audit & Visualization)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"1) í™˜ê²½ì„¤ì •/ì„¤ì¹˜ ì¤‘ ...\")\n",
        "!pip -q install tensorflow tensorflow_hub soundfile librosa boto3 noisereduce umap-learn psutil\n",
        "\n",
        "# -------------------------- Imports & Setup -----------------------------------\n",
        "import os, sys, subprocess, random, math, gc, time, warnings, shutil\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa, librosa.display\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
        "import umap.umap_ as umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Fonts (optional, for Korean labels)\n",
        "!sudo apt-get -y install fonts-nanum > /dev/null\n",
        "!sudo fc-cache -fv > /dev/null\n",
        "import matplotlib.font_manager as fm\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "if os.path.exists(font_path):\n",
        "    fm.fontManager.addfont(font_path)\n",
        "    plt.rc('font', family='NanumGothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# -------------------------- Paths & Consts ------------------------------------\n",
        "YAMNET_SAMPLE_RATE = 16000\n",
        "BASE = \"/content\"\n",
        "DEEPSHIP_BASE_PATH = f\"{BASE}/DeepShip\"\n",
        "MBARI_BASE_DIR     = f\"{BASE}/MBARI_noise_data\"\n",
        "REPORT_DIR         = f\"{BASE}/reports\"\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------- Small Utils ---------------------------------------\n",
        "class Timer:\n",
        "    def __init__(self, name): self.name=name\n",
        "    def __enter__(self): self.t=time.time(); return self\n",
        "    def __exit__(self, *a):\n",
        "        rss = psutil.Process().memory_info().rss/1024**3\n",
        "        print(f\"[TIMER] {self.name}: {time.time()-self.t:.2f}s | RSSâ‰ˆ{rss:.2f} GB\")\n",
        "\n",
        "def mem():\n",
        "    return f\"RSSâ‰ˆ{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2) ë°ì´í„° í™•ë³´ (DeepShip + MBARI ì¼ë¶€)\n",
        "# ==============================================================================\n",
        "print(\"\\n2) ë°ì´í„° í™•ë³´ ...\")\n",
        "with Timer(\"DeepShip clone\"):\n",
        "    if not os.path.exists(DEEPSHIP_BASE_PATH):\n",
        "        subprocess.run(['git','clone','--depth','1','https://github.com/irfankamboh/DeepShip.git', DEEPSHIP_BASE_PATH],\n",
        "                       check=True, capture_output=True)\n",
        "        print(\" - DeepShip OK\")\n",
        "    else: print(\" - DeepShip ì´ë¯¸ ì¡´ì¬\")\n",
        "\n",
        "with Timer(\"MBARI fetch (ìµœëŒ€ 10ê°œ)\"):\n",
        "    os.makedirs(MBARI_BASE_DIR, exist_ok=True)\n",
        "    if not os.listdir(MBARI_BASE_DIR):\n",
        "        s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "        pages = s3.get_paginator('list_objects_v2').paginate(Bucket='pacific-sound-16khz', Prefix='2018/01/')\n",
        "        dl_count, MAX_DL = 0, 10\n",
        "        for page in pages:\n",
        "            for obj in page.get('Contents', []):\n",
        "                if obj['Key'].endswith('.wav') and obj.get('Size',0) > 0:\n",
        "                    local = os.path.join(MBARI_BASE_DIR, os.path.basename(obj['Key']))\n",
        "                    if not os.path.exists(local):\n",
        "                        s3.download_file('pacific-sound-16khz', obj['Key'], local); dl_count+=1\n",
        "                if dl_count >= MAX_DL: break\n",
        "            if dl_count >= MAX_DL: break\n",
        "        print(f\" - MBARI OK ({dl_count}ê°œ)\")\n",
        "    else:\n",
        "        print(f\" - MBARI OK (ì´ë¯¸ {len(os.listdir(MBARI_BASE_DIR))}ê°œ ì¡´ì¬)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3) ìŠ¤íŠ¸ë¦¬ë° VAD + ì €ìˆ˜ì§€ í‘œë³¸ì¶”ì¶œ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (RAM-safe)\n",
        "# ==============================================================================\n",
        "EPS = 1e-12\n",
        "\n",
        "def get_activity_intervals_streaming(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    \"\"\"\n",
        "    librosa.load ì—†ì´ 2-pass ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í™œì„±/ë¹„í™œì„±(ì´ˆ ë‹¨ìœ„)ì„ êµ¬í•¨.\n",
        "    pass1: block RMS dB max, pass2: ì„ê³„ì¹˜ ì´ìƒ ë¸”ë¡ ë³‘í•©.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr = f.samplerate; n = len(f)\n",
        "            F = max(1, int(round(frame_sec*sr))); H = max(1, int(round(hop_sec*sr)))\n",
        "\n",
        "            # pass1\n",
        "            max_db = -np.inf; pos = 0\n",
        "            while pos + F <= n:\n",
        "                f.seek(pos); y = f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms = float(np.sqrt(np.mean(y**2)) + EPS)\n",
        "                db  = 20*np.log10(rms+EPS)\n",
        "                if db > max_db: max_db = db\n",
        "                pos += H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "\n",
        "            thresh = max_db - top_db\n",
        "\n",
        "            # pass2\n",
        "            active=[]; in_active=False; cur_start=0.0\n",
        "            pos=0\n",
        "            while pos + F <= n:\n",
        "                f.seek(pos); y = f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms = float(np.sqrt(np.mean(y**2)) + EPS); db = 20*np.log10(rms+EPS)\n",
        "                t0 = pos/sr; t1=(pos+F)/sr\n",
        "                if db >= thresh:\n",
        "                    if not in_active: in_active=True; cur_start=t0\n",
        "                else:\n",
        "                    if in_active: in_active=False; active.append((cur_start, t1))\n",
        "                pos+=H\n",
        "            if in_active: active.append((cur_start, n/sr))\n",
        "\n",
        "            # inactive\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s))\n",
        "                last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except Exception:\n",
        "        return [], []\n",
        "\n",
        "def _reservoir_segments_from_spans(spans, file_path, sr_orig, seg_dur, hop, cap):\n",
        "    \"\"\" spansë¥¼ seg_durë¡œ ìª¼ê°œë˜, capê°œë§Œ ì €ìˆ˜ì§€ í‘œë³¸ì¶”ì¶œë¡œ ìœ ì§€ (O(cap)) \"\"\"\n",
        "    res=[]; seen=0\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st = s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            seg = (file_path, float(st), sr_orig)\n",
        "            seen += 1\n",
        "            if len(res) < cap:\n",
        "                res.append(seg)\n",
        "            else:\n",
        "                j = random.randint(1, seen)\n",
        "                if j <= cap: res[j-1] = seg\n",
        "            st += hop\n",
        "    return res\n",
        "\n",
        "def create_dataset_deepship_plus_mbari(\n",
        "    deepship_root, mbari_root,\n",
        "    segment_duration=5.0,\n",
        "    deepship_overlap=0.2,  # DeepShipì€ ë‹¤ì–‘ì„± ìœ„í•´ ì•½ê°„ì˜ ê²¹ì¹¨ í—ˆìš©\n",
        "    mbari_overlap=0.0,     # MBARIëŠ” ì¤‘ë³µ ì¤„ì´ê¸° ìœ„í•´ 0~0.1 ì¶”ì²œ\n",
        "    mbari_cap_active_per_file=120,\n",
        "    mbari_cap_inactive_per_file=120,\n",
        "    vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "):\n",
        "    ship_segments=[]; noise_segments=[]\n",
        "    # DeepShip\n",
        "    print(\"\\n[ì„¸ê·¸ ìƒì„±] DeepShip ...\")\n",
        "    hop_ds = segment_duration*(1-deepship_overlap)\n",
        "    ds_files=ds_ship=ds_noise=0\n",
        "    for root,_,files in os.walk(deepship_root):\n",
        "        for fn in sorted([f for f in files if f.lower().endswith('.wav')]):\n",
        "            fp = os.path.join(root, fn)\n",
        "            try:\n",
        "                info = sf.info(fp); ds_files += 1\n",
        "            except: continue\n",
        "            act, inact = get_activity_intervals_streaming(fp, top_db=vad_top_db, frame_sec=vad_frame_sec, hop_sec=vad_hop_sec)\n",
        "            for s,e in act:\n",
        "                st=s\n",
        "                while st <= e - segment_duration + 1e-9:\n",
        "                    ship_segments.append((fp, float(st), info.samplerate)); ds_ship+=1\n",
        "                    st += hop_ds\n",
        "            for s,e in inact:\n",
        "                st=s\n",
        "                while st <= e - segment_duration + 1e-9:\n",
        "                    noise_segments.append((fp, float(st), info.samplerate)); ds_noise+=1\n",
        "                    st += hop_ds\n",
        "            gc.collect()\n",
        "    print(f\" - DeepShip íŒŒì¼:{ds_files} | ship:{ds_ship} | noise:{ds_noise} | {mem()}\")\n",
        "\n",
        "    # MBARI â†’ hard-negatives\n",
        "    print(\"[ì„¸ê·¸ ìƒì„±] MBARI (hard negatives, cap ì ìš©) ...\")\n",
        "    hop_mb = segment_duration*(1-mbari_overlap)\n",
        "    mb_files=mb_noise_added=0\n",
        "    for fn in sorted([f for f in os.listdir(mbari_root) if f.lower().endswith('.wav')]):\n",
        "        fp = os.path.join(mbari_root, fn)\n",
        "        try: info = sf.info(fp); mb_files += 1\n",
        "        except: continue\n",
        "        act, inact = get_activity_intervals_streaming(fp, top_db=vad_top_db, frame_sec=vad_frame_sec, hop_sec=vad_hop_sec)\n",
        "        active_segs   = _reservoir_segments_from_spans(act,   fp, info.samplerate, segment_duration, hop_mb, mbari_cap_active_per_file)\n",
        "        inactive_segs = _reservoir_segments_from_spans(inact, fp, info.samplerate, segment_duration, hop_mb, mbari_cap_inactive_per_file)\n",
        "        noise_segments.extend(active_segs); noise_segments.extend(inactive_segs)\n",
        "        mb_noise_added += (len(active_segs)+len(inactive_segs))\n",
        "        gc.collect()\n",
        "    print(f\" - MBARI íŒŒì¼:{mb_files} | noise ì¶”ê°€:{mb_noise_added} | {mem()}\")\n",
        "\n",
        "    infos  = ship_segments + noise_segments\n",
        "    labels = (['ship']*len(ship_segments)) + (['noise']*len(noise_segments))\n",
        "    return infos, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 4) ì˜¤ë””ì˜¤ ë¡œë”©(ì„¸ê·¸ë¨¼íŠ¸) & ì„ë² ë”© & ëª¨ë¸\n",
        "# ==============================================================================\n",
        "def load_and_process_segment(file_info, duration, target_sr, rms_norm=True):\n",
        "    \"\"\" íŒŒì¼ì—ì„œ í•´ë‹¹ êµ¬ê°„ë§Œ ì½ì–´ ë¦¬ìƒ˜í”Œ & (ì„ íƒ)RMS ì •ê·œí™” â€” ë©”ëª¨ë¦¬ ì¹œí™”ì  \"\"\"\n",
        "    file_path, start_time, orig_sr = file_info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_yamnet_embedding(info, yamnet_model, seg_dur):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        _, emb, _ = yamnet_model(y)\n",
        "        if emb.shape[0]==0: return None\n",
        "        return tf.reduce_mean(emb, axis=0).numpy()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def embed_infos(infos, yamnet_model, seg_dur, max_items=None, show_every=1000):\n",
        "    \"\"\" infos â†’ (N,1024) ì„ë² ë”©. max_itemsë¡œ ìƒí•œ ì„¤ì • ê°€ëŠ¥ \"\"\"\n",
        "    X = []; kept_indices = []\n",
        "    for i,info in enumerate(infos):\n",
        "        if (max_items is not None) and (len(X)>=max_items): break\n",
        "        e = extract_yamnet_embedding(info, yamnet_model, seg_dur)\n",
        "        if e is not None:\n",
        "            X.append(e); kept_indices.append(i)\n",
        "        if (i+1)%show_every==0:\n",
        "            print(f\"  ì„ë² ë”© {i+1}/{len(infos)} ... {mem()}\")\n",
        "    return np.asarray(X, dtype=np.float32), kept_indices\n",
        "\n",
        "def load_yamnet():\n",
        "    print(\"\\nYAMNet ë¡œë“œ ...\", end=\"\")\n",
        "    m = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
        "    print(\" OK\")\n",
        "    return m\n",
        "\n",
        "def build_classifier(input_dim, num_classes=2, lr=5e-4):\n",
        "    inp = tf.keras.Input(shape=(input_dim,), name=\"emb\")\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(inp)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ==============================================================================\n",
        "# 5) ë°ì´í„°ì…‹ ìƒì„± + ê°ì‚¬(Audit) + ì‹œê°í™”\n",
        "# ==============================================================================\n",
        "CONFIG = {\n",
        "    \"segment_duration\": 5.0,\n",
        "    \"deepship_overlap\": 0.2,\n",
        "    \"mbari_overlap\": 0.0,\n",
        "    \"mbari_cap_active_per_file\": 120,\n",
        "    \"mbari_cap_inactive_per_file\": 120,\n",
        "    \"vad_frame_sec\": 0.5, \"vad_hop_sec\": 0.25, \"vad_top_db\": 25.0,\n",
        "    \"test_size\": 0.2, \"epochs\": 40, \"batch_size\": 32, \"learning_rate\": 5e-4,\n",
        "}\n",
        "\n",
        "with Timer(\"ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±(Streaming)\"):\n",
        "    infos, labels = create_dataset_deepship_plus_mbari(\n",
        "        DEEPSHIP_BASE_PATH, MBARI_BASE_DIR,\n",
        "        segment_duration=CONFIG[\"segment_duration\"],\n",
        "        deepship_overlap=CONFIG[\"deepship_overlap\"],\n",
        "        mbari_overlap=CONFIG[\"mbari_overlap\"],\n",
        "        mbari_cap_active_per_file=CONFIG[\"mbari_cap_active_per_file\"],\n",
        "        mbari_cap_inactive_per_file=CONFIG[\"mbari_cap_inactive_per_file\"],\n",
        "        vad_frame_sec=CONFIG[\"vad_frame_sec\"], vad_hop_sec=CONFIG[\"vad_hop_sec\"], vad_top_db=CONFIG[\"vad_top_db\"],\n",
        "    )\n",
        "\n",
        "# ---- Audit: íŒŒì¼ ìˆ˜/ì´ ê¸¸ì´/í´ë˜ìŠ¤ ë¶„í¬(ì„¸ê·¸ë¨¼íŠ¸ ê¸°ì¤€) ----\n",
        "def audit_dataset(infos, labels):\n",
        "    print(\"\\n[DATASET ê°ì‚¬]\")\n",
        "    cnt = Counter(labels)\n",
        "    n_files = len(set([i[0] for i in infos]))\n",
        "    total_dur_h = 0.0\n",
        "    # ëŒ€ëµì  ì´ ê¸¸ì´(ì¤‘ë³µ í¬í•¨) : ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ * seg_dur\n",
        "    total_dur_h = (len(infos)*CONFIG[\"segment_duration\"])/3600.0\n",
        "    print(f\" - ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(infos)} (filesâ‰ˆ{n_files})\")\n",
        "    for k,v in cnt.items(): print(f\"   Â· {k}: {v}\")\n",
        "    print(f\" - (ì¤‘ë³µ í¬í•¨) ì„¸ê·¸ë¨¼íŠ¸ ì´ ê¸¸ì´â‰ˆ {total_dur_h:.2f} h\")\n",
        "\n",
        "audit_dataset(infos, labels)\n",
        "\n",
        "# ---- Class ë¶„í¬ ì‹œê°í™” ----\n",
        "def plot_class_distribution(labels, title=\"ì„¸ê·¸ë¨¼íŠ¸ í´ë˜ìŠ¤ ë¶„í¬\"):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    vc = pd.Series(labels).value_counts().sort_index()\n",
        "    sns.barplot(x=vc.index, y=vc.values)\n",
        "    plt.title(title); plt.ylabel(\"count\"); plt.grid(axis='y', alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution(labels)\n",
        "\n",
        "# ---- ìƒ˜í”Œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í™•ì¸ ----\n",
        "def show_sample_spectrograms(infos, labels, n_per_class=3):\n",
        "    classes = sorted(set(labels))\n",
        "    plt.figure(figsize=(5*n_per_class, 4*len(classes)))\n",
        "    idx=1\n",
        "    for c in classes:\n",
        "        idxs = [i for i,l in enumerate(labels) if l==c]\n",
        "        random.shuffle(idxs)\n",
        "        for j in idxs[:n_per_class]:\n",
        "            file_path, start_time, sr = infos[j]\n",
        "            y = load_and_process_segment(infos[j], CONFIG[\"segment_duration\"], YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "            plt.subplot(len(classes), n_per_class, idx)\n",
        "            if y is None:\n",
        "                plt.title(f\"{c}: load fail\"); idx+=1; continue\n",
        "            D = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, hop_length=320)), ref=np.max)\n",
        "            librosa.display.specshow(D, sr=YAMNET_SAMPLE_RATE, x_axis='time', y_axis='log', cmap='magma')\n",
        "            plt.title(f\"{c} | {os.path.basename(file_path)} @ {start_time:.1f}s\")\n",
        "            idx+=1\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "show_sample_spectrograms(infos, labels, n_per_class=3)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6) í•™ìŠµ/ê²€ì¦ ë¶„í•  (íŒŒì¼ ê¸°ì¤€ Group split â†’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "# ==============================================================================\n",
        "le = LabelEncoder(); y_enc = le.fit_transform(labels)\n",
        "groups = np.array([i[0] for i in infos])\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG[\"test_size\"], random_state=SEED)\n",
        "tr_idx, te_idx = next(gss.split(infos, y_enc, groups))\n",
        "\n",
        "Xtr_info = [infos[i] for i in tr_idx]; ytr_enc = y_enc[tr_idx]\n",
        "Xte_info = [infos[i] for i in te_idx]; yte_enc = y_enc[te_idx]\n",
        "\n",
        "print(f\"\\n[Split] train={len(Xtr_info)} | test={len(Xte_info)} (files train/test = {len(set([i[0] for i in Xtr_info]))}/{len(set([i[0] for i in Xte_info]))})\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7) YAMNet ì„ë² ë”© â†’ ë¶„ë¥˜ê¸° í•™ìŠµ/í‰ê°€ + ì‹œê°í™” + UMAP\n",
        "# ==============================================================================\n",
        "yamnet = load_yamnet()\n",
        "\n",
        "with Timer(\"ì„ë² ë”©(Train)\"):\n",
        "    Xtr, tr_kept_indices = embed_infos(Xtr_info, yamnet, CONFIG[\"segment_duration\"])\n",
        "    ytr_enc_filtered = ytr_enc[tr_kept_indices]\n",
        "    ytr = tf.keras.utils.to_categorical(ytr_enc_filtered, num_classes=len(le.classes_))\n",
        "    print(f\" - Xtr:{Xtr.shape} | {mem()}\")\n",
        "\n",
        "with Timer(\"ì„ë² ë”©(Test)\"):\n",
        "    Xte, te_kept_indices = embed_infos(Xte_info, yamnet, CONFIG[\"segment_duration\"])\n",
        "    yte_enc_filtered = yte_enc[te_kept_indices]\n",
        "    yte = tf.keras.utils.to_categorical(yte_enc_filtered, num_classes=len(le.classes_))\n",
        "    print(f\" - Xte:{Xte.shape} | {mem()}\")\n",
        "\n",
        "# ---- ì„ë² ë”© ë¶„í¬ quick check ----\n",
        "print(f\"[ì„ë² ë”© norm] Train mean={Xtr.mean():.3f} | std={Xtr.std():.3f}\")\n",
        "print(f\"[ì„ë² ë”© norm] Test  mean={Xte.mean():.3f} | std={Xte.std():.3f}\")\n",
        "\n",
        "# ---- Class weight (ì–¸ë”ìƒ˜í”Œë§ ëŒ€ì‹ ) ----\n",
        "cnt_tr = Counter(ytr_enc_filtered); total = sum(cnt_tr.values())\n",
        "class_weight = {cls: total/(len(cnt_tr)*cnt) for cls,cnt in cnt_tr.items()}\n",
        "print(\"[class_weight]\", class_weight, \" (0:noise, 1:ship ìˆœì„œì¼ ê°€ëŠ¥ì„±)\")\n",
        "\n",
        "# ---- ëª¨ë¸ ----\n",
        "clf = build_classifier(Xtr.shape[-1], num_classes=len(le.classes_), lr=CONFIG[\"learning_rate\"])\n",
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)\n",
        "]\n",
        "with Timer(\"ëª¨ë¸ í•™ìŠµ\"):\n",
        "    hist = clf.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=CONFIG[\"epochs\"], batch_size=CONFIG[\"batch_size\"],\n",
        "                   class_weight=class_weight, verbose=1, callbacks=cb)\n",
        "\n",
        "# ---- í•™ìŠµ ê³¡ì„  ----\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1); plt.plot(hist.history['accuracy'], label='train'); plt.plot(hist.history['val_accuracy'], label='val'); plt.legend()\n",
        "plt.title('ì •í™•ë„'); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1,2,2); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend()\n",
        "plt.title('ì†ì‹¤'); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# ---- í‰ê°€ ì§€í‘œ ----\n",
        "probs = clf.predict(Xte, verbose=0); preds = probs.argmax(axis=1); true = yte.argmax(axis=1)\n",
        "acc = (preds==true).mean()\n",
        "f1m = f1_score(true, preds, average='macro')\n",
        "try:\n",
        "    ship_idx = list(le.classes_).index('ship')\n",
        "    auc_ship = roc_auc_score((true==ship_idx).astype(int), probs[:, ship_idx]) if len(np.unique(true))>1 else float('nan')\n",
        "except: auc_ship = float('nan')\n",
        "print(f\"\\n[TEST] Acc={acc:.4f} | Macro-F1={f1m:.4f} | AUC(ship)={auc_ship:.4f}\")\n",
        "print(\"\\n[ë¶„ë¥˜ ë¦¬í¬íŠ¸]\\n\", classification_report(true, preds, target_names=le.classes_))\n",
        "\n",
        "# ---- í˜¼ë™í–‰ë ¬ ----\n",
        "cm = confusion_matrix(true, preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('ì˜ˆì¸¡'); plt.ylabel('ì‹¤ì œ'); plt.title('í˜¼ë™ í–‰ë ¬'); plt.show()\n",
        "\n",
        "# ---- ROC ----\n",
        "if len(np.unique(true))==2:\n",
        "    fpr, tpr, _ = roc_curve((true==ship_idx).astype(int), probs[:, ship_idx])\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "    plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (ship)'); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# ---- UMAP (ìƒ˜í”Œ ì œí•œ; ë©”ëª¨ë¦¬ ì•ˆì „) ----\n",
        "def show_umap(X, y, classes, title=\"UMAP (YAMNet Embeddings)\", max_points=1500):\n",
        "    if len(X) > max_points:\n",
        "        idx = np.random.RandomState(SEED).choice(len(X), size=max_points, replace=False)\n",
        "        Xs, ys = X[idx], y[idx]\n",
        "    else:\n",
        "        Xs, ys = X, y\n",
        "    if len(Xs) < 10:\n",
        "        print(\"UMAP: í‘œë³¸ì´ ë„ˆë¬´ ì ì–´ ìƒëµ\"); return\n",
        "    reducer = umap.UMAP(n_neighbors=min(15, len(Xs)-1), min_dist=0.1, n_components=2, random_state=SEED)\n",
        "    XY = reducer.fit_transform(Xs)\n",
        "    df = pd.DataFrame(dict(x=XY[:,0], y=XY[:,1], label=[classes[i] for i in ys]))\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sns.scatterplot(data=df, x='x', y='y', hue='label', s=20, alpha=0.7)\n",
        "    plt.title(title); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "show_umap(np.vstack([Xtr,Xte]), np.hstack([ytr.argmax(1), yte.argmax(1)]), le.classes_, title=\"UMAP (Train+Test)\")\n",
        "\n",
        "print(\"\\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f4a57-f5bb-4696-dc30-151061ba8e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) í™˜ê²½ì„¤ì •/ì„¤ì¹˜ ì¤‘ ...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hdebconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "2) ë°ì´í„° í™•ë³´ ...\n",
            " - DeepShip OK\n",
            "[TIMER] DeepShip clone: 80.42s | RSSâ‰ˆ1.46 GB\n",
            " - MBARI OK (10ê°œ)\n",
            "[TIMER] MBARI fetch (ìµœëŒ€ 10ê°œ): 316.13s | RSSâ‰ˆ1.47 GB\n",
            "\n",
            "[ì„¸ê·¸ ìƒì„±] DeepShip ...\n",
            " - DeepShip íŒŒì¼:63 | ship:1390 | noise:0 | RSSâ‰ˆ1.47 GB\n",
            "[ì„¸ê·¸ ìƒì„±] MBARI (hard negatives, cap ì ìš©) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD í‰ê°€ ëª¨ë“ˆ =================================\n",
        "# ì´ ë¸”ë¡ì€ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì—ì„œ í•™ìŠµì´ ëë‚œ í›„ì— ë¶™ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "# í•„ìš” ì „ì—­: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(í•™ìŠµëœ ë¶„ë¥˜ê¸°), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE ê²½ë¡œ\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) Gitì—ì„œ OOD ìƒ˜í”Œ ì˜¤ë””ì˜¤ ê°€ë³ê²Œ ìˆ˜ì§‘ ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # ì†Œí˜• ì˜ˆì œ/í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ê°€ ë¹„êµì  ë“¤ì–´ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] ë¦¬í¬ì§€í† ë¦¬ ìˆ˜ì§‘ ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# ì˜¤ë””ì˜¤ í™•ì¥ì íŒ¨í„´(ë„“ê²Œ ì¡ë˜ ê°œìˆ˜ ì œí•œ)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # ë„ˆë¬´ ë§ì€ ê²½ìš° ìƒ˜í”Œë§\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - ìˆ˜ì§‘ëœ OOD ì›ë³¸ íŒŒì¼: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD ì„¸ê·¸ë¨¼íŠ¸(5ì´ˆ) ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load ì—†ì´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ 5ì´ˆ êµ¬ê°„ì„ ê· ì¼ ìŠ¤íŠ¸ë¼ì´ë“œë¡œ ìµœëŒ€ capë§Œ ì¶”ì¶œ\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # ê· ì¼ ìŠ¤íŠ¸ë¼ì´ë“œë¡œ ì‹œì‘ì  í›„ë³´ ìƒì„±\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# ë„ˆë¬´ ë§ì´ ë½‘ì§€ ì•Šë„ë¡ ì „ì²´ cap (ì˜ˆ: 800 ì„¸ê·¸ë¨¼íŠ¸)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - ìƒì„±ëœ OOD ì„¸ê·¸ë¨¼íŠ¸: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD ì„ë² ë”© ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(ì •ê·œí™” ì „ì—)ë„ ì €ì¥í•´ ì—ë„ˆì§€ í¸í–¥ ë¶„ì„\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD ì„ë² ë”© {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] ì„ë² ë”© ì¶”ì¶œ ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"ê²½ê³ : OOD ì„ë² ë”©ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. ë¦¬í¬ ì†ŒìŠ¤ë‚˜ max_total, capì„ ì¡°ì •í•´ë³´ì„¸ìš”.\")\n",
        "\n",
        "# ---------- 4) ì„ê³„ê°’ ì„ íƒ(ê²€ì¦ì…‹ TPR=95%) & ID/OOD FPR ë¹„êµ ----------\n",
        "# í•™ìŠµì— ì‚¬ìš©í•œ trainì—ì„œ validationì„ ë¶„ë¦¬(ê°„ë‹¨íˆ 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# ì¬í•™ìŠµ ì—†ì´ clfë¥¼ ì¬ì‚¬ìš©í•˜ë˜, val í™•ë¥ ë§Œ ìƒˆë¡œ ì¶”ì •\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPRì´ targetì— ê°€ì¥ ê·¼ì ‘í•œ ì ì˜ threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[ì„ê³„ê°’] TPR@valâ‰ˆ95% â†’ Ï„={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-í…ŒìŠ¤íŠ¸ FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@Ï„={fpr_id:.4f} | OOD FPR@Ï„={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) ì‹œê°í™”: í™•ë¥  ë¶„í¬ / ROC-PR / ì—ë„ˆì§€ í¸í–¥ ----------\n",
        "# (a) í™•ë¥  íˆìŠ¤í† ê·¸ë¨\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"Ï„={tau:.2f}\")\n",
        "plt.title(\"Ship í™•ë¥  ë¶„í¬(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID ê¸°ì¤€)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) ì—ë„ˆì§€ decile ë³„ FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMSì™€ í™•ë¥ \n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (ë‚®ìŒâ†’ë†’ìŒ)\")\n",
        "    plt.ylabel(f\"FPR@Ï„\"); plt.title(\"ì—ë„ˆì§€ êµ¬ê°„ë³„ FPR (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile ë¶„ì„ì„ ìœ„í•œ ìœ íš¨ í‘œë³¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"\\n[ìš”ì•½]\")\n",
        "print(f\" - ì„ê³„ê°’ Ï„(Val TPRâ‰ˆ95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@Ï„: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@Ï„: {fpr_ood:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - ê·¸ë˜í”„: í™•ë¥ ë¶„í¬/ROC/PR/ì—ë„ˆì§€-ë””ì‚¬ì¼ FPRìœ¼ë¡œ, ì—ë„ˆì§€-í¸í–¥ ì—¬ë¶€ë¥¼ í•¨ê»˜ ì ê²€\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}