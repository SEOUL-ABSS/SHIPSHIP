{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "SHIPDETECTOR_V1.ipynb",
      "authorship_tag": "ABX9TyNnxZ4DVMgcDiFz6KJcEbxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SHIPDETECTOR_V2_BYOL-A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#         DeepShip + MBARI (Streaming VAD) — Ship vs Noise Binary Classifier\n",
        "#     (RAM-safe MBARI ingestion, Reservoir Sampling, Full Audit & Visualization)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"1) 환경설정/설치 중 ...\")\n",
        "!pip -q install tensorflow tensorflow_hub soundfile librosa boto3 noisereduce umap-learn psutil\n",
        "\n",
        "# -------------------------- Imports & Setup -----------------------------------\n",
        "import os, sys, subprocess, random, math, gc, time, warnings, shutil\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa, librosa.display\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
        "import umap.umap_ as umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Fonts (optional, for Korean labels)\n",
        "!sudo apt-get -y install fonts-nanum > /dev/null\n",
        "!sudo fc-cache -fv > /dev/null\n",
        "import matplotlib.font_manager as fm\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "if os.path.exists(font_path):\n",
        "    fm.fontManager.addfont(font_path)\n",
        "    plt.rc('font', family='NanumGothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# -------------------------- Paths & Consts ------------------------------------\n",
        "YAMNET_SAMPLE_RATE = 16000\n",
        "BASE = \"/content\"\n",
        "DEEPSHIP_BASE_PATH = f\"{BASE}/DeepShip\"\n",
        "MBARI_BASE_DIR     = f\"{BASE}/MBARI_noise_data\"\n",
        "REPORT_DIR         = f\"{BASE}/reports\"\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------- Small Utils ---------------------------------------\n",
        "class Timer:\n",
        "    def __init__(self, name): self.name=name\n",
        "    def __enter__(self): self.t=time.time(); return self\n",
        "    def __exit__(self, *a):\n",
        "        rss = psutil.Process().memory_info().rss/1024**3\n",
        "        print(f\"[TIMER] {self.name}: {time.time()-self.t:.2f}s | RSS≈{rss:.2f} GB\")\n",
        "\n",
        "def mem():\n",
        "    return f\"RSS≈{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2) 데이터 확보 (DeepShip + MBARI 일부)\n",
        "# ==============================================================================\n",
        "print(\"\\n2) 데이터 확보 ...\")\n",
        "with Timer(\"DeepShip clone\"):\n",
        "    if not os.path.exists(DEEPSHIP_BASE_PATH):\n",
        "        subprocess.run(['git','clone','--depth','1','https://github.com/irfankamboh/DeepShip.git', DEEPSHIP_BASE_PATH],\n",
        "                       check=True, capture_output=True)\n",
        "        print(\" - DeepShip OK\")\n",
        "    else: print(\" - DeepShip 이미 존재\")\n",
        "\n",
        "with Timer(\"MBARI fetch (최대 10개)\"):\n",
        "    os.makedirs(MBARI_BASE_DIR, exist_ok=True)\n",
        "    if not os.listdir(MBARI_BASE_DIR):\n",
        "        s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "        pages = s3.get_paginator('list_objects_v2').paginate(Bucket='pacific-sound-16khz', Prefix='2018/01/')\n",
        "        dl_count, MAX_DL = 0, 10\n",
        "        for page in pages:\n",
        "            for obj in page.get('Contents', []):\n",
        "                if obj['Key'].endswith('.wav') and obj.get('Size',0) > 0:\n",
        "                    local = os.path.join(MBARI_BASE_DIR, os.path.basename(obj['Key']))\n",
        "                    if not os.path.exists(local):\n",
        "                        s3.download_file('pacific-sound-16khz', obj['Key'], local); dl_count+=1\n",
        "                if dl_count >= MAX_DL: break\n",
        "            if dl_count >= MAX_DL: break\n",
        "        print(f\" - MBARI OK ({dl_count}개)\")\n",
        "    else:\n",
        "        print(f\" - MBARI OK (이미 {len(os.listdir(MBARI_BASE_DIR))}개 존재)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3) 스트리밍 VAD + 저수지 표본추출 기반 세그먼트 생성 (RAM-safe)\n",
        "# ==============================================================================\n",
        "EPS = 1e-12\n",
        "\n",
        "def get_activity_intervals_streaming(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    \"\"\"\n",
        "    librosa.load 없이 2-pass 스트리밍으로 활성/비활성(초 단위)을 구함.\n",
        "    pass1: block RMS dB max, pass2: 임계치 이상 블록 병합.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr = f.samplerate; n = len(f)\n",
        "            F = max(1, int(round(frame_sec*sr))); H = max(1, int(round(hop_sec*sr)))\n",
        "\n",
        "            # pass1\n",
        "            max_db = -np.inf; pos = 0\n",
        "            while pos + F <= n:\n",
        "                f.seek(pos); y = f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms = float(np.sqrt(np.mean(y**2)) + EPS)\n",
        "                db  = 20*np.log10(rms+EPS)\n",
        "                if db > max_db: max_db = db\n",
        "                pos += H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "\n",
        "            thresh = max_db - top_db\n",
        "\n",
        "            # pass2\n",
        "            active=[]; in_active=False; cur_start=0.0\n",
        "            pos=0\n",
        "            while pos + F <= n:\n",
        "                f.seek(pos); y = f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms = float(np.sqrt(np.mean(y**2)) + EPS); db = 20*np.log10(rms+EPS)\n",
        "                t0 = pos/sr; t1=(pos+F)/sr\n",
        "                if db >= thresh:\n",
        "                    if not in_active: in_active=True; cur_start=t0\n",
        "                else:\n",
        "                    if in_active: in_active=False; active.append((cur_start, t1))\n",
        "                pos+=H\n",
        "            if in_active: active.append((cur_start, n/sr))\n",
        "\n",
        "            # inactive\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s))\n",
        "                last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except Exception:\n",
        "        return [], []\n",
        "\n",
        "def _reservoir_segments_from_spans(spans, file_path, sr_orig, seg_dur, hop, cap):\n",
        "    \"\"\" spans를 seg_dur로 쪼개되, cap개만 저수지 표본추출로 유지 (O(cap)) \"\"\"\n",
        "    res=[]; seen=0\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st = s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            seg = (file_path, float(st), sr_orig)\n",
        "            seen += 1\n",
        "            if len(res) < cap:\n",
        "                res.append(seg)\n",
        "            else:\n",
        "                j = random.randint(1, seen)\n",
        "                if j <= cap: res[j-1] = seg\n",
        "            st += hop\n",
        "    return res\n",
        "\n",
        "def create_dataset_deepship_plus_mbari(\n",
        "    deepship_root, mbari_root,\n",
        "    segment_duration=5.0,\n",
        "    deepship_overlap=0.2,  # DeepShip은 다양성 위해 약간의 겹침 허용\n",
        "    mbari_overlap=0.0,     # MBARI는 중복 줄이기 위해 0~0.1 추천\n",
        "    mbari_cap_active_per_file=120,\n",
        "    mbari_cap_inactive_per_file=120,\n",
        "    vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "):\n",
        "    ship_segments=[]; noise_segments=[]\n",
        "    # DeepShip\n",
        "    print(\"\\n[세그 생성] DeepShip ...\")\n",
        "    hop_ds = segment_duration*(1-deepship_overlap)\n",
        "    ds_files=ds_ship=ds_noise=0\n",
        "    for root,_,files in os.walk(deepship_root):\n",
        "        for fn in sorted([f for f in files if f.lower().endswith('.wav')]):\n",
        "            fp = os.path.join(root, fn)\n",
        "            try:\n",
        "                info = sf.info(fp); ds_files += 1\n",
        "            except: continue\n",
        "            act, inact = get_activity_intervals_streaming(fp, top_db=vad_top_db, frame_sec=vad_frame_sec, hop_sec=vad_hop_sec)\n",
        "            for s,e in act:\n",
        "                st=s\n",
        "                while st <= e - segment_duration + 1e-9:\n",
        "                    ship_segments.append((fp, float(st), info.samplerate)); ds_ship+=1\n",
        "                    st += hop_ds\n",
        "            for s,e in inact:\n",
        "                st=s\n",
        "                while st <= e - segment_duration + 1e-9:\n",
        "                    noise_segments.append((fp, float(st), info.samplerate)); ds_noise+=1\n",
        "                    st += hop_ds\n",
        "            gc.collect()\n",
        "    print(f\" - DeepShip 파일:{ds_files} | ship:{ds_ship} | noise:{ds_noise} | {mem()}\")\n",
        "\n",
        "    # MBARI → hard-negatives\n",
        "    print(\"[세그 생성] MBARI (hard negatives, cap 적용) ...\")\n",
        "    hop_mb = segment_duration*(1-mbari_overlap)\n",
        "    mb_files=mb_noise_added=0\n",
        "    for fn in sorted([f for f in os.listdir(mbari_root) if f.lower().endswith('.wav')]):\n",
        "        fp = os.path.join(mbari_root, fn)\n",
        "        try: info = sf.info(fp); mb_files += 1\n",
        "        except: continue\n",
        "        act, inact = get_activity_intervals_streaming(fp, top_db=vad_top_db, frame_sec=vad_frame_sec, hop_sec=vad_hop_sec)\n",
        "        active_segs   = _reservoir_segments_from_spans(act,   fp, info.samplerate, segment_duration, hop_mb, mbari_cap_active_per_file)\n",
        "        inactive_segs = _reservoir_segments_from_spans(inact, fp, info.samplerate, segment_duration, hop_mb, mbari_cap_inactive_per_file)\n",
        "        noise_segments.extend(active_segs); noise_segments.extend(inactive_segs)\n",
        "        mb_noise_added += (len(active_segs)+len(inactive_segs))\n",
        "        gc.collect()\n",
        "    print(f\" - MBARI 파일:{mb_files} | noise 추가:{mb_noise_added} | {mem()}\")\n",
        "\n",
        "    infos  = ship_segments + noise_segments\n",
        "    labels = (['ship']*len(ship_segments)) + (['noise']*len(noise_segments))\n",
        "    return infos, labels\n",
        "\n",
        "# ==============================================================================\n",
        "# 4) 오디오 로딩(세그먼트) & 임베딩 & 모델\n",
        "# ==============================================================================\n",
        "def load_and_process_segment(file_info, duration, target_sr, rms_norm=True):\n",
        "    \"\"\" 파일에서 해당 구간만 읽어 리샘플 & (선택)RMS 정규화 — 메모리 친화적 \"\"\"\n",
        "    file_path, start_time, orig_sr = file_info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_yamnet_embedding(info, yamnet_model, seg_dur):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        _, emb, _ = yamnet_model(y)\n",
        "        if emb.shape[0]==0: return None\n",
        "        return tf.reduce_mean(emb, axis=0).numpy()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def embed_infos(infos, yamnet_model, seg_dur, max_items=None, show_every=1000):\n",
        "    \"\"\" infos → (N,1024) 임베딩. max_items로 상한 설정 가능 \"\"\"\n",
        "    X = []; kept_indices = []\n",
        "    for i,info in enumerate(infos):\n",
        "        if (max_items is not None) and (len(X)>=max_items): break\n",
        "        e = extract_yamnet_embedding(info, yamnet_model, seg_dur)\n",
        "        if e is not None:\n",
        "            X.append(e); kept_indices.append(i)\n",
        "        if (i+1)%show_every==0:\n",
        "            print(f\"  임베딩 {i+1}/{len(infos)} ... {mem()}\")\n",
        "    return np.asarray(X, dtype=np.float32), kept_indices\n",
        "\n",
        "def load_yamnet():\n",
        "    print(\"\\nYAMNet 로드 ...\", end=\"\")\n",
        "    m = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
        "    print(\" OK\")\n",
        "    return m\n",
        "\n",
        "def build_classifier(input_dim, num_classes=2, lr=5e-4):\n",
        "    inp = tf.keras.Input(shape=(input_dim,), name=\"emb\")\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(inp)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ==============================================================================\n",
        "# 5) 데이터셋 생성 + 감사(Audit) + 시각화\n",
        "# ==============================================================================\n",
        "CONFIG = {\n",
        "    \"segment_duration\": 5.0,\n",
        "    \"deepship_overlap\": 0.2,\n",
        "    \"mbari_overlap\": 0.0,\n",
        "    \"mbari_cap_active_per_file\": 120,\n",
        "    \"mbari_cap_inactive_per_file\": 120,\n",
        "    \"vad_frame_sec\": 0.5, \"vad_hop_sec\": 0.25, \"vad_top_db\": 25.0,\n",
        "    \"test_size\": 0.2, \"epochs\": 40, \"batch_size\": 32, \"learning_rate\": 5e-4,\n",
        "}\n",
        "\n",
        "with Timer(\"세그먼트 생성(Streaming)\"):\n",
        "    infos, labels = create_dataset_deepship_plus_mbari(\n",
        "        DEEPSHIP_BASE_PATH, MBARI_BASE_DIR,\n",
        "        segment_duration=CONFIG[\"segment_duration\"],\n",
        "        deepship_overlap=CONFIG[\"deepship_overlap\"],\n",
        "        mbari_overlap=CONFIG[\"mbari_overlap\"],\n",
        "        mbari_cap_active_per_file=CONFIG[\"mbari_cap_active_per_file\"],\n",
        "        mbari_cap_inactive_per_file=CONFIG[\"mbari_cap_inactive_per_file\"],\n",
        "        vad_frame_sec=CONFIG[\"vad_frame_sec\"], vad_hop_sec=CONFIG[\"vad_hop_sec\"], vad_top_db=CONFIG[\"vad_top_db\"],\n",
        "    )\n",
        "\n",
        "# ---- Audit: 파일 수/총 길이/클래스 분포(세그먼트 기준) ----\n",
        "def audit_dataset(infos, labels):\n",
        "    print(\"\\n[DATASET 감사]\")\n",
        "    cnt = Counter(labels)\n",
        "    n_files = len(set([i[0] for i in infos]))\n",
        "    total_dur_h = 0.0\n",
        "    # 대략적 총 길이(중복 포함) : 세그먼트 수 * seg_dur\n",
        "    total_dur_h = (len(infos)*CONFIG[\"segment_duration\"])/3600.0\n",
        "    print(f\" - 세그먼트 수: {len(infos)} (files≈{n_files})\")\n",
        "    for k,v in cnt.items(): print(f\"   · {k}: {v}\")\n",
        "    print(f\" - (중복 포함) 세그먼트 총 길이≈ {total_dur_h:.2f} h\")\n",
        "\n",
        "audit_dataset(infos, labels)\n",
        "\n",
        "# ---- Class 분포 시각화 ----\n",
        "def plot_class_distribution(labels, title=\"세그먼트 클래스 분포\"):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    vc = pd.Series(labels).value_counts().sort_index()\n",
        "    sns.barplot(x=vc.index, y=vc.values)\n",
        "    plt.title(title); plt.ylabel(\"count\"); plt.grid(axis='y', alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution(labels)\n",
        "\n",
        "# ---- 샘플 스펙트로그램 확인 ----\n",
        "def show_sample_spectrograms(infos, labels, n_per_class=3):\n",
        "    classes = sorted(set(labels))\n",
        "    plt.figure(figsize=(5*n_per_class, 4*len(classes)))\n",
        "    idx=1\n",
        "    for c in classes:\n",
        "        idxs = [i for i,l in enumerate(labels) if l==c]\n",
        "        random.shuffle(idxs)\n",
        "        for j in idxs[:n_per_class]:\n",
        "            file_path, start_time, sr = infos[j]\n",
        "            y = load_and_process_segment(infos[j], CONFIG[\"segment_duration\"], YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "            plt.subplot(len(classes), n_per_class, idx)\n",
        "            if y is None:\n",
        "                plt.title(f\"{c}: load fail\"); idx+=1; continue\n",
        "            D = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, hop_length=320)), ref=np.max)\n",
        "            librosa.display.specshow(D, sr=YAMNET_SAMPLE_RATE, x_axis='time', y_axis='log', cmap='magma')\n",
        "            plt.title(f\"{c} | {os.path.basename(file_path)} @ {start_time:.1f}s\")\n",
        "            idx+=1\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "show_sample_spectrograms(infos, labels, n_per_class=3)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6) 학습/검증 분할 (파일 기준 Group split → 데이터 누수 방지)\n",
        "# ==============================================================================\n",
        "le = LabelEncoder(); y_enc = le.fit_transform(labels)\n",
        "groups = np.array([i[0] for i in infos])\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG[\"test_size\"], random_state=SEED)\n",
        "tr_idx, te_idx = next(gss.split(infos, y_enc, groups))\n",
        "\n",
        "Xtr_info = [infos[i] for i in tr_idx]; ytr_enc = y_enc[tr_idx]\n",
        "Xte_info = [infos[i] for i in te_idx]; yte_enc = y_enc[te_idx]\n",
        "\n",
        "print(f\"\\n[Split] train={len(Xtr_info)} | test={len(Xte_info)} (files train/test = {len(set([i[0] for i in Xtr_info]))}/{len(set([i[0] for i in Xte_info]))})\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7) YAMNet 임베딩 → 분류기 학습/평가 + 시각화 + UMAP\n",
        "# ==============================================================================\n",
        "yamnet = load_yamnet()\n",
        "\n",
        "with Timer(\"임베딩(Train)\"):\n",
        "    Xtr, tr_kept_indices = embed_infos(Xtr_info, yamnet, CONFIG[\"segment_duration\"])\n",
        "    ytr_enc_filtered = ytr_enc[tr_kept_indices]\n",
        "    ytr = tf.keras.utils.to_categorical(ytr_enc_filtered, num_classes=len(le.classes_))\n",
        "    print(f\" - Xtr:{Xtr.shape} | {mem()}\")\n",
        "\n",
        "with Timer(\"임베딩(Test)\"):\n",
        "    Xte, te_kept_indices = embed_infos(Xte_info, yamnet, CONFIG[\"segment_duration\"])\n",
        "    yte_enc_filtered = yte_enc[te_kept_indices]\n",
        "    yte = tf.keras.utils.to_categorical(yte_enc_filtered, num_classes=len(le.classes_))\n",
        "    print(f\" - Xte:{Xte.shape} | {mem()}\")\n",
        "\n",
        "# ---- 임베딩 분포 quick check ----\n",
        "print(f\"[임베딩 norm] Train mean={Xtr.mean():.3f} | std={Xtr.std():.3f}\")\n",
        "print(f\"[임베딩 norm] Test  mean={Xte.mean():.3f} | std={Xte.std():.3f}\")\n",
        "\n",
        "# ---- Class weight (언더샘플링 대신) ----\n",
        "cnt_tr = Counter(ytr_enc_filtered); total = sum(cnt_tr.values())\n",
        "class_weight = {cls: total/(len(cnt_tr)*cnt) for cls,cnt in cnt_tr.items()}\n",
        "print(\"[class_weight]\", class_weight, \" (0:noise, 1:ship 순서일 가능성)\")\n",
        "\n",
        "# ---- 모델 ----\n",
        "clf = build_classifier(Xtr.shape[-1], num_classes=len(le.classes_), lr=CONFIG[\"learning_rate\"])\n",
        "cb = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)\n",
        "]\n",
        "with Timer(\"모델 학습\"):\n",
        "    hist = clf.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=CONFIG[\"epochs\"], batch_size=CONFIG[\"batch_size\"],\n",
        "                   class_weight=class_weight, verbose=1, callbacks=cb)\n",
        "\n",
        "# ---- 학습 곡선 ----\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1); plt.plot(hist.history['accuracy'], label='train'); plt.plot(hist.history['val_accuracy'], label='val'); plt.legend()\n",
        "plt.title('정확도'); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1,2,2); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend()\n",
        "plt.title('손실'); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# ---- 평가 지표 ----\n",
        "probs = clf.predict(Xte, verbose=0); preds = probs.argmax(axis=1); true = yte.argmax(axis=1)\n",
        "acc = (preds==true).mean()\n",
        "f1m = f1_score(true, preds, average='macro')\n",
        "try:\n",
        "    ship_idx = list(le.classes_).index('ship')\n",
        "    auc_ship = roc_auc_score((true==ship_idx).astype(int), probs[:, ship_idx]) if len(np.unique(true))>1 else float('nan')\n",
        "except: auc_ship = float('nan')\n",
        "print(f\"\\n[TEST] Acc={acc:.4f} | Macro-F1={f1m:.4f} | AUC(ship)={auc_ship:.4f}\")\n",
        "print(\"\\n[분류 리포트]\\n\", classification_report(true, preds, target_names=le.classes_))\n",
        "\n",
        "# ---- 혼동행렬 ----\n",
        "cm = confusion_matrix(true, preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('예측'); plt.ylabel('실제'); plt.title('혼동 행렬'); plt.show()\n",
        "\n",
        "# ---- ROC ----\n",
        "if len(np.unique(true))==2:\n",
        "    fpr, tpr, _ = roc_curve((true==ship_idx).astype(int), probs[:, ship_idx])\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "    plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC (ship)'); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# ---- UMAP (샘플 제한; 메모리 안전) ----\n",
        "def show_umap(X, y, classes, title=\"UMAP (YAMNet Embeddings)\", max_points=1500):\n",
        "    if len(X) > max_points:\n",
        "        idx = np.random.RandomState(SEED).choice(len(X), size=max_points, replace=False)\n",
        "        Xs, ys = X[idx], y[idx]\n",
        "    else:\n",
        "        Xs, ys = X, y\n",
        "    if len(Xs) < 10:\n",
        "        print(\"UMAP: 표본이 너무 적어 생략\"); return\n",
        "    reducer = umap.UMAP(n_neighbors=min(15, len(Xs)-1), min_dist=0.1, n_components=2, random_state=SEED)\n",
        "    XY = reducer.fit_transform(Xs)\n",
        "    df = pd.DataFrame(dict(x=XY[:,0], y=XY[:,1], label=[classes[i] for i in ys]))\n",
        "    plt.figure(figsize=(7,6))\n",
        "    sns.scatterplot(data=df, x='x', y='y', hue='label', s=20, alpha=0.7)\n",
        "    plt.title(title); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "show_umap(np.vstack([Xtr,Xte]), np.hstack([ytr.argmax(1), yte.argmax(1)]), le.classes_, title=\"UMAP (Train+Test)\")\n",
        "\n",
        "print(\"\\n🎉 전체 파이프라인 완료.\")"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f4a57-f5bb-4696-dc30-151061ba8e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) 환경설정/설치 중 ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hdebconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "2) 데이터 확보 ...\n",
            " - DeepShip OK\n",
            "[TIMER] DeepShip clone: 80.42s | RSS≈1.46 GB\n",
            " - MBARI OK (10개)\n",
            "[TIMER] MBARI fetch (최대 10개): 316.13s | RSS≈1.47 GB\n",
            "\n",
            "[세그 생성] DeepShip ...\n",
            " - DeepShip 파일:63 | ship:1390 | noise:0 | RSS≈1.47 GB\n",
            "[세그 생성] MBARI (hard negatives, cap 적용) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD 평가 모듈 =================================\n",
        "# 이 블록은 기존 파이프라인에서 학습이 끝난 후에 붙여 실행하세요.\n",
        "# 필요 전역: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(학습된 분류기), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE 경로\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) Git에서 OOD 샘플 오디오 가볍게 수집 ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # 소형 예제/테스트 오디오가 비교적 들어있는 경우가 많음\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] 리포지토리 수집 ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# 오디오 확장자 패턴(넓게 잡되 개수 제한)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # 너무 많은 경우 샘플링\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - 수집된 OOD 원본 파일: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD 세그먼트(5초) 스트리밍 생성 ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load 없이 스트리밍으로 5초 구간을 균일 스트라이드로 최대 cap만 추출\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # 균일 스트라이드로 시작점 후보 생성\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# 너무 많이 뽑지 않도록 전체 cap (예: 800 세그먼트)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - 생성된 OOD 세그먼트: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD 임베딩 ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(정규화 전에)도 저장해 에너지 편향 분석\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD 임베딩 {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] 임베딩 추출 ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"경고: OOD 임베딩이 비었습니다. 리포 소스나 max_total, cap을 조정해보세요.\")\n",
        "\n",
        "# ---------- 4) 임계값 선택(검증셋 TPR=95%) & ID/OOD FPR 비교 ----------\n",
        "# 학습에 사용한 train에서 validation을 분리(간단히 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# 재학습 없이 clf를 재사용하되, val 확률만 새로 추정\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPR이 target에 가장 근접한 점의 threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[임계값] TPR@val≈95% → τ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-테스트 FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@τ={fpr_id:.4f} | OOD FPR@τ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) 시각화: 확률 분포 / ROC-PR / 에너지 편향 ----------\n",
        "# (a) 확률 히스토그램\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"τ={tau:.2f}\")\n",
        "plt.title(\"Ship 확률 분포(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID 기준)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) 에너지 decile 별 FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMS와 확률\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (낮음→높음)\")\n",
        "    plt.ylabel(f\"FPR@τ\"); plt.title(\"에너지 구간별 FPR (낮을수록 좋음)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile 분석을 위한 유효 표본이 부족합니다.\")\n",
        "\n",
        "print(\"\\n[요약]\")\n",
        "print(f\" - 임계값 τ(Val TPR≈95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@τ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@τ: {fpr_ood:.4f} (낮을수록 좋음)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - 그래프: 확률분포/ROC/PR/에너지-디사일 FPR으로, 에너지-편향 여부를 함께 점검\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}