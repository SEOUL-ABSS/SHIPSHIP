{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb",
      "authorship_tag": "ABX9TyOq59HIURpMi42yLUTtzf3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ShipsEar Ship-vs-Noise (1s) ‚Äî V0a/V0b/V5~V8 (RAM-safe, short)\n",
        "print(\"Setup...\")\n",
        "!pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1 librosa==0.10.2.post1 soundfile==0.12.1 scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2\n",
        "!apt -yq install fonts-nanum >/dev/null\n",
        "\n",
        "import os, re, random, math, time, json, glob, shutil, warnings\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import numpy as np, pandas as pd, psutil, soundfile as sf\n",
        "import tensorflow as tf, tensorflow_hub as hub, librosa\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt, seaborn as sns, matplotlib.font_manager as fm\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, average_precision_score, balanced_accuracy_score, top_k_accuracy_score, accuracy_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42; np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "try:\n",
        "    for g in tf.config.experimental.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "except: pass\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "if os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "def mem(): return f\"{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# Paths\n",
        "BASE=\"/content\"; SHIPSEAR_DRIVE=\"/content/drive/MyDrive/ShipsEar\"; SHIPSEAR=f\"{BASE}/ShipsEar_colab\"\n",
        "os.makedirs(\"results\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True)\n",
        "try:\n",
        "    from google.colab import drive; drive.mount('/content/drive', force_remount=False); print(\"Drive mounted.\")\n",
        "except Exception as e: print(\"Not Colab or Drive:\", e)\n",
        "\n",
        "# Config\n",
        "YAM_SR=16000; BINARY_MODE=True; POS_LABEL=\"Ship\"\n",
        "CFG=dict(seg_dur=1.0, ship_overlap=0.2, noise_overlap=0.0,\n",
        "         vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "         test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "         max_seg_per_group_per_class=500, noise_jitter_sec=0.5,\n",
        "         topk=1, cache_emb=True)\n",
        "\n",
        "# Speed toggles\n",
        "MAKE_PLOTS=False\n",
        "SAVE_AP=False\n",
        "\n",
        "VERSIONS=[\n",
        "    dict(name=\"v0a_yamnet_zeroshot\", type=\"zero\"),\n",
        "    dict(name=\"v0b_emb_logreg_basic\", type=\"emb\", classifier=\"logreg\", pooling=\"meanstd\", aug=None),\n",
        "    dict(name=\"v5_meanstd_mlp_aug\",  type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v6_ft_mean_headonly\", type=\"ft\",  pooling=\"mean\",    aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headonly\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_headonly_tinyLR\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "]\n",
        "\n",
        "# --- Data copy ---\n",
        "print(\"Data...\")\n",
        "if os.path.exists(SHIPSEAR_DRIVE):\n",
        "    if not os.path.exists(SHIPSEAR) or not os.listdir(SHIPSEAR):\n",
        "        shutil.copytree(SHIPSEAR_DRIVE, SHIPSEAR, dirs_exist_ok=True)\n",
        "        print(\" - Copied ShipsEar\")\n",
        "    else: print(\" - ShipsEar exists\")\n",
        "else: raise FileNotFoundError(f\"ShipsEar drive not found: {SHIPSEAR_DRIVE}\")\n",
        "\n",
        "# --- Labeling / groups ---\n",
        "KW={\"A\":[\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"],\n",
        "    \"B\":[\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"],\n",
        "    \"C\":[\"ferry\",\"passenger\"],\n",
        "    \"D\":[\"oceanliner\",\"ocean liner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"],\n",
        "    \"E\":[\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]}\n",
        "def resolve_class(path):\n",
        "    txt=(os.path.basename(os.path.dirname(path))+\" \"+os.path.basename(path)).lower()\n",
        "    for c,kws in ((\"E\",KW[\"E\"]),(\"A\",KW[\"A\"]),(\"B\",KW[\"B\"]),(\"C\",KW[\"C\"]),(\"D\",KW[\"D\"])):\n",
        "        if any(k in txt for k in kws): return c\n",
        "    m=re.search(r'\\bclass[_\\s-]*([abcde])\\b', txt); return m.group(1).upper() if m else None\n",
        "def group_key(path):\n",
        "    stem=os.path.splitext(os.path.basename(path))[0]\n",
        "    m=re.search(r'(\\d{8}[_-]?\\d{4})', stem) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', stem)\n",
        "    if m: return m.group(1)\n",
        "    parent=os.path.basename(os.path.dirname(path)); toks=re.split(r'[_\\-]+', stem); pref=\"_\".join(toks[:3]) if len(toks)>=3 else stem\n",
        "    return f\"{parent}:{pref}\"\n",
        "\n",
        "# --- VAD / segments ---\n",
        "EPS=1e-12\n",
        "def get_activity(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr=f.samplerate; n=len(f); F=max(1,int(frame_sec*sr)); H=max(1,int(hop_sec*sr))\n",
        "            max_db=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); max_db=max(max_db, 20*np.log10(rms+EPS)); pos+=H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "            th=max_db-top_db; active=[]; in_act=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                db=20*np.log10(float(np.sqrt(np.mean(y**2))+EPS))\n",
        "                t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not in_act: in_act=True; cur=t0\n",
        "                else:\n",
        "                    if in_act: in_act=False; active.append((cur,t1))\n",
        "                pos+=H\n",
        "            if in_act: active.append((cur,n/sr))\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s)); last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except: return [], []\n",
        "\n",
        "def spans_to_segs(spans, seg_dur, hop):\n",
        "    segs=[]\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st=s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            segs.append((float(st),)); st += hop\n",
        "    return segs\n",
        "\n",
        "def build_segments(root, cfg):\n",
        "    seg_dur=cfg[\"seg_dur\"]; hop_ship=seg_dur*(1-cfg[\"ship_overlap\"]); hop_noise=seg_dur*(1-cfg[\"noise_overlap\"])\n",
        "    noise_jitter=cfg[\"noise_jitter_sec\"]; cap=cfg[\"max_seg_per_group_per_class\"]\n",
        "    infos=[]; labels=[]; groups=[]; missing=0; per_gc=defaultdict(int); summary=defaultdict(int)\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        c=resolve_class(fp)\n",
        "        if c is None: missing+=1; continue\n",
        "        try: info=sf.info(fp)\n",
        "        except: continue\n",
        "        gk=group_key(fp)\n",
        "        if c in \"ABCD\":\n",
        "            act,_=get_activity(fp, cfg[\"vad_top_db\"], cfg[\"vad_frame_sec\"], cfg[\"vad_hop_sec\"]); spans=act; hop=hop_ship\n",
        "        else:\n",
        "            dur=info.frames/info.samplerate; spans=[(0.0,dur)]; hop=hop_noise\n",
        "        segs=spans_to_segs(spans, seg_dur, hop); random.shuffle(segs)\n",
        "        for (st,) in segs:\n",
        "            if c==\"E\" and noise_jitter>0:\n",
        "                j=random.uniform(-noise_jitter, noise_jitter)\n",
        "                st=max(0.0, min(st+j, (info.frames/info.samplerate) - seg_dur))\n",
        "            key=(gk,c)\n",
        "            if cap and per_gc[key]>=cap: continue\n",
        "            infos.append((fp, float(st), info.samplerate)); labels.append(c); groups.append(gk)\n",
        "            per_gc[key]+=1; summary[c]+=1\n",
        "    return infos, labels, groups, summary, missing\n",
        "\n",
        "# --- Audio IO / augment / resample ---\n",
        "_WAVE_CACHE=OrderedDict()\n",
        "_WAVE_CACHE_BYTES=0\n",
        "_MAX_CACHE_BYTES=256*1024*1024\n",
        "\n",
        "def _cache_get(fp):\n",
        "    arr=_WAVE_CACHE.get(fp)\n",
        "    if arr is not None:\n",
        "        _WAVE_CACHE.move_to_end(fp)\n",
        "    return arr\n",
        "\n",
        "def _cache_put(fp, arr):\n",
        "    global _WAVE_CACHE_BYTES\n",
        "    size=getattr(arr, \"nbytes\", None)\n",
        "    if size is None:\n",
        "        try: size=arr.size*arr.itemsize\n",
        "        except: size=0\n",
        "    _WAVE_CACHE[fp]=arr\n",
        "    _WAVE_CACHE.move_to_end(fp)\n",
        "    _WAVE_CACHE_BYTES += size\n",
        "    while _WAVE_CACHE_BYTES > _MAX_CACHE_BYTES and len(_WAVE_CACHE)>1:\n",
        "        k,v=_WAVE_CACHE.popitem(last=False)\n",
        "        try: _WAVE_CACHE_BYTES -= v.nbytes\n",
        "        except: pass\n",
        "def safe_resample(y, sr0, sr1):\n",
        "    if sr0==sr1: return y.astype(np.float32)\n",
        "    try:\n",
        "        import scipy.signal as spsig\n",
        "        g=math.gcd(int(sr0),int(sr1)); up=int(sr1)//g; down=int(sr0)//g\n",
        "        return spsig.resample_poly(y, up, down).astype(np.float32)\n",
        "    except Exception:\n",
        "        try: return librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr1, res_type=\"fft\").astype(np.float32)\n",
        "        except Exception:\n",
        "            new_len=int(round(len(y)*float(sr1)/float(sr0)))\n",
        "            xp=np.arange(len(y)); x_new=np.linspace(0,len(y),new_len,endpoint=False)\n",
        "            return np.interp(x_new, xp, y).astype(np.float32)\n",
        "\n",
        "def load_segment_cached(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        y_full = _cache_get(fp)\n",
        "        if y_full is None:\n",
        "            y_full, sr_read = sf.read(fp, dtype='float32', always_2d=False)\n",
        "            if y_full.ndim>1: y_full = y_full.mean(axis=1)\n",
        "            if sr_read != target_sr: y_full = safe_resample(y_full, sr_read, target_sr)\n",
        "            _cache_put(fp, y_full)\n",
        "        L = int(seg_dur*target_sr); start = int(st*target_sr)\n",
        "        if start >= len(y_full): return None\n",
        "        y = y_full[start : min(start+L, len(y_full))]\n",
        "        if len(y) < L: y = np.pad(y, (0, L-len(y)), mode='constant')\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def load_segment(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        start=int(st*sr0); num=int(seg_dur*sr0)\n",
        "        with sf.SoundFile(fp, 'r') as f:\n",
        "            remain=f.frames-start\n",
        "            if remain<=0: return None\n",
        "            num=min(num, remain)\n",
        "        y,_=sf.read(fp, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y is None: return None\n",
        "        if y.ndim>1: y=y.mean(axis=1)\n",
        "        if sr0!=target_sr: y=safe_resample(y, sr0, target_sr)\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def augment(y, sr, kind=\"light\"):\n",
        "    if y is None or kind!=\"light\": return y\n",
        "    y = y * (10**(random.uniform(-3,3)/20))\n",
        "    sh = random.randint(-int(0.25*sr), int(0.25*sr))\n",
        "    if sh>0: y=np.concatenate([np.zeros(sh, dtype=y.dtype), y[:-sh]])\n",
        "    elif sh<0: y=np.concatenate([y[-sh:], np.zeros(-sh, dtype=y.dtype)])\n",
        "    return y\n",
        "\n",
        "# --- YAMNet embed / zero-shot ---\n",
        "YAM_URL=\"https://tfhub.dev/google/yamnet/1\"\n",
        "def make_yam_infer():\n",
        "    ship_idx=[]\n",
        "    try:\n",
        "        module=hub.load(YAM_URL)\n",
        "        def infer(y): return module(tf.convert_to_tensor(y, tf.float32))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] hub.load\")\n",
        "        try:\n",
        "            path=module.class_map_path().numpy().decode(\"utf-8\")\n",
        "            df=pd.read_csv(path); col='display_name' if 'display_name' in df.columns else df.columns[-1]\n",
        "            names=df[col].astype(str).str.lower().tolist()\n",
        "            subs=[\"boat\",\"ship\",\"sail\",\"sailing\",\"ferry\",\"cargo\",\"tanker\",\"submarine\",\"motorboat\",\"watercraft\",\"water vehicle\",\"ocean liner\",\"yacht\",\"kayak\",\"canoe\",\"rowboat\",\"row\",\"fishing\"]\n",
        "            ship_idx=[i for i,n in enumerate(names) if any(s in n for s in subs)]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return infer, ship_idx\n",
        "    except Exception:\n",
        "        layer=hub.KerasLayer(YAM_URL, trainable=False)\n",
        "        def infer(y):\n",
        "            t=tf.convert_to_tensor(y, tf.float32)\n",
        "            try: return layer(t)\n",
        "            except: return layer(tf.expand_dims(t,0))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] KerasLayer\")\n",
        "        return infer, ship_idx\n",
        "\n",
        "def _emb_from_out(out):\n",
        "    emb=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=2: emb=out[1]\n",
        "    elif isinstance(out,dict):\n",
        "        emb=out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "        if emb is None:\n",
        "            for v in out.values():\n",
        "                if isinstance(v,dict):\n",
        "                    emb=v.get(\"embeddings\") or v.get(\"embedding\")\n",
        "                    if emb is not None: break\n",
        "    if emb is None: return None\n",
        "    t=tf.convert_to_tensor(emb)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: t=tf.expand_dims(t,0)\n",
        "    return t\n",
        "\n",
        "def embed_one(infer, y, pooling=\"meanstd\"):\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        t=_emb_from_out(infer(y))\n",
        "        if t is None or t.shape.rank!=2 or int(t.shape[0])==0: return None\n",
        "        if pooling==\"mean\":\n",
        "            feat=tf.reduce_mean(t,axis=0)\n",
        "        else:\n",
        "            m=tf.reduce_mean(t,axis=0); s=tf.math.reduce_std(t,axis=0); feat=tf.concat([m,s],axis=0)\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR embed:\", e); return None\n",
        "\n",
        "def embed_many(infos, infer, cfg, pooling=\"meanstd\", aug=None, cache_key=None, show_every=4000):\n",
        "    cache=None\n",
        "    if cfg[\"cache_emb\"] and cache_key:\n",
        "        cache=f\"cache/emb_{cache_key}.npz\"\n",
        "        if os.path.exists(cache):\n",
        "            z=np.load(cache, allow_pickle=True); print(f\" - cache {cache} | X:{z['X'].shape} keep:{z['keep'].shape}\"); return z[\"X\"], z[\"keep\"]\n",
        "    X=[]; keep=[]\n",
        "    for i,info in enumerate(infos,1):\n",
        "        y=load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "        if aug: y=augment(y, YAM_SR, aug)\n",
        "        e=embed_one(infer, y, pooling)\n",
        "        if e is not None: X.append(e); keep.append(i-1)\n",
        "        if i%show_every==0: print(f\"  ... {i}/{len(infos)} (mem {mem()})\")\n",
        "    X=np.asarray(X,np.float32); keep=np.array(keep,np.int64)\n",
        "    if cache and X.size>0: np.savez_compressed(cache, X=X, keep=keep)\n",
        "    if X.size==0: print(f\"ERR: no embeddings for {len(infos)} segs\")\n",
        "    return X, keep\n",
        "\n",
        "\n",
        "def yam_scores(infer, y):\n",
        "    out=infer(y); sc=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=1: sc=out[0]\n",
        "    elif isinstance(out,dict): sc=out.get('scores') or out.get('predictions')\n",
        "    if sc is None: return None\n",
        "    t=tf.convert_to_tensor(sc)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: return t.numpy().astype(np.float32)\n",
        "    return tf.reduce_mean(t,axis=0).numpy().astype(np.float32)\n",
        "\n",
        "\n",
        "# --- Models (emb/logreg/mlp) ---\n",
        "def build_mlp(in_dim, n_cls, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    x=tf.keras.Input(shape=(in_dim,)); h=tf.keras.layers.BatchNormalization()(x)\n",
        "    h=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.5)(h)\n",
        "    h=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.4)(h)\n",
        "    y=tf.keras.layers.Dense(n_cls, activation='softmax')(h)\n",
        "    m=tf.keras.Model(x,y); m.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy']); return m\n",
        "\n",
        "def train_eval_emb(version, Xtr, ytr, Xte, yte, classes, cfg):\n",
        "    res={}\n",
        "    if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"[emb] empty features\")\n",
        "    if version.get(\"classifier\")==\"mlp\":\n",
        "        clf=build_mlp(Xtr.shape[-1], len(classes), cfg[\"lr\"])\n",
        "        cb=[tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)]\n",
        "        ytrc=tf.keras.utils.to_categorical(ytr, num_classes=len(classes)); ytec=tf.keras.utils.to_categorical(yte, num_classes=len(classes))\n",
        "        cw={c:len(ytr)/ (len(np.unique(ytr))*cnt) for c,cnt in Counter(ytr).items()}\n",
        "        t0=time.time(); clf.fit(Xtr, ytrc, validation_data=(Xte,ytec), epochs=cfg[\"epochs\"], batch_size=cfg[\"batch\"], verbose=0, class_weight=cw, callbacks=cb)\n",
        "        probs=clf.predict(Xte, verbose=0).astype(np.float32); pred=probs.argmax(1); path=f\"artifacts/{version['name']}_mlp.keras\"; clf.save(path); res[\"artifact\"]=path; res[\"time_sec\"]=time.time()-t0\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression; from sklearn.svm import SVC; import joblib\n",
        "        sc=StandardScaler().fit(Xtr); Xtr_s=sc.transform(Xtr); Xte_s=sc.transform(Xte); t0=time.time()\n",
        "        if version.get(\"classifier\")==\"logreg\":\n",
        "            clf=LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        else:\n",
        "            clf=SVC(C=2.0, kernel='rbf', probability=True, class_weight='balanced'); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        res[\"time_sec\"]=time.time()-t0; joblib.dump(clf, f\"artifacts/{version['name']}_{version['classifier']}.joblib\"); joblib.dump(sc, f\"artifacts/{version['name']}_scaler.joblib\"); res[\"artifact\"]=\"artifacts/*\"\n",
        "    true=yte; res[\"acc\"]=accuracy_score(true,pred); res[\"bal_acc\"]=balanced_accuracy_score(true,pred); res[\"macroF1\"]=f1_score(true,pred,average='macro')\n",
        "    try:\n",
        "        res[\"macroROC\"]=roc_auc_score(tf.keras.utils.to_categorical(true, len(classes)), probs, average='macro', multi_class='ovr')\n",
        "    except: res[\"macroROC\"]=np.nan\n",
        "    try: res[\"topk\"]=top_k_accuracy_score(true, probs, k=cfg['topk'], labels=range(len(classes)))\n",
        "    except: res[\"topk\"]=np.nan\n",
        "    ap={};\n",
        "    for i,lab in enumerate(classes):\n",
        "        yb=(true==i).astype(int)\n",
        "        ap[lab]=float(average_precision_score(yb, probs[:,i])) if 0<yb.sum()<len(yb) else float(\"nan\")\n",
        "    res[\"ap_per_class\"]=ap; res[\"cm\"]=confusion_matrix(true,pred)\n",
        "    if len(classes)==2:\n",
        "        try: pos_idx=classes.index(POS_LABEL) if POS_LABEL in classes else 1; res[\"macroROC\"]=roc_auc_score(true, probs[:,pos_idx])\n",
        "        except: res[\"macroROC\"]=np.nan\n",
        "        res[\"topk\"]=np.nan\n",
        "    return res\n",
        "\n",
        "# --- Split ---\n",
        "def strat_group_split(y, groups, test_size=0.2, seed=SEED):\n",
        "    n=len(y)\n",
        "    if n<2 or len(set(y))<2:\n",
        "        raise RuntimeError(\"[Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±] ÏÑ∏Í∑∏Î®ºÌä∏ ÏàòÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÍ±∞ÎÇò ÌÅ¥ÎûòÏä§Í∞Ä 2Ï¢Ö ÎØ∏ÎßåÏûÖÎãàÎã§.\\n- SHIPSEAR_DRIVE Í≤ΩÎ°úÏôÄ Ìè¥Îçî/ÌååÏùºÎ™ÖÏùÑ Ïû¨ÌôïÏù∏ÌïòÏÑ∏Ïöî.\\n- ÎùºÎ≤® Îß§Ìïë Í∑úÏπô(resolve_class)Í≥º Ïã§Ï†ú Ìè¥ÎçîÎ™ÖÏù¥ ÎßûÎäîÏßÄ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.\")\n",
        "    gss=GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    tr,te=next(gss.split(np.arange(n), y, groups)); return tr,te,\"GroupShuffleSplit\"\n",
        "\n",
        "# --- Pipeline ---\n",
        "def run_all(cfg=CFG, versions=VERSIONS):\n",
        "    print(\"Build segments...\")\n",
        "    infos,labels,groups,summary,missing=build_segments(SHIPSEAR,cfg)\n",
        "    print(f\" - per-class: {dict(summary)} | missing: {missing}\")\n",
        "    if BINARY_MODE: labels=[\"Ship\" if l in \"ABCD\" else \"Noise\" for l in labels]\n",
        "    le=LabelEncoder(); y=le.fit_transform(labels); classes=list(le.classes_); g=np.array(groups)\n",
        "    tr,te,method=strat_group_split(y,g,cfg[\"test_size\"]); print(f\"[Split] {method} | train={len(tr)} test={len(te)} groups {len(set(g[tr]))}/{len(set(g[te]))}\")\n",
        "    Xtr_i=[infos[i] for i in tr]; ytr=y[tr]; Xte_i=[infos[i] for i in te]; yte=y[te]\n",
        "    print(\"YAMNet infer...\", end=\"\"); infer, ship_idx = make_yam_infer(); print(f\" OK (ship_idx={len(ship_idx)})\")\n",
        "\n",
        "    feature_bank = {}\n",
        "    def get_feats(tag, infos, pooling, aug):\n",
        "        key = (tag, pooling, aug or 'none', cfg['seg_dur'], len(infos))\n",
        "        if key not in feature_bank:\n",
        "            X, keep = embed_many(\n",
        "                infos, infer, cfg, pooling, aug,\n",
        "                cache_key=f\"{tag}_pool={pooling}_aug={(aug or 'none')}_seg={cfg['seg_dur']}s\"\n",
        "            )\n",
        "            feature_bank[key] = (X, keep)\n",
        "        return feature_bank[key]\n",
        "\n",
        "    all_rows = []\n",
        "    for v in versions:\n",
        "        print(f\"\\n==== {v['name']} ====\")\n",
        "        if v[\"type\"] in (\"emb\",\"ft\"):\n",
        "            pooling = v.get(\"pooling\",\"meanstd\")\n",
        "            aug = v.get(\"aug\", None)\n",
        "            print(\" - embeds (train)...\", end=\"\"); Xtr, kt = get_feats(\"train\", Xtr_i, pooling, aug); ytr_v = ytr[kt]; print(f\" OK {Xtr.shape} (mem {mem()})\")\n",
        "            print(\" - embeds (test)...\",  end=\"\"); Xte, ke = get_feats(\"test\",  Xte_i, pooling, None); yte_v = yte[ke]; print(f\" OK {Xte.shape} (mem {mem()})\")\n",
        "            if Xtr.size == 0 or Xte.size == 0: raise RuntimeError(f\"[{v['name']}] ÏûÑÎ≤†Îî© Ïã§Ìå®\")\n",
        "            if v[\"type\"] == \"emb\":\n",
        "                res = train_eval_emb(v, Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "            else:\n",
        "                res = train_eval_emb(dict(v, classifier=\"mlp\"), Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "\n",
        "        elif v[\"type\"] == \"zero\":\n",
        "            if not ship_idx:\n",
        "                print(\" - zero-shot skipped (no ship idx)\")\n",
        "                continue\n",
        "            print(\" - zero-shot scoring...\", end=\"\")\n",
        "            def score_list(infos):\n",
        "                s = []\n",
        "                for info in infos:\n",
        "                    yseg = load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "                    if yseg is None: continue\n",
        "                    sc = yam_scores(infer, yseg)\n",
        "                    if sc is None: continue\n",
        "                    s.append(float(1.0 - np.prod(1.0 - sc[ship_idx])))\n",
        "                return np.array(s, np.float32)\n",
        "\n",
        "            s_tr = score_list(Xtr_i); s_te = score_list(Xte_i)\n",
        "            keep_tr = np.where(~np.isnan(s_tr))[0]; keep_te = np.where(~np.isnan(s_te))[0]\n",
        "            s_tr = s_tr[keep_tr]; ytr_v = ytr[keep_tr]; s_te = s_te[keep_te]; yte_v = yte[keep_te]\n",
        "            pos_idx = classes.index(POS_LABEL) if POS_LABEL in classes else 1\n",
        "            ytr_bin = (ytr_v == pos_idx).astype(int); yte_bin = (yte_v == pos_idx).astype(int)\n",
        "            t_best = 0.5; f_best = -1.0\n",
        "            for t in np.linspace(0, 1, 21):\n",
        "                f = f1_score(ytr_bin, (s_tr >= t).astype(int), average='binary', zero_division=0)\n",
        "                if f > f_best: f_best = f; t_best = float(t)\n",
        "            pred = (s_te >= t_best).astype(int)\n",
        "            res = dict(\n",
        "                artifact=\"\",\n",
        "                time_sec=0.0,\n",
        "                acc=accuracy_score(yte_bin, pred),\n",
        "                bal_acc=balanced_accuracy_score(yte_bin, pred),\n",
        "                macroF1=f1_score(yte_bin, pred, average='macro'),\n",
        "                macroROC=(roc_auc_score(yte_bin, s_te) if len(np.unique(yte_bin)) == 2 else np.nan),\n",
        "                topk=np.nan,\n",
        "                ap_per_class={POS_LABEL: float(average_precision_score(yte_bin, s_te))},\n",
        "                cm=confusion_matrix(yte_bin, pred),\n",
        "            )\n",
        "            print(\" OK\")\n",
        "\n",
        "        else:\n",
        "            print(\" - unknown type; skip\")\n",
        "            continue\n",
        "\n",
        "        row = dict(\n",
        "            version=v['name'], type=v['type'],\n",
        "            pooling=v.get('pooling','-'),\n",
        "            classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'),\n",
        "            aug=(v.get('aug') or 'none'),\n",
        "            acc=res[\"acc\"], bal_acc=res[\"bal_acc\"], macroF1=res[\"macroF1\"],\n",
        "            macroROC=res[\"macroROC\"], topk=res[\"topk\"],\n",
        "            time_sec=res[\"time_sec\"], artifact=res.get(\"artifact\",\"\")\n",
        "        )\n",
        "        all_rows.append((row, res))\n",
        "\n",
        "        if MAKE_PLOTS:\n",
        "            cm = res[\"cm\"]; plt.figure(figsize=(5.2, 4.5))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "            plt.xlabel(\"ÏòàÏ∏°\"); plt.ylabel(\"Ïã§Ï†ú\"); plt.title(f\"CM ‚Äî {v['name']}\"); plt.tight_layout()\n",
        "            plt.savefig(f\"results/cm_{v['name']}.png\", dpi=150); plt.close()\n",
        "\n",
        "        if SAVE_AP:\n",
        "            with open(f\"results/ap_{v['name']}.json\", \"w\") as f:\n",
        "                json.dump(res[\"ap_per_class\"], f, indent=2)\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No results. Check data path.\"); return\n",
        "    df=pd.DataFrame([r[0] for r in all_rows]).sort_values([\"macroF1\",\"bal_acc\",\"acc\"], ascending=False)\n",
        "    df.to_csv(\"results/summary.csv\", index=False)\n",
        "    print(\"\\n[SUMMARY]\"); print(df.to_string(index=False))\n",
        "    with open(\"results/report.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Ship vs Noise ‚Äî V0a/V0b/V5~V8 ÎπÑÍµê ÏöîÏïΩ\\n\")\n",
        "        f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\\n\")\n",
        "        f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\\n\")\n",
        "        for _,row in df.iterrows():\n",
        "            macroROC = np.nan if pd.isna(row['macroROC']) else row['macroROC']\n",
        "            topk = np.nan if pd.isna(row['topk']) else row['topk']\n",
        "            f.write(f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{macroROC:.4f}|{topk:.4f}|{row['time_sec']:.1f}|{row['artifact']}|\\n\")\n",
        "        f.write(\"- ÌòºÎèôÌñâÎ†¨: results/cm_*.png\\n\\n- AP per class: results/ap_*.json\\n\")\n",
        "    print(\"\\nÍ≤∞Í≥º ÌååÏùº: results/summary.csv, results/report.md, results/cm_*.png, results/ap_*.json, artifacts/*\")\n",
        "\n",
        "# Run\n",
        "run_all(CFG, VERSIONS)\n",
        "print(\"\\nüéâ ÏôÑÎ£å\")\n"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a88410-4ddb-4a03-cd09-a69c3b73e962"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Data...\n",
            " - Copied ShipsEar\n",
            "Build segments...\n",
            " - per-class: {'C': 5085, 'B': 3134, 'E': 1140, 'A': 1855, 'D': 1513} | missing: 0\n",
            "[Split] GroupShuffleSplit | train=10227 test=2500 groups 68/17\n",
            "YAMNet infer...[YAMNet] hub.load\n",
            " OK (ship_idx=11)\n",
            "\n",
            "==== v0a_yamnet_zeroshot ====\n",
            " - zero-shot scoring... OK\n",
            "\n",
            "==== v0b_emb_logreg_basic ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 1.94 GB)\n",
            "  ... 8000/10227 (mem 1.94 GB)\n",
            " OK (10227, 2048) (mem 2.15 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.15 GB)\n",
            "\n",
            "==== v5_meanstd_mlp_aug ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 2.13 GB)\n",
            "  ... 8000/10227 (mem 2.13 GB)\n",
            " OK (10227, 2048) (mem 2.26 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.26 GB)\n",
            "\n",
            "==== v6_ft_mean_headonly ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 2.84 GB)\n",
            "  ... 8000/10227 (mem 2.81 GB)\n",
            " OK (10227, 1024) (mem 2.97 GB)\n",
            " - embeds (test)... OK (2500, 1024) (mem 3.00 GB)\n",
            "\n",
            "==== v7_ft_meanstd_headonly ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 2.88 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.88 GB)\n",
            "\n",
            "==== v8_ft_meanstd_headonly_tinyLR ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 2.95 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.95 GB)\n",
            "\n",
            "[SUMMARY]\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9912 0.983162 0.969552  0.999049   NaN 26.407738            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9912 0.973502 0.968971  0.997884   NaN 21.949281           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9904 0.975484 0.966470  0.998879   NaN 18.376309 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9884 0.952666 0.958198  0.991546   NaN 20.081191        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9868 0.939724 0.951722  0.981891   NaN  4.022647                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.494919   NaN  0.000000                                                  \n",
            "\n",
            "Í≤∞Í≥º ÌååÏùº: results/summary.csv, results/report.md, results/cm_*.png, results/ap_*.json, artifacts/*\n",
            "\n",
            "üéâ ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD ÌèâÍ∞Ä Î™®Îìà =================================\n",
        "# Ïù¥ Î∏îÎ°ùÏùÄ Í∏∞Ï°¥ ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú ÌïôÏäµÏù¥ ÎÅùÎÇú ÌõÑÏóê Î∂ôÏó¨ Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
        "# ÌïÑÏöî Ï†ÑÏó≠: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(ÌïôÏäµÎêú Î∂ÑÎ•òÍ∏∞), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE Í≤ΩÎ°ú\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) GitÏóêÏÑú OOD ÏÉòÌîå Ïò§ÎîîÏò§ Í∞ÄÎ≥çÍ≤å ÏàòÏßë ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # ÏÜåÌòï ÏòàÏ†ú/ÌÖåÏä§Ìä∏ Ïò§ÎîîÏò§Í∞Ä ÎπÑÍµêÏ†Å Îì§Ïñ¥ÏûàÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏùå\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] Î¶¨Ìè¨ÏßÄÌÜ†Î¶¨ ÏàòÏßë ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# Ïò§ÎîîÏò§ ÌôïÏû•Ïûê Ìå®ÌÑ¥(ÎÑìÍ≤å Ïû°Îêò Í∞úÏàò Ï†úÌïú)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # ÎÑàÎ¨¥ ÎßéÏùÄ Í≤ΩÏö∞ ÏÉòÌîåÎßÅ\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - ÏàòÏßëÎêú OOD ÏõêÎ≥∏ ÌååÏùº: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD ÏÑ∏Í∑∏Î®ºÌä∏(5Ï¥à) Ïä§Ìä∏Î¶¨Î∞ç ÏÉùÏÑ± ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load ÏóÜÏù¥ Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú 5Ï¥à Íµ¨Í∞ÑÏùÑ Í∑†Ïùº Ïä§Ìä∏ÎùºÏù¥ÎìúÎ°ú ÏµúÎåÄ capÎßå Ï∂îÏ∂ú\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # Í∑†Ïùº Ïä§Ìä∏ÎùºÏù¥ÎìúÎ°ú ÏãúÏûëÏ†ê ÌõÑÎ≥¥ ÏÉùÏÑ±\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# ÎÑàÎ¨¥ ÎßéÏù¥ ÎΩëÏßÄ ÏïäÎèÑÎ°ù Ï†ÑÏ≤¥ cap (Ïòà: 800 ÏÑ∏Í∑∏Î®ºÌä∏)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - ÏÉùÏÑ±Îêú OOD ÏÑ∏Í∑∏Î®ºÌä∏: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD ÏûÑÎ≤†Îî© ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(Ï†ïÍ∑úÌôî Ï†ÑÏóê)ÎèÑ Ï†ÄÏû•Ìï¥ ÏóêÎÑàÏßÄ Ìé∏Ìñ• Î∂ÑÏÑù\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD ÏûÑÎ≤†Îî© {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] ÏûÑÎ≤†Îî© Ï∂îÏ∂ú ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"Í≤ΩÍ≥†: OOD ÏûÑÎ≤†Îî©Ïù¥ ÎπÑÏóàÏäµÎãàÎã§. Î¶¨Ìè¨ ÏÜåÏä§ÎÇò max_total, capÏùÑ Ï°∞Ï†ïÌï¥Î≥¥ÏÑ∏Ïöî.\")\n",
        "\n",
        "# ---------- 4) ÏûÑÍ≥ÑÍ∞í ÏÑ†ÌÉù(Í≤ÄÏ¶ùÏÖã TPR=95%) & ID/OOD FPR ÎπÑÍµê ----------\n",
        "# ÌïôÏäµÏóê ÏÇ¨Ïö©Ìïú trainÏóêÏÑú validationÏùÑ Î∂ÑÎ¶¨(Í∞ÑÎã®Ìûà 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# Ïû¨ÌïôÏäµ ÏóÜÏù¥ clfÎ•º Ïû¨ÏÇ¨Ïö©ÌïòÎêò, val ÌôïÎ•†Îßå ÏÉàÎ°ú Ï∂îÏ†ï\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPRÏù¥ targetÏóê Í∞ÄÏû• Í∑ºÏ†ëÌïú Ï†êÏùò threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[ÏûÑÍ≥ÑÍ∞í] TPR@val‚âà95% ‚Üí œÑ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-ÌÖåÏä§Ìä∏ FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@œÑ={fpr_id:.4f} | OOD FPR@œÑ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) ÏãúÍ∞ÅÌôî: ÌôïÎ•† Î∂ÑÌè¨ / ROC-PR / ÏóêÎÑàÏßÄ Ìé∏Ìñ• ----------\n",
        "# (a) ÌôïÎ•† ÌûàÏä§ÌÜ†Í∑∏Îû®\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"œÑ={tau:.2f}\")\n",
        "plt.title(\"Ship ÌôïÎ•† Î∂ÑÌè¨(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID Í∏∞Ï§Ä)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) ÏóêÎÑàÏßÄ decile Î≥Ñ FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMSÏôÄ ÌôïÎ•†\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (ÎÇÆÏùå‚ÜíÎÜíÏùå)\")\n",
        "    plt.ylabel(f\"FPR@œÑ\"); plt.title(\"ÏóêÎÑàÏßÄ Íµ¨Í∞ÑÎ≥Ñ FPR (ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile Î∂ÑÏÑùÏùÑ ÏúÑÌïú Ïú†Ìö® ÌëúÎ≥∏Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§.\")\n",
        "\n",
        "print(\"\\n[ÏöîÏïΩ]\")\n",
        "print(f\" - ÏûÑÍ≥ÑÍ∞í œÑ(Val TPR‚âà95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@œÑ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@œÑ: {fpr_ood:.4f} (ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - Í∑∏ÎûòÌîÑ: ÌôïÎ•†Î∂ÑÌè¨/ROC/PR/ÏóêÎÑàÏßÄ-ÎîîÏÇ¨Ïùº FPRÏúºÎ°ú, ÏóêÎÑàÏßÄ-Ìé∏Ìñ• Ïó¨Î∂ÄÎ•º Ìï®Íªò Ï†êÍ≤Ä\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}