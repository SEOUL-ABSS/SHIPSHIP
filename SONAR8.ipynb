{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDjHLmuVomKPtG5gyPNftI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# AI_수중음향탐지 — 오프라인(Windows) 번들 생성 + 파이프라인 실행 (All-in-One)\n",
        "# ===========================================\n",
        "# ※ Colab에서 실행 → artifacts/offline_bundle_pip_win/ 폴더가 생성됩니다.\n",
        "#    이 폴더를 Windows 오프라인 PC로 가져가서 install_offline.bat, run_offline.bat 순으로 실행하세요.\n",
        "\n",
        "# --- 토글 ---\n",
        "PACK_OFFLINE = True       # Colab에서 Windows 오프라인 번들 생성\n",
        "WIN_PY       = \"3.10\"     # 타깃 Windows의 Python 주/부버전 (3.10 추천)\n",
        "GEN_PORTABLE = True       # 포터블(임베더블) Python용 배치 스크립트도 생성\n",
        "\n",
        "print(\"Setup...\")\n",
        "\n",
        "# (Colab 전용) 핵심 라이브러리 설치 — Windows용 번들에 넣을 버전과 동일\n",
        "try:\n",
        "    import google.colab  # 존재하면 Colab\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # TF 2.20.0: Windows 휠 있고, GCS 플러그인 의존 제거됨\n",
        "    !pip -q install \"tensorflow==2.17.1\" tensorflow_hub==0.16.1 librosa==0.10.2.post1 soundfile==0.12.1 \\\n",
        "                    scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2 \\\n",
        "                    numpy==1.26.4 scipy==1.11.4 pandas==2.2.2 matplotlib==3.8.4 audioread==3.0.1\n",
        "    # (옵션) 한글폰트\n",
        "    !apt -yq install fonts-nanum >/dev/null || true\n",
        "\n",
        "import os, re, random, math, time, json, glob, shutil, warnings, subprocess, pathlib\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import numpy as np, pandas as pd, psutil, soundfile as sf\n",
        "import tensorflow as tf, tensorflow_hub as hub, librosa\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt, seaborn as sns, matplotlib.font_manager as fm\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, average_precision_score, balanced_accuracy_score, top_k_accuracy_score, accuracy_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# ============== 공통 환경 ==============\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42; np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "try:\n",
        "    for g in tf.config.experimental.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "except: pass\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Colab 한글 폰트\n",
        "if IN_COLAB and os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "def mem(): return f\"{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# ============== 경로/데이터 세팅 ==============\n",
        "IS_WIN = (os.name == \"nt\")\n",
        "BASE = os.getcwd() if not IN_COLAB else \"/content\"\n",
        "\n",
        "# TF-Hub 캐시(Colab에서 미리 받아서 번들에 동봉)\n",
        "TFHUB_CACHE_DIR = os.path.join(\"artifacts\", \"tfhub_cache\")\n",
        "os.makedirs(TFHUB_CACHE_DIR, exist_ok=True)\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = os.path.abspath(TFHUB_CACHE_DIR)\n",
        "\n",
        "# 데이터 루트 (Colab: Drive, Windows: 환경변수 SHIPSEAR_DIR 사용)\n",
        "if IN_COLAB:\n",
        "    SHIPSEAR_DRIVE = \"/content/drive/MyDrive/ShipsEar\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(\"Not Colab or Drive:\", e)\n",
        "else:\n",
        "    SHIPSEAR_DRIVE = os.getenv(\"SHIPSEAR_DIR\", r\"D:\\Datasets\\ShipsEar\")  # Windows에서는 반드시 환경변수/실경로로 맞추세요.\n",
        "\n",
        "SHIPSEAR = os.path.join(BASE, \"ShipsEar_local\" if not IN_COLAB else \"ShipsEar_colab\")\n",
        "os.makedirs(\"results\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "print(\"Data...\")\n",
        "if os.path.exists(SHIPSEAR_DRIVE):\n",
        "    if not os.path.exists(SHIPSEAR) or not os.listdir(SHIPSEAR):\n",
        "        shutil.copytree(SHIPSEAR_DRIVE, SHIPSEAR, dirs_exist_ok=True)\n",
        "        print(\" - Copied ShipsEar\")\n",
        "    else:\n",
        "        print(\" - ShipsEar exists\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"ShipsEar path not found: {SHIPSEAR_DRIVE}\")\n",
        "\n",
        "# ============== 파이프라인 설정 ==============\n",
        "YAM_SR=16000; BINARY_MODE=True; POS_LABEL=\"Ship\"\n",
        "CFG=dict(seg_dur=1.0, ship_overlap=0.2, noise_overlap=0.0,\n",
        "         vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "         test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "         max_seg_per_group_per_class=500, noise_jitter_sec=0.5,\n",
        "         topk=1, cache_emb=True)\n",
        "\n",
        "MAKE_PLOTS=False\n",
        "SAVE_AP=False\n",
        "\n",
        "VERSIONS=[\n",
        "    dict(name=\"v0a_yamnet_zeroshot\", type=\"zero\"),\n",
        "    dict(name=\"v0b_emb_logreg_basic\", type=\"emb\", classifier=\"logreg\", pooling=\"meanstd\", aug=None),\n",
        "    dict(name=\"v5_meanstd_mlp_aug\",  type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v6_ft_mean_headonly\", type=\"ft\",  pooling=\"mean\",    aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headonly\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_headonly_tinyLR\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "]\n",
        "\n",
        "# ============== 오디오 VAD/세그먼트/입출력 ==============\n",
        "KW={\"A\":[\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"],\n",
        "    \"B\":[\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"],\n",
        "    \"C\":[\"ferry\",\"passenger\"],\n",
        "    \"D\":[\"oceanliner\",\"ocean liner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"],\n",
        "    \"E\":[\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]}\n",
        "\n",
        "def resolve_class(path):\n",
        "    txt=(os.path.basename(os.path.dirname(path))+\" \"+os.path.basename(path)).lower()\n",
        "    for c,kws in ((\"E\",KW[\"E\"]),(\"A\",KW[\"A\"]),(\"B\",KW[\"B\"]),(\"C\",KW[\"C\"]),(\"D\",KW[\"D\"])):\n",
        "        if any(k in txt for k in kws): return c\n",
        "    m=re.search(r'\\bclass[_\\s-]*([abcde])\\b', txt); return m.group(1).upper() if m else None\n",
        "\n",
        "def group_key(path):\n",
        "    stem=os.path.splitext(os.path.basename(path))[0]\n",
        "    m=re.search(r'(\\d{8}[_-]?\\d{4})', stem) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', stem)\n",
        "    if m: return m.group(1)\n",
        "    parent=os.path.basename(os.path.dirname(path)); toks=re.split(r'[_\\-]+', stem); pref=\"_\".join(toks[:3]) if len(toks)>=3 else stem\n",
        "    return f\"{parent}:{pref}\"\n",
        "\n",
        "EPS=1e-12\n",
        "def get_activity(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr=f.samplerate; n=len(f); F=max(1,int(frame_sec*sr)); H=max(1,int(hop_sec*sr))\n",
        "            max_db=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); max_db=max(max_db, 20*np.log10(rms+EPS)); pos+=H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "            th=max_db-top_db; active=[]; in_act=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                db=20*np.log10(float(np.sqrt(np.mean(y**2))+EPS))\n",
        "                t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not in_act: in_act=True; cur=t0\n",
        "                else:\n",
        "                    if in_act: in_act=False; active.append((cur,t1))\n",
        "                pos+=H\n",
        "            if in_act: active.append((cur,n/sr))\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s)); last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except: return [], []\n",
        "\n",
        "def spans_to_segs(spans, seg_dur, hop):\n",
        "    segs=[]\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st=s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            segs.append((float(st),)); st += hop\n",
        "    return segs\n",
        "\n",
        "def build_segments(root, cfg):\n",
        "    seg_dur=cfg[\"seg_dur\"]; hop_ship=seg_dur*(1-cfg[\"ship_overlap\"]); hop_noise=seg_dur*(1-cfg[\"noise_overlap\"])\n",
        "    noise_jitter=cfg[\"noise_jitter_sec\"]; cap=cfg[\"max_seg_per_group_per_class\"]\n",
        "    infos=[]; labels=[]; groups=[]; missing=0; per_gc=defaultdict(int); summary=defaultdict(int)\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        c=resolve_class(fp)\n",
        "        if c is None: missing+=1; continue\n",
        "        try: info=sf.info(fp)\n",
        "        except: continue\n",
        "        gk=group_key(fp)\n",
        "        if c in \"ABCD\":\n",
        "            act,_=get_activity(fp, cfg[\"vad_top_db\"], cfg[\"vad_frame_sec\"], cfg[\"vad_hop_sec\"]); spans=act; hop=hop_ship\n",
        "        else:\n",
        "            dur=info.frames/info.samplerate; spans=[(0.0,dur)]; hop=hop_noise\n",
        "        segs=spans_to_segs(spans, seg_dur, hop); random.shuffle(segs)\n",
        "        for (st,) in segs:\n",
        "            if c==\"E\" and noise_jitter>0:\n",
        "                j=random.uniform(-noise_jitter, noise_jitter)\n",
        "                st=max(0.0, min(st+j, (info.frames/info.samplerate) - seg_dur))\n",
        "            key=(gk,c)\n",
        "            if cap and per_gc[key]>=cap: continue\n",
        "            infos.append((fp, float(st), info.samplerate)); labels.append(c); groups.append(gk)\n",
        "            per_gc[key]+=1; summary[c]+=1\n",
        "    return infos, labels, groups, summary, missing\n",
        "\n",
        "# 캐시형 세그먼트 로딩/리샘플\n",
        "_WAVE_CACHE=OrderedDict()\n",
        "_WAVE_CACHE_BYTES=0\n",
        "_MAX_CACHE_BYTES=256*1024*1024\n",
        "\n",
        "def _cache_get(fp):\n",
        "    arr=_WAVE_CACHE.get(fp)\n",
        "    if arr is not None:\n",
        "        _WAVE_CACHE.move_to_end(fp)\n",
        "    return arr\n",
        "\n",
        "def _cache_put(fp, arr):\n",
        "    global _WAVE_CACHE_BYTES\n",
        "    size=getattr(arr, \"nbytes\", None)\n",
        "    if size is None:\n",
        "        try: size=arr.size*arr.itemsize\n",
        "        except: size=0\n",
        "    _WAVE_CACHE[fp]=arr\n",
        "    _WAVE_CACHE.move_to_end(fp)\n",
        "    _WAVE_CACHE_BYTES += size\n",
        "    while _WAVE_CACHE_BYTES > _MAX_CACHE_BYTES and len(_WAVE_CACHE)>1:\n",
        "        k,v=_WAVE_CACHE.popitem(last=False)\n",
        "        try: _WAVE_CACHE_BYTES -= v.nbytes\n",
        "        except: pass\n",
        "\n",
        "def safe_resample(y, sr0, sr1):\n",
        "    if sr0==sr1: return y.astype(np.float32)\n",
        "    try:\n",
        "        import scipy.signal as spsig\n",
        "        g=math.gcd(int(sr0),int(sr1)); up=int(sr1)//g; down=int(sr0)//g\n",
        "        return spsig.resample_poly(y, up, down).astype(np.float32)\n",
        "    except Exception:\n",
        "        try: return librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr1, res_type=\"fft\").astype(np.float32)\n",
        "        except Exception:\n",
        "            new_len=int(round(len(y)*float(sr1)/float(sr0)))\n",
        "            xp=np.arange(len(y)); x_new=np.linspace(0,len(y),new_len,endpoint=False)\n",
        "            return np.interp(x_new, xp, y).astype(np.float32)\n",
        "\n",
        "def load_segment_cached(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        y_full = _cache_get(fp)\n",
        "        if y_full is None:\n",
        "            y_full, sr_read = sf.read(fp, dtype='float32', always_2d=False)\n",
        "            if y_full.ndim>1: y_full = y_full.mean(axis=1)\n",
        "            if sr_read != target_sr: y_full = safe_resample(y_full, sr_read, target_sr)\n",
        "            _cache_put(fp, y_full)\n",
        "        L = int(seg_dur*target_sr); start = int(st*target_sr)\n",
        "        if start >= len(y_full): return None\n",
        "        y = y_full[start : min(start+L, len(y_full))]\n",
        "        if len(y) < L: y = np.pad(y, (0, L-len(y)), mode='constant')\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def load_segment(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        start=int(st*sr0); num=int(seg_dur*sr0)\n",
        "        with sf.SoundFile(fp, 'r') as f:\n",
        "            remain=f.frames-start\n",
        "            if remain<=0: return None\n",
        "            num=min(num, remain)\n",
        "        y,_=sf.read(fp, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y is None: return None\n",
        "        if y.ndim>1: y=y.mean(axis=1)\n",
        "        if sr0!=target_sr: y=safe_resample(y, sr0, target_sr)\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def augment(y, sr, kind=\"light\"):\n",
        "    if y is None or kind!=\"light\": return y\n",
        "    y = y * (10**(random.uniform(-3,3)/20))\n",
        "    sh = random.randint(-int(0.25*sr), int(0.25*sr))\n",
        "    if sh>0: y=np.concatenate([np.zeros(sh, dtype=y.dtype), y[:-sh]])\n",
        "    elif sh<0: y=np.concatenate([y[-sh:], np.zeros(-sh, dtype=y.dtype)])\n",
        "    return y\n",
        "\n",
        "# ============== YAMNet 임베딩/제로샷 ==============\n",
        "YAM_URL=\"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "def make_yam_infer():\n",
        "    ship_idx=[]\n",
        "    try:\n",
        "        module=hub.load(YAM_URL)  # 캐시가 있으면 오프라인 OK\n",
        "        def infer(y): return module(tf.convert_to_tensor(y, tf.float32))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] hub.load (cached ok)\")\n",
        "        try:\n",
        "            path=module.class_map_path().numpy().decode(\"utf-8\")\n",
        "            df=pd.read_csv(path); col='display_name' if 'display_name' in df.columns else df.columns[-1]\n",
        "            names=df[col].astype(str).str.lower().tolist()\n",
        "            subs=[\"boat\",\"ship\",\"sail\",\"sailing\",\"ferry\",\"cargo\",\"tanker\",\"submarine\",\"motorboat\",\"watercraft\",\"water vehicle\",\"ocean liner\",\"yacht\",\"kayak\",\"canoe\",\"rowboat\",\"row\",\"fishing\"]\n",
        "            ship_idx=[i for i,n in enumerate(names) if any(s in n for s in subs)]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return infer, ship_idx\n",
        "    except Exception:\n",
        "        layer=hub.KerasLayer(YAM_URL, trainable=False)\n",
        "        def infer(y):\n",
        "            t=tf.convert_to_tensor(y, tf.float32)\n",
        "            try: return layer(t)\n",
        "            except: return layer(tf.expand_dims(t,0))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] KerasLayer\")\n",
        "        return infer, ship_idx\n",
        "\n",
        "def _emb_from_out(out):\n",
        "    emb=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=2: emb=out[1]\n",
        "    elif isinstance(out,dict):\n",
        "        emb=out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "        if emb is None:\n",
        "            for v in out.values():\n",
        "                if isinstance(v,dict):\n",
        "                    emb=v.get(\"embeddings\") or v.get(\"embedding\")\n",
        "                    if emb is not None: break\n",
        "    if emb is None: return None\n",
        "    t=tf.convert_to_tensor(emb)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: t=tf.expand_dims(t,0)\n",
        "    return t\n",
        "\n",
        "def embed_one(infer, y, pooling=\"meanstd\"):\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        t=_emb_from_out(infer(y))\n",
        "        if t is None or t.shape.rank!=2 or int(t.shape[0])==0: return None\n",
        "        if pooling==\"mean\":\n",
        "            feat=tf.reduce_mean(t,axis=0)\n",
        "        else:\n",
        "            m=tf.reduce_mean(t,axis=0); s=tf.math.reduce_std(t,axis=0); feat=tf.concat([m,s],axis=0)\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR embed:\", e); return None\n",
        "\n",
        "def embed_many(infos, infer, cfg, pooling=\"meanstd\", aug=None, cache_key=None, show_every=4000):\n",
        "    cache=None\n",
        "    if cfg[\"cache_emb\"] and cache_key:\n",
        "        cache=f\"cache/emb_{cache_key}.npz\"\n",
        "        if os.path.exists(cache):\n",
        "            z=np.load(cache, allow_pickle=True); print(f\" - cache {cache} | X:{z['X'].shape} keep:{z['keep'].shape}\"); return z[\"X\"], z[\"keep\"]\n",
        "    X=[]; keep=[]\n",
        "    for i,info in enumerate(infos,1):\n",
        "        y=load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "        if aug: y=augment(y, YAM_SR, aug)\n",
        "        e=embed_one(infer, y, pooling)\n",
        "        if e is not None: X.append(e); keep.append(i-1)\n",
        "        if i%show_every==0: print(f\"  ... {i}/{len(infos)} (mem {mem()})\")\n",
        "    X=np.asarray(X,np.float32); keep=np.array(keep,np.int64)\n",
        "    if cache and X.size>0: np.savez_compressed(cache, X=X, keep=keep)\n",
        "    if X.size==0: print(f\"ERR: no embeddings for {len(infos)} segs\")\n",
        "    return X, keep\n",
        "\n",
        "def yam_scores(infer, y):\n",
        "    out=infer(y); sc=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=1: sc=out[0]\n",
        "    elif isinstance(out,dict): sc=out.get('scores') or out.get('predictions')\n",
        "    if sc is None: return None\n",
        "    t=tf.convert_to_tensor(sc)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: return t.numpy().astype(np.float32)\n",
        "    return tf.reduce_mean(t,axis=0).numpy().astype(np.float32)\n",
        "\n",
        "# ============== 분류기 (Emb/MLP/LogReg) ==============\n",
        "def build_mlp(in_dim, n_cls, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    x=tf.keras.Input(shape=(in_dim,)); h=tf.keras.layers.BatchNormalization()(x)\n",
        "    h=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.5)(h)\n",
        "    h=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.4)(h)\n",
        "    y=tf.keras.layers.Dense(n_cls, activation='softmax')(h)\n",
        "    m=tf.keras.Model(x,y); m.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy']); return m\n",
        "\n",
        "def train_eval_emb(version, Xtr, ytr, Xte, yte, classes, cfg):\n",
        "    res={}\n",
        "    if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"[emb] empty features\")\n",
        "    if version.get(\"classifier\")==\"mlp\":\n",
        "        clf=build_mlp(Xtr.shape[-1], len(classes), cfg[\"lr\"])\n",
        "        cb=[tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)]\n",
        "        ytrc=tf.keras.utils.to_categorical(ytr, num_classes=len(classes)); ytec=tf.keras.utils.to_categorical(yte, num_classes=len(classes))\n",
        "        cw={c:len(ytr)/ (len(np.unique(ytr))*cnt) for c,cnt in Counter(ytr).items()}\n",
        "        t0=time.time(); clf.fit(Xtr, ytrc, validation_data=(Xte,ytec), epochs=cfg[\"epochs\"], batch_size=cfg[\"batch\"], verbose=0, class_weight=cw, callbacks=cb)\n",
        "        probs=clf.predict(Xte, verbose=0).astype(np.float32); pred=probs.argmax(1); path=f\"artifacts/{version['name']}_mlp.keras\"; clf.save(path); res[\"artifact\"]=path; res[\"time_sec\"]=time.time()-t0\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression; from sklearn.svm import SVC; import joblib\n",
        "        sc=StandardScaler().fit(Xtr); Xtr_s=sc.transform(Xtr); Xte_s=sc.transform(Xte); t0=time.time()\n",
        "        if version.get(\"classifier\")==\"logreg\":\n",
        "            clf=LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=1); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        else:\n",
        "            clf=SVC(C=2.0, kernel='rbf', probability=True, class_weight='balanced'); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        res[\"time_sec\"]=time.time()-t0; joblib.dump(clf, f\"artifacts/{version['name']}_{version['classifier']}.joblib\"); joblib.dump(sc, f\"artifacts/{version['name']}_scaler.joblib\"); res[\"artifact\"]=\"artifacts/*\"\n",
        "    true=yte; res[\"acc\"]=accuracy_score(true,pred); res[\"bal_acc\"]=balanced_accuracy_score(true,pred); res[\"macroF1\"]=f1_score(true,pred,average='macro')\n",
        "    try:\n",
        "        res[\"macroROC\"]=roc_auc_score(tf.keras.utils.to_categorical(true, len(classes)), probs, average='macro', multi_class='ovr')\n",
        "    except: res[\"macroROC\"]=np.nan\n",
        "    try: res[\"topk\"]=top_k_accuracy_score(true, probs, k=cfg['topk'], labels=range(len(classes)))\n",
        "    except: res[\"topk\"]=np.nan\n",
        "    ap={};\n",
        "    for i,lab in enumerate(classes):\n",
        "        yb=(true==i).astype(int)\n",
        "        ap[lab]=float(average_precision_score(yb, probs[:,i])) if 0<yb.sum()<len(yb) else float(\"nan\")\n",
        "    res[\"ap_per_class\"]=ap; res[\"cm\"]=confusion_matrix(true,pred)\n",
        "    if len(classes)==2:\n",
        "        try: pos_idx=classes.index(POS_LABEL) if POS_LABEL in classes else 1; res[\"macroROC\"]=roc_auc_score(true, probs[:,pos_idx])\n",
        "        except: res[\"macroROC\"]=np.nan\n",
        "        res[\"topk\"]=np.nan\n",
        "    return res\n",
        "\n",
        "# ============== Split ==============\n",
        "def strat_group_split(y, groups, test_size=0.2, seed=SEED):\n",
        "    n=len(y)\n",
        "    if n<2 or len(set(y))<2:\n",
        "        raise RuntimeError(\"[데이터 부족] 세그먼트 수가 너무 적거나 클래스가 2종 미만입니다.\\n- 데이터 경로를 재확인하세요.\\n- 라벨링 규칙(resolve_class)과 실제 폴더명이 맞는지 점검하세요.\")\n",
        "    gss=GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    tr,te=next(gss.split(np.arange(n), y, groups)); return tr,te,\"GroupShuffleSplit\"\n",
        "\n",
        "# ============== Pipeline ==============\n",
        "def run_all(cfg=CFG, versions=VERSIONS):\n",
        "    print(\"Build segments...\")\n",
        "    infos,labels,groups,summary,missing=build_segments(SHIPSEAR,cfg)\n",
        "    print(f\" - per-class: {dict(summary)} | missing: {missing}\")\n",
        "    if BINARY_MODE: labels=[\"Ship\" if l in \"ABCD\" else \"Noise\" for l in labels]\n",
        "    le=LabelEncoder(); y=le.fit_transform(labels); classes=list(le.classes_); g=np.array(groups)\n",
        "    tr,te,method=strat_group_split(y,g,cfg[\"test_size\"]); print(f\"[Split] {method} | train={len(tr)} test={len(te)} groups {len(set(g[tr]))}/{len(set(g[te]))}\")\n",
        "    Xtr_i=[infos[i] for i in tr]; ytr=y[tr]; Xte_i=[infos[i] for i in te]; yte=y[te]\n",
        "    print(\"YAMNet infer...\", end=\"\"); infer, ship_idx = make_yam_infer(); print(f\" OK (ship_idx={len(ship_idx)})\")\n",
        "\n",
        "    feature_bank = {}\n",
        "    def get_feats(tag, infos, pooling, aug):\n",
        "        key = (tag, pooling, aug or 'none', cfg['seg_dur'], len(infos))\n",
        "        if key not in feature_bank:\n",
        "            X, keep = embed_many(\n",
        "                infos, infer, cfg, pooling, aug,\n",
        "                cache_key=f\"{tag}_pool={pooling}_aug={(aug or 'none')}_seg={cfg['seg_dur']}s\"\n",
        "            )\n",
        "            feature_bank[key] = (X, keep)\n",
        "        return feature_bank[key]\n",
        "\n",
        "    all_rows = []\n",
        "    for v in versions:\n",
        "        print(f\"\\n==== {v['name']} ====\")\n",
        "        if v[\"type\"] in (\"emb\",\"ft\"):\n",
        "            pooling = v.get(\"pooling\",\"meanstd\")\n",
        "            aug = v.get(\"aug\", None)\n",
        "            print(\" - embeds (train)...\", end=\"\"); Xtr, kt = get_feats(\"train\", Xtr_i, pooling, aug); ytr_v = ytr[kt]; print(f\" OK {Xtr.shape} (mem {mem()})\")\n",
        "            print(\" - embeds (test)...\",  end=\"\"); Xte, ke = get_feats(\"test\",  Xte_i, pooling, None); yte_v = yte[ke]; print(f\" OK {Xte.shape} (mem {mem()})\")\n",
        "            if Xtr.size == 0 or Xte.size == 0: raise RuntimeError(f\"[{v['name']}] 임베딩 실패\")\n",
        "            if v[\"type\"] == \"emb\":\n",
        "                res = train_eval_emb(v, Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "            else:\n",
        "                res = train_eval_emb(dict(v, classifier=\"mlp\"), Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "\n",
        "        elif v[\"type\"] == \"zero\":\n",
        "            if not ship_idx:\n",
        "                print(\" - zero-shot skipped (no ship idx)\")\n",
        "                continue\n",
        "            print(\" - zero-shot scoring...\", end=\"\")\n",
        "            def score_list(infos):\n",
        "                s = []\n",
        "                for info in infos:\n",
        "                    yseg = load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "                    if yseg is None: continue\n",
        "                    sc = yam_scores(infer, yseg)\n",
        "                    if sc is None: continue\n",
        "                    s.append(float(1.0 - np.prod(1.0 - sc[ship_idx])))\n",
        "                return np.array(s, np.float32)\n",
        "\n",
        "            s_tr = score_list(Xtr_i); s_te = score_list(Xte_i)\n",
        "            keep_tr = np.where(~np.isnan(s_tr))[0]; keep_te = np.where(~np.isnan(s_te))[0]\n",
        "            s_tr = s_tr[keep_tr]; ytr_v = ytr[keep_tr]; s_te = s_te[keep_te]; yte_v = yte[keep_te]\n",
        "            pos_idx = classes.index(POS_LABEL) if POS_LABEL in classes else 1\n",
        "            ytr_bin = (ytr_v == pos_idx).astype(int); yte_bin = (yte_v == pos_idx).astype(int)\n",
        "            t_best = 0.5; f_best = -1.0\n",
        "            for t in np.linspace(0, 1, 21):\n",
        "                f = f1_score(ytr_bin, (s_tr >= t).astype(int), average='binary', zero_division=0)\n",
        "                if f > f_best: f_best = f; t_best = float(t)\n",
        "            pred = (s_te >= t_best).astype(int)\n",
        "            res = dict(\n",
        "                artifact=\"\",\n",
        "                time_sec=0.0,\n",
        "                acc=accuracy_score(yte_bin, pred),\n",
        "                bal_acc=balanced_accuracy_score(yte_bin, pred),\n",
        "                macroF1=f1_score(yte_bin, pred, average='macro'),\n",
        "                macroROC=(roc_auc_score(yte_bin, s_te) if len(np.unique(yte_bin)) == 2 else np.nan),\n",
        "                topk=np.nan,\n",
        "                ap_per_class={POS_LABEL: float(average_precision_score(yte_bin, s_te))},\n",
        "                cm=confusion_matrix(yte_bin, pred),\n",
        "            )\n",
        "            print(\" OK\")\n",
        "\n",
        "        else:\n",
        "            print(\" - unknown type; skip\")\n",
        "            continue\n",
        "\n",
        "        row = dict(\n",
        "            version=v['name'], type=v['type'],\n",
        "            pooling=v.get('pooling','-'),\n",
        "            classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'),\n",
        "            aug=(v.get('aug') or 'none'),\n",
        "            acc=res[\"acc\"], bal_acc=res[\"bal_acc\"], macroF1=res[\"macroF1\"],\n",
        "            macroROC=res[\"macroROC\"], topk=res[\"topk\"],\n",
        "            time_sec=res[\"time_sec\"], artifact=res.get(\"artifact\",\"\")\n",
        "        )\n",
        "        all_rows.append((row, res))\n",
        "\n",
        "        if MAKE_PLOTS:\n",
        "            cm = res[\"cm\"]; plt.figure(figsize=(5.2, 4.5))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "            plt.xlabel(\"예측\"); plt.ylabel(\"실제\"); plt.title(f\"CM — {v['name']}\"); plt.tight_layout()\n",
        "            plt.savefig(f\"results/cm_{v['name']}.png\", dpi=150); plt.close()\n",
        "\n",
        "        if SAVE_AP:\n",
        "            with open(f\"results/ap_{v['name']}.json\", \"w\") as f:\n",
        "                json.dump(res[\"ap_per_class\"], f, indent=2)\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No results. Check data path.\"); return\n",
        "    df=pd.DataFrame([r[0] for r in all_rows]).sort_values([\"macroF1\",\"bal_acc\",\"acc\"], ascending=False)\n",
        "    df.to_csv(\"results/summary.csv\", index=False)\n",
        "    print(\"\\n[SUMMARY]\"); print(df.to_string(index=False))\n",
        "    with open(\"results/report.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Ship vs Noise — V0a/V0b/V5~V8 비교 요약\\n\")\n",
        "        f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\\n\")\n",
        "        f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\\n\")\n",
        "        for _,row in df.iterrows():\n",
        "            macroROC = np.nan if pd.isna(row['macroROC']) else row['macroROC']\n",
        "            topk = np.nan if pd.isna(row['topk']) else row['topk']\n",
        "            f.write(f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{macroROC:.4f}|{topk:.4f}|{row['time_sec']:.1f}|{row['artifact']}|\\n\")\n",
        "        f.write(\"- 혼동행렬: results/cm_*.png\\n\\n- AP per class: results/ap_*.json\\n\")\n",
        "    print(\"\\n결과 파일: results/summary.csv, results/report.md, results/cm_*.json, artifacts/*\")\n",
        "\n",
        "# ============== Windows 오프라인 번들 생성기 ==============\n",
        "def _run(cmd: str):\n",
        "    print(\"+\", cmd)\n",
        "    return subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "def prepare_offline_windows_bundle(win_py=\"3.10\", gen_portable=True, verbose=False, use_minimal_reqs=True):\n",
        "    \"\"\"\n",
        "    artifacts/offline_bundle_pip_win/ 에 Windows 오프라인 번들을 생성.\n",
        "      - wheelhouse/: Windows(win_amd64) 휠만 수집 (sdist 제외)\n",
        "      - models/tfhub_cache/: TF-Hub(YAMNet) 캐시 복사 (오프라인 허브 로드)\n",
        "      - install_offline.bat / run_offline.bat 생성 (UTF-8 고정 포함)\n",
        "      - (옵션) portable_install.bat / run_offline_portable.bat 생성\n",
        "    \"\"\"\n",
        "    out_dir = pathlib.Path(\"artifacts/offline_bundle_pip_win\")\n",
        "    wheelhouse = out_dir / \"wheelhouse\"\n",
        "    models_dir = out_dir / \"models\"\n",
        "    tfhub_out = models_dir / \"tfhub_cache\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    wheelhouse.mkdir(parents=True, exist_ok=True)\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # requirements 작성 (최소 셋: 윈도우 휠 확인된 조합)\n",
        "    req = out_dir / \"requirements.txt\"\n",
        "    if use_minimal_reqs:\n",
        "        base_reqs = [\n",
        "            \"tensorflow==2.20.0\",\n",
        "            \"tensorflow-hub==0.16.1\",\n",
        "            \"numpy==1.26.4\",\n",
        "            \"scipy==1.11.4\",\n",
        "            \"pandas==2.2.2\",\n",
        "            \"matplotlib==3.8.4\",\n",
        "            \"seaborn==0.13.2\",\n",
        "            \"librosa==0.10.2.post1\",\n",
        "            \"soundfile==0.12.1\",\n",
        "            \"scikit-learn==1.5.2\",\n",
        "            \"joblib==1.4.2\",\n",
        "            \"psutil==5.9.8\",\n",
        "            \"packaging\",\n",
        "            \"pooch>=1.0.0\",\n",
        "            \"lazy-loader>=0.3\",\n",
        "            \"decorator>=5.0.0\",\n",
        "            \"threadpoolctl>=3.1.0\",\n",
        "        ]\n",
        "        req.write_text(\"\\n\".join(base_reqs) + \"\\n\", encoding=\"utf-8\")\n",
        "        print(f\"[OK] Wrote MINIMAL {req} (lines: {len(base_reqs)})\")\n",
        "    else:\n",
        "        # (대체 경로) freeze 후 잡패키지 제거 및 필수 핀 추가 — 필요 시 사용\n",
        "        req_full = out_dir / \"requirements_full.txt\"\n",
        "        _run(f\"python -m pip freeze > {req_full}\")\n",
        "        drop_patterns = [\n",
        "            r\"^ipykernel==\", r\"^ipython==\", r\"^jupyter\", r\"^notebook==\", r\"^qtconsole==\", r\"^jedi==\",\n",
        "            r\"^google-colab==\", r\"^matplotlib-inline==\", r\"^tornado==\", r\"^pyzmq==\", r\"^debugpy==\",\n",
        "            r\"^tensorboard==\", r\"^grpcio==.*\", r\"^google-.*==\",\n",
        "            r\"^tensorflow-io-gcs-filesystem==\",\n",
        "        ]\n",
        "        kept=[]\n",
        "        for ln in req_full.read_text(encoding=\"utf-8\").splitlines():\n",
        "            if any(re.search(p, ln) for p in drop_patterns): continue\n",
        "            kept.append(ln)\n",
        "        pins = [\n",
        "            \"tensorflow==2.20.0\",\"tensorflow-hub==0.16.1\",\n",
        "            \"librosa==0.10.2.post1\",\"soundfile==0.12.1\",\n",
        "            \"scikit-learn==1.5.2\",\"psutil==5.9.8\",\"seaborn==0.13.2\",\n",
        "            \"joblib==1.4.2\",\"numpy==1.26.4\",\"scipy\",\"matplotlib\",\"pandas\"\n",
        "        ]\n",
        "        def has(lines, name):\n",
        "            n=name.split(\"==\")[0].lower().replace(\"_\",\"-\")\n",
        "            return any((l.split(\"==\")[0].lower().replace(\"_\",\"-\")==n) for l in lines if l.strip())\n",
        "        base = list(dict.fromkeys([k.strip() for k in kept if k.strip()]))\n",
        "        for p in pins:\n",
        "            if not has(base, p): base.append(p)\n",
        "        req.write_text(\"\\n\".join(base) + \"\\n\", encoding=\"utf-8\")\n",
        "        print(f\"[OK] Wrote TRIMMED {req} (lines: {len(base)})\")\n",
        "\n",
        "    # Windows 휠 교차 다운로드\n",
        "    py_major, py_minor = win_py.split(\".\")\n",
        "    plat = \"win_amd64\"\n",
        "    common = f\"--platform {plat} --python-version {py_major}{py_minor} --only-binary=:all: --prefer-binary\"\n",
        "    vflag = \"-v\" if verbose else \"\"\n",
        "    try:\n",
        "        _run(f\"python -m pip download -r {req} -d {wheelhouse} {common} {vflag}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        logf = out_dir / \"download_fail.log\"\n",
        "        os.system(f\"python -m pip download -r {req} -d {wheelhouse} {common} -v > {logf} 2>&1\")\n",
        "        print(\"\\n[WARN] 일부 패키지의 Windows 휠이 없어 실패했습니다.\")\n",
        "        print(f\" - 자세한 로그: {logf} (마지막 'No matching distribution found for ...' 확인)\")\n",
        "        raise\n",
        "\n",
        "    # TF-Hub 캐시 복사\n",
        "    if os.path.isdir(TFHUB_CACHE_DIR):\n",
        "        if os.path.isdir(tfhub_out): shutil.rmtree(tfhub_out)\n",
        "        shutil.copytree(TFHUB_CACHE_DIR, tfhub_out)\n",
        "        print(f\"[OK] Copied TF-Hub cache → {tfhub_out}\")\n",
        "\n",
        "    # 배치 스크립트 (UTF-8 강제 포함)\n",
        "    (out_dir / \"install_offline.bat\").write_text(fr\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "where python >nul 2>&1 || (echo [ERROR] Python {win_py}+ required & exit /b 1)\n",
        "python -c \"import sys;mi=list(map(int,'{win_py}'.split('.')));v=sys.version_info;exit(0 if (v.major>mi[0] or (v.major==mi[0] and v.minor>=mi[1])) else 1)\" || (echo [ERROR] Python >= {win_py} required & exit /b 1)\n",
        "python -m venv .venv\n",
        "call .venv\\Scripts\\activate\n",
        "python -m pip install --upgrade pip\n",
        "python -X utf8 -m pip install --no-index --find-links=wheelhouse -r requirements.txt\n",
        "echo [OK] Installed from local wheelhouse.\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    (out_dir / \"run_offline.bat\").write_text(rf\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set TFHUB_CACHE_DIR=%~dp0models\\tfhub_cache\n",
        "call .venv\\Scripts\\activate\n",
        "python -X utf8 -c \"import os;print('TFHUB_CACHE_DIR=', r'%TFHUB_CACHE_DIR%')\"\n",
        "echo Ready. Activate venv and run your script, e.g.:\n",
        "echo     call .venv\\Scripts\\activate\n",
        "echo     python -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    (out_dir / \"README-Windows.txt\").write_text(f\"\"\"오프라인 설치/실행 안내 (Windows)\n",
        "\n",
        "1) Python {win_py} (64-bit) 설치 + PATH 추가\n",
        "2) 이 폴더에서 install_offline.bat 실행 → wheelhouse에서 오프라인 설치\n",
        "3) run_offline.bat 실행 → TF-Hub 캐시 변수 세팅\n",
        "4) 가상환경 활성화 후 파이썬 실행:\n",
        "   > call .venv\\\\Scripts\\\\activate\n",
        "   > python -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    if gen_portable:\n",
        "        (out_dir / \"portable_install.bat\").write_text(r\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set BASE=%~dp0\n",
        "set EMBED=%BASE%py_embed\n",
        "\n",
        "if not exist \"%EMBED%\" mkdir \"%EMBED%\"\n",
        "for %%Z in (\"%BASE%python-*-embed-amd64.zip\") do (\n",
        "  powershell -NoP -C \"Expand-Archive -Path '%%~fZ' -DestinationPath '%EMBED%' -Force\"\n",
        ")\n",
        "for %%F in (\"%EMBED%\\python*.pth\") do (\n",
        "  powershell -NoP -C \"(Get-Content '%%~fF') -replace '^\\s*#\\s*import site','import site' | Set-Content '%%~fF'\"\n",
        ")\n",
        "\"%EMBED%\\python.exe\" \"%BASE%get-pip.py\" --no-index --find-links=\"%BASE%wheelhouse\" pip setuptools wheel\n",
        "\"%EMBED%\\python.exe\" -X utf8 -m pip install --no-index --find-links=\"%BASE%wheelhouse\" -r \"%BASE%requirements.txt\"\n",
        "echo [OK] Portable Python + packages installed.\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "        (out_dir / \"run_offline_portable.bat\").write_text(r\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set BASE=%~dp0\n",
        "set EMBED=%BASE%py_embed\n",
        "set TFHUB_CACHE_DIR=%BASE%models\\tfhub_cache\n",
        "\"%EMBED%\\python.exe\" -X utf8 -c \"import sys,os;print('Python:',sys.version);print('TFHUB_CACHE_DIR=',os.getenv('TFHUB_CACHE_DIR'))\"\n",
        "echo Ready. Use:\n",
        "echo    \"%EMBED%\\python.exe\" -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"\\n[OFFLINE BUNDLE READY] {out_dir.resolve()}\")\n",
        "\n",
        "# ============== 번들 생성 + 실행 ==============\n",
        "if PACK_OFFLINE and IN_COLAB:\n",
        "    print(\"[PREP] Pre-fetch TF-Hub YAMNet to cache...\")\n",
        "    _ = make_yam_infer()  # hub.load → TFHUB_CACHE_DIR에 캐시 생성\n",
        "    print(\"[PREP] Building Windows offline bundle...\")\n",
        "    prepare_offline_windows_bundle(WIN_PY, gen_portable=GEN_PORTABLE, verbose=False, use_minimal_reqs=True)\n",
        "\n",
        "# Colab에서도 바로 파이프라인 실행 가능\n",
        "run_all(CFG, VERSIONS)\n",
        "print(\"\\n🎉 완료\")"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4a7b12-a666-4d59-caf8-b4e416333a13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup...\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Data...\n",
            " - ShipsEar exists\n",
            "[PREP] Pre-fetch TF-Hub YAMNet to cache...\n",
            "[YAMNet] hub.load (cached ok)\n",
            "[PREP] Building Windows offline bundle...\n",
            "[OK] Wrote MINIMAL artifacts/offline_bundle_pip_win/requirements.txt (lines: 17)\n",
            "+ python -m pip download -r artifacts/offline_bundle_pip_win/requirements.txt -d artifacts/offline_bundle_pip_win/wheelhouse --platform win_amd64 --python-version 310 --only-binary=:all: --prefer-binary \n",
            "[OK] Copied TF-Hub cache → artifacts/offline_bundle_pip_win/models/tfhub_cache\n",
            "\n",
            "[OFFLINE BUNDLE READY] /content/artifacts/offline_bundle_pip_win\n",
            "Build segments...\n",
            " - per-class: {'C': 5085, 'E': 1140, 'D': 1513, 'B': 3134, 'A': 1855} | missing: 0\n",
            "[Split] GroupShuffleSplit | train=10227 test=2500 groups 68/17\n",
            "YAMNet infer...[YAMNet] hub.load (cached ok)\n",
            " OK (ship_idx=11)\n",
            "\n",
            "==== v0a_yamnet_zeroshot ====\n",
            " - zero-shot scoring... OK\n",
            "\n",
            "==== v0b_emb_logreg_basic ====\n",
            " - embeds (train)... - cache cache/emb_train_pool=meanstd_aug=none_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 2.72 GB)\n",
            " - embeds (test)... - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 2.72 GB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  # that has no feature names.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  # that has no feature names.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== v5_meanstd_mlp_aug ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 2.77 GB)\n",
            "  ... 8000/10227 (mem 2.88 GB)\n",
            " OK (10227, 2048) (mem 2.99 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.99 GB)\n",
            "\n",
            "==== v6_ft_mean_headonly ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 3.18 GB)\n",
            "  ... 8000/10227 (mem 3.17 GB)\n",
            " OK (10227, 1024) (mem 3.21 GB)\n",
            " - embeds (test)... OK (2500, 1024) (mem 3.21 GB)\n",
            "\n",
            "==== v7_ft_meanstd_headonly ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 3.14 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 3.14 GB)\n",
            "\n",
            "==== v8_ft_meanstd_headonly_tinyLR ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 3.27 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 3.27 GB)\n",
            "\n",
            "[SUMMARY]\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9916 0.966473 0.969877  0.994891   NaN 21.624325        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9892 0.948268 0.960499  0.995247   NaN 21.779872 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9876 0.973969 0.957394  0.995494   NaN 21.942016            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9836 0.974220 0.945168  0.997685   NaN 20.872626           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9828 0.908578 0.934808  0.985578   NaN  2.795108                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.471962   NaN  0.000000                                                  \n",
            "\n",
            "결과 파일: results/summary.csv, results/report.md, results/cm_*.json, artifacts/*\n",
            "\n",
            "🎉 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD 평가 모듈 =================================\n",
        "# 이 블록은 기존 파이프라인에서 학습이 끝난 후에 붙여 실행하세요.\n",
        "# 필요 전역: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(학습된 분류기), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE 경로\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) Git에서 OOD 샘플 오디오 가볍게 수집 ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # 소형 예제/테스트 오디오가 비교적 들어있는 경우가 많음\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] 리포지토리 수집 ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# 오디오 확장자 패턴(넓게 잡되 개수 제한)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # 너무 많은 경우 샘플링\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - 수집된 OOD 원본 파일: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD 세그먼트(5초) 스트리밍 생성 ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load 없이 스트리밍으로 5초 구간을 균일 스트라이드로 최대 cap만 추출\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # 균일 스트라이드로 시작점 후보 생성\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# 너무 많이 뽑지 않도록 전체 cap (예: 800 세그먼트)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - 생성된 OOD 세그먼트: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD 임베딩 ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(정규화 전에)도 저장해 에너지 편향 분석\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD 임베딩 {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] 임베딩 추출 ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"경고: OOD 임베딩이 비었습니다. 리포 소스나 max_total, cap을 조정해보세요.\")\n",
        "\n",
        "# ---------- 4) 임계값 선택(검증셋 TPR=95%) & ID/OOD FPR 비교 ----------\n",
        "# 학습에 사용한 train에서 validation을 분리(간단히 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# 재학습 없이 clf를 재사용하되, val 확률만 새로 추정\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPR이 target에 가장 근접한 점의 threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[임계값] TPR@val≈95% → τ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-테스트 FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@τ={fpr_id:.4f} | OOD FPR@τ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) 시각화: 확률 분포 / ROC-PR / 에너지 편향 ----------\n",
        "# (a) 확률 히스토그램\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"τ={tau:.2f}\")\n",
        "plt.title(\"Ship 확률 분포(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID 기준)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) 에너지 decile 별 FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMS와 확률\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (낮음→높음)\")\n",
        "    plt.ylabel(f\"FPR@τ\"); plt.title(\"에너지 구간별 FPR (낮을수록 좋음)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile 분석을 위한 유효 표본이 부족합니다.\")\n",
        "\n",
        "print(\"\\n[요약]\")\n",
        "print(f\" - 임계값 τ(Val TPR≈95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@τ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@τ: {fpr_ood:.4f} (낮을수록 좋음)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - 그래프: 확률분포/ROC/PR/에너지-디사일 FPR으로, 에너지-편향 여부를 함께 점검\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10cba29f",
        "outputId": "b6e9ceeb-77f1-4bb4-fcbd-da9710bd86d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "output_filename = 'offline_bundle_pip_win.zip'\n",
        "directory_to_zip = 'artifacts/offline_bundle_pip_win'\n",
        "\n",
        "# Create a zip archive of the directory\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', directory_to_zip)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(output_filename)\n",
        "\n",
        "print(f\"'{directory_to_zip}' 폴더가 '{output_filename}'으로 압축되어 다운로드됩니다.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a8259313-299b-456c-9830-2407a170e9b9\", \"offline_bundle_pip_win.zip\", 532653704)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'artifacts/offline_bundle_pip_win' 폴더가 'offline_bundle_pip_win.zip'으로 압축되어 다운로드됩니다.\n"
          ]
        }
      ]
    }
  ]
}