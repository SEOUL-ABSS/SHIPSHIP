{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "history_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb",
      "authorship_tag": "ABX9TyMI6ItAoLhFE7DrgoZ2ngb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#     ShipsEar Ship vs Noise — V5~V8 Only (RAM‑safe Fine‑Tuning + Report)\n",
        "#     - Removed V1~V4. Keep V5 (emb) + V6~V8 (fine‑tune) only\n",
        "#     - Major RAM fixes for V6+ (YAMNet map_fn returns pooled vectors, not (B,T,1024))\n",
        "#     - No layer creation inside Layer.call; deterministic tf.data prefetch; GPU mem‑growth\n",
        "#     - Summary CSV + per‑version CM + AP JSON + pretty report printed at the end\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"1) 환경설정/설치 중 ...\")\n",
        "!pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1\n",
        "!pip -q install librosa==0.10.2.post1 soundfile==0.12.1 umap-learn==0.5.6 scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2\n",
        "\n",
        "# (Colab일 때) 구글 드라이브 마운트\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Drive mounted.\")\n",
        "except Exception as e:\n",
        "    print(\"Colab이 아니라면 무시:\", e)\n",
        "\n",
        "# (선택) 한글 폰트\n",
        "!apt -yq install fonts-nanum >/dev/null\n",
        "\n",
        "# ------------------------- Imports & Setup ------------------------------------\n",
        "import os, re, sys, random, math, gc, time, warnings, shutil, glob, json\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import scipy.signal as spsig\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             balanced_accuracy_score, top_k_accuracy_score, accuracy_score)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"]=str(SEED)\n",
        "\n",
        "# GPU 메모리 성장 허용 (VRAM OOM 방지)\n",
        "try:\n",
        "    gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    if gpus: print(f\" - GPU found: {len(gpus)} | memory growth enabled\")\n",
        "except Exception as e:\n",
        "    print(\" - GPU memory growth set failed (ok):\", e)\n",
        "\n",
        "def mem(): return f\"RSS≈{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# 폰트\n",
        "if os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "    print(\" - 폰트 OK: NanumGothic\")\n",
        "\n",
        "# ------------------------- Paths & Config -------------------------------------\n",
        "BASE=\"/content\"\n",
        "SHIPSEAR_DRIVE=\"/content/drive/MyDrive/ShipsEar\"   # ← 필요시 수정\n",
        "SHIPSEAR=f\"{BASE}/ShipsEar_colab\"\n",
        "os.makedirs(\"results\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "YAM_SR=16000\n",
        "\n",
        "# ---- Binary mode (Ship vs Noise) switch ----\n",
        "BINARY_MODE = True\n",
        "POS_LABEL = \"Ship\"\n",
        "# For binary classification, Top-K is not meaningful; keep for compatibility\n",
        "BASE_CONFIG[\"topk\"] = 1\n",
        "BASE_CONFIG=dict(\n",
        "    seg_dur=1.0,               # 5초\n",
        "    ship_overlap=0.2,          # A–D overlap 비율(0.2→stride=4s)\n",
        "    noise_overlap=0.0,         # E 중복 최소화\n",
        "    vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "    test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "    spec_per_class=2,\n",
        "    umap_max_points=2000,\n",
        "    max_seg_per_group_per_class=500,\n",
        "    noise_jitter_sec=0.5,\n",
        "    topk=2,\n",
        "    cache_emb=True,            # 임베딩 캐시\n",
        ")\n",
        "\n",
        "# ------------------------- 버전 정의 (V5~V8만 유지) --------------------------\n",
        "# type: 'emb' (사전 임베딩+헤드) | 'ft' (end-to-end fine-tuning)\n",
        "VERSIONS = [\n",
        "    # ----- (A) 임베딩 기반 -----\n",
        "    dict(name=\"v5_meanstd_mlp_aug\", type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "\n",
        "    # ----- (B) 부분 파인튜닝 (RAM‑safe) -----\n",
        "    dict(name=\"v6_ft_mean_headwarmup_unfreeze\",    type=\"ft\", pooling=\"mean\",\n",
        "         warmup_epochs=5, ft_epochs=10, base_lr=3e-4, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headwarmup_unfreeze\", type=\"ft\", pooling=\"meanstd\",\n",
        "         warmup_epochs=5, ft_epochs=10, base_lr=3e-4, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_fullft_tinyLR\",       type=\"ft\", pooling=\"meanstd\",\n",
        "         warmup_epochs=0, ft_epochs=12, base_lr=1e-5, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "]\n",
        "\n",
        "# ======================================================================\n",
        "# 2) 데이터 확보\n",
        "# ======================================================================\n",
        "print(\"\\n2) 데이터 확보 중 ...\")\n",
        "if os.path.exists(SHIPSEAR_DRIVE):\n",
        "    if not os.path.exists(SHIPSEAR) or not os.listdir(SHIPSEAR):\n",
        "        shutil.copytree(SHIPSEAR_DRIVE, SHIPSEAR, dirs_exist_ok=True)\n",
        "        print(\" - ShipsEar 복사 완료\")\n",
        "    else:\n",
        "        print(\" - ShipsEar 이미 존재\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\" - ShipsEar 드라이브 경로 없음: {SHIPSEAR_DRIVE}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3) 라벨 매핑 & 그룹 키\n",
        "# ======================================================================\n",
        "A_kw = [\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"]\n",
        "B_kw = [\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"]\n",
        "C_kw = [\"ferry\",\"passenger\"]\n",
        "D_kw = [\"oceanliner\",\"ocean liner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"]\n",
        "E_kw = [\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]\n",
        "\n",
        "def resolve_ships_ear_class(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    parent = os.path.basename(os.path.dirname(path)).lower()\n",
        "    txt = f\"{parent} {name}\"\n",
        "    def has_any(txt, kws): return any(k in txt for k in kws)\n",
        "    if has_any(txt, E_kw): return \"E\"\n",
        "    if has_any(txt, A_kw): return \"A\"\n",
        "    if has_any(txt, B_kw): return \"B\"\n",
        "    if has_any(txt, C_kw): return \"C\"\n",
        "    if has_any(txt, D_kw): return \"D\"\n",
        "    m = re.search(r'\\bclass[_\\s-]*([abcde])\\b', txt)\n",
        "    if m: return m.group(1).upper()\n",
        "    return None\n",
        "\n",
        "def ships_ear_group_key(path):\n",
        "    base = os.path.basename(path)\n",
        "    stem = os.path.splitext(base)[0]\n",
        "    m = re.search(r'(\\d{8}[_-]?\\d{4})', stem) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', stem)\n",
        "    if m: return m.group(1)\n",
        "    parent = os.path.basename(os.path.dirname(path))\n",
        "    toks = re.split(r'[_\\-]+', stem)\n",
        "    prefix = \"_\".join(toks[:3]) if len(toks)>=3 else stem\n",
        "    return f\"{parent}:{prefix}\"\n",
        "\n",
        "# ======================================================================\n",
        "# 4) VAD & 세그 생성\n",
        "# ======================================================================\n",
        "EPS=1e-12\n",
        "\n",
        "def get_activity_intervals_streaming(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr=f.samplerate; n=len(f)\n",
        "            F=max(1,int(round(frame_sec*sr))); H=max(1,int(round(hop_sec*sr)))\n",
        "            # pass1: 최대 dB\n",
        "            max_db=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS)\n",
        "                db=20*np.log10(rms+EPS)\n",
        "                if db>max_db: max_db=db\n",
        "                pos+=H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "            th = max_db - top_db\n",
        "            # pass2: 병합\n",
        "            active=[]; in_act=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); db=20*np.log10(rms+EPS)\n",
        "                t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not in_act: in_act=True; cur=t0\n",
        "                else:\n",
        "                    if in_act: in_act=False; active.append((cur,t1))\n",
        "                pos+=H\n",
        "            if in_act: active.append((cur,n/sr))\n",
        "            # (참고) 비활성\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s))\n",
        "                last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except Exception:\n",
        "        return [], []\n",
        "\n",
        "def slice_spans_to_segments(spans, seg_dur, hop):\n",
        "    segs=[]\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st=s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            segs.append((float(st),))\n",
        "            st += hop\n",
        "    return segs\n",
        "\n",
        "def build_segments_ships_ear(root, cfg):\n",
        "    seg_dur=cfg[\"seg_dur\"]\n",
        "    hop_ship = seg_dur*(1-cfg[\"ship_overlap\"])\n",
        "    hop_noise= seg_dur*(1-cfg[\"noise_overlap\"])\n",
        "    noise_jitter=cfg.get(\"noise_jitter_sec\", 0.0)\n",
        "    cap=cfg.get(\"max_seg_per_group_per_class\", None)\n",
        "\n",
        "    infos=[]; labels=[]; groups=[]\n",
        "    missing=0\n",
        "    per_gc_count=defaultdict(int)\n",
        "    summary = defaultdict(int)\n",
        "\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        cls = resolve_ships_ear_class(fp)\n",
        "        if cls is None:\n",
        "            missing+=1; continue\n",
        "        try:\n",
        "            info=sf.info(fp)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        gkey = ships_ear_group_key(fp)\n",
        "        if cls in [\"A\",\"B\",\"C\",\"D\"]:\n",
        "            act,_ = get_activity_intervals_streaming(fp, top_db=cfg[\"vad_top_db\"],\n",
        "                                                     frame_sec=cfg[\"vad_frame_sec\"], hop_sec=cfg[\"vad_hop_sec\"])\n",
        "            spans = act; hop = hop_ship\n",
        "        else: # E\n",
        "            dur = info.frames/info.samplerate\n",
        "            spans = [(0.0, dur)]; hop = hop_noise\n",
        "\n",
        "        segs = slice_spans_to_segments(spans, seg_dur, hop)\n",
        "        random.shuffle(segs)\n",
        "\n",
        "        for (st,) in segs:\n",
        "            if cls == \"E\" and noise_jitter>0:\n",
        "                j = random.uniform(-noise_jitter, noise_jitter)\n",
        "                st = max(0.0, min(st + j, (info.frames/info.samplerate) - seg_dur))\n",
        "            key=(gkey, cls)\n",
        "            if cap is not None and per_gc_count[key] >= cap:\n",
        "                continue\n",
        "            infos.append((fp, float(st), info.samplerate))\n",
        "            labels.append(cls)\n",
        "            groups.append(gkey)\n",
        "            per_gc_count[key]+=1\n",
        "            summary[cls]+=1\n",
        "\n",
        "    return infos, labels, groups, summary, missing\n",
        "\n",
        "# ======================================================================\n",
        "# 5) 오디오 로드/증강/임베딩 (★ YAMNet 패치 포함)\n",
        "# ======================================================================\n",
        "\n",
        "def load_segment(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, start_time, orig_sr = info\n",
        "    try:\n",
        "        start=int(start_time*orig_sr); num=int(seg_dur*orig_sr)\n",
        "        with sf.SoundFile(fp, 'r') as f:\n",
        "            actual_num_frames = f.frames - start\n",
        "            if actual_num_frames <= 0:\n",
        "                print(f\"WARN: Segment start ({start_time:.2f}s) beyond file end for {os.path.basename(fp)}\")\n",
        "                return None\n",
        "            num = min(num, actual_num_frames)\n",
        "        y,_=sf.read(fp, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y is None: return None\n",
        "        if y.ndim>1: y=y.mean(axis=1)\n",
        "        if orig_sr!=target_sr:\n",
        "            y = safe_resample(y, orig_sr, target_sr)\n",
        "        if y is None:\n",
        "            print(f\"WARN: Resampling returned None for {os.path.basename(fp)}\"); return None\n",
        "        if rms_norm:\n",
        "            rms=np.sqrt(np.mean(y**2))+1e-12\n",
        "            y *= (10**(-20/20))/rms  # -20 dBFS\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load segment {info} - {e}\")\n",
        "        return None\n",
        "\n",
        "def augment_wave(y, sr, kind=\"light\"):\n",
        "    if y is None: return None\n",
        "    if kind==\"light\":\n",
        "        g_db = random.uniform(-3, 3)\n",
        "        y = y * (10**(g_db/20))\n",
        "        max_shift = int(0.25*sr)\n",
        "        sh = random.randint(-max_shift, max_shift)\n",
        "        if sh>0:\n",
        "            y = np.concatenate([np.zeros(sh, dtype=y.dtype), y[:-sh]])\n",
        "        elif sh<0:\n",
        "            y = np.concatenate([y[-sh:], np.zeros(-sh, dtype=y.dtype)])\n",
        "    return y\n",
        "\n",
        "# ------------------------- Resampling helper (avoid resampy) ------------------\n",
        "# Uses SciPy's polyphase resampler when available; otherwise falls back to\n",
        "# librosa's FFT resampler (no resampy), and finally to linear interpolation.\n",
        "try:\n",
        "    import scipy.signal as _spsig_internal\n",
        "except Exception:\n",
        "    _spsig_internal = None\n",
        "\n",
        "def safe_resample(y, orig_sr, target_sr):\n",
        "    if orig_sr == target_sr:\n",
        "        return y\n",
        "    try:\n",
        "        if _spsig_internal is not None:\n",
        "            g = math.gcd(int(orig_sr), int(target_sr))\n",
        "            up = int(target_sr)//g; down = int(orig_sr)//g\n",
        "            return _spsig_internal.resample_poly(y, up, down).astype(np.float32)\n",
        "        # Fallback: librosa FFT backend (does not require resampy)\n",
        "        return librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"fft\").astype(np.float32)\n",
        "    except Exception:\n",
        "        # Last resort: linear interpolation\n",
        "        new_len = int(round(len(y) * float(target_sr) / float(orig_sr)))\n",
        "        xp = np.arange(len(y))\n",
        "        x_new = np.linspace(0, len(y), new_len, endpoint=False)\n",
        "        return np.interp(x_new, xp, y).astype(np.float32)\n",
        "\n",
        "YAM_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "def make_yamnet_infer():\n",
        "    \"\"\"hub.load → 실패 시 hub.KerasLayer 폴백. infer(y: 1D float32) -> raw outputs\"\"\"\n",
        "    try:\n",
        "        module = hub.load(YAM_URL)\n",
        "        def infer(y):\n",
        "            y = tf.convert_to_tensor(y, tf.float32)     # (N,)\n",
        "            return module(y)                            # tuple or dict\n",
        "        _ = infer(np.zeros(16000, np.float32))\n",
        "        print(\"[YAMNet] backend=hub.load\")\n",
        "        return infer\n",
        "    except Exception as e1:\n",
        "        print(\"[YAMNet] hub.load failed → fallback to KerasLayer:\", repr(e1))\n",
        "        layer = hub.KerasLayer(YAM_URL, trainable=False)\n",
        "        def infer(y):\n",
        "            y = tf.convert_to_tensor(y, tf.float32)\n",
        "            try:\n",
        "                return layer(y)                         # 일부 환경에선 바로 동작\n",
        "            except Exception:\n",
        "                return layer(tf.expand_dims(y, 0))      # 배치 차원 강제 (1, N)\n",
        "        _ = infer(np.zeros(16000, np.float32))\n",
        "        print(\"[YAMNet] backend=KerasLayer\")\n",
        "        return infer\n",
        "\n",
        "def _extract_embeddings_from_output(out):\n",
        "    emb = None\n",
        "    if isinstance(out, (list, tuple)):\n",
        "        if len(out) >= 2: emb = out[1]\n",
        "    elif isinstance(out, dict):\n",
        "        emb = out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "        if emb is None:\n",
        "            for v in out.values():\n",
        "                if isinstance(v, dict):\n",
        "                    emb = v.get(\"embeddings\") or v.get(\"embedding\")\n",
        "                    if emb is not None:\n",
        "                        break\n",
        "    if emb is None:\n",
        "        return None\n",
        "\n",
        "    emb = tf.convert_to_tensor(emb)\n",
        "    if emb.shape.rank == 3 and emb.shape[0] == 1:\n",
        "        emb = tf.squeeze(emb, axis=0)\n",
        "    if emb.shape.rank == 1:\n",
        "        emb = tf.expand_dims(emb, 0)\n",
        "    return emb  # (T, 1024)\n",
        "\n",
        "def yamnet_embed(infer, y, pooling=\"meanstd\"):\n",
        "    if y is None:\n",
        "        return None\n",
        "    try:\n",
        "        out = infer(y)\n",
        "        emb = _extract_embeddings_from_output(out)\n",
        "        if emb is None or emb.shape.rank != 2 or int(emb.shape[0]) == 0:\n",
        "            return None\n",
        "        if pooling == \"mean\":\n",
        "            feat = tf.reduce_mean(emb, axis=0)\n",
        "        elif pooling == \"meanstd\":\n",
        "            m = tf.reduce_mean(emb, axis=0)\n",
        "            s = tf.math.reduce_std(emb, axis=0)\n",
        "            feat = tf.concat([m, s], axis=0)\n",
        "        else:\n",
        "            raise ValueError(\"pooling must be 'mean' or 'meanstd'\")\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to embed waveform - {e}\")\n",
        "        return None\n",
        "\n",
        "def embed_many(infos, yam_infer, cfg, pooling=\"mean\", aug=None, cache_key=None, show_every=5000):\n",
        "    cache_path = None\n",
        "    if cfg.get(\"cache_emb\", True) and cache_key:\n",
        "        cache_path = os.path.join(\"cache\", f\"emb_{cache_key}.npz\")\n",
        "        if os.path.exists(cache_path):\n",
        "            try:\n",
        "                z=np.load(cache_path, allow_pickle=True)\n",
        "                print(f\" - 캐시 로드: {cache_path} | X:{z['X'].shape} | keep:{z['keep'].shape}\")\n",
        "                return z[\"X\"], z[\"keep\"]\n",
        "            except Exception as e:\n",
        "                print(f\"WARN: 캐시 로드 실패 {cache_path} - {e}. 재생성합니다.\")\n",
        "                if os.path.exists(cache_path): os.remove(cache_path)\n",
        "\n",
        "    X=[]; keep=[]\n",
        "    for i,info in enumerate(infos,1):\n",
        "        y=load_segment(info, cfg[\"seg_dur\"], YAM_SR, rms_norm=True)\n",
        "        if aug:\n",
        "            y=augment_wave(y, YAM_SR, kind=aug)\n",
        "        e=yamnet_embed(yam_infer, y, pooling=pooling)\n",
        "        if e is not None:\n",
        "            X.append(e); keep.append(i-1)\n",
        "        if i%show_every==0:\n",
        "            print(f\"  ... {i}/{len(infos)} | {mem()}\")\n",
        "\n",
        "    X=np.asarray(X, np.float32)\n",
        "    keep=np.array(keep, np.int64)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"ERROR: Failed to generate any embeddings for {len(infos)} segments.\")\n",
        "\n",
        "    if cache_path is not None and X.size > 0:\n",
        "        try:\n",
        "            np.savez_compressed(cache_path, X=X, keep=keep)\n",
        "            print(f\" - 캐시 저장: {cache_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARN: 캐시 저장 실패 {cache_path} - {e}\")\n",
        "\n",
        "    return X, keep\n",
        "\n",
        "# ======================================================================\n",
        "# 6) 분할(가능하면 그룹-계층)\n",
        "# ======================================================================\n",
        "\n",
        "def stratified_group_split(y, groups, test_size=0.2, seed=SEED):\n",
        "    n=len(y)\n",
        "    if n < 2:\n",
        "        raise RuntimeError(\"[데이터 부족] 세그먼트가 2개 미만입니다.\")\n",
        "    uniq_groups = len(np.unique(groups))\n",
        "    counts = np.bincount(y) if len(y)>0 else np.array([])\n",
        "    min_per_class = int(counts[counts > 0].min()) if counts.size else 0\n",
        "    desired = max(2, int(round(1.0 / max(1e-9, test_size))))\n",
        "    n_splits = max(2, min(desired, uniq_groups, max(2, min_per_class)))\n",
        "\n",
        "    try:\n",
        "        from sklearn.model_selection import StratifiedGroupKFold\n",
        "        sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "        tr_idx, te_idx = next(sgkf.split(np.zeros(n), y, groups))\n",
        "        method=f\"StratifiedGroupKFold(n_splits={n_splits})\"\n",
        "    except Exception:\n",
        "        gss=GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "        tr_idx, te_idx = next(gss.split(np.arange(n), y, groups))\n",
        "        method=\"GroupShuffleSplit\"\n",
        "    return tr_idx, te_idx, method\n",
        "\n",
        "# ======================================================================\n",
        "# 7) (A) 임베딩 기반 분류기(MLP/LogReg/SVM)\n",
        "# ======================================================================\n",
        "\n",
        "def build_mlp(input_dim, num_classes, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    inp=tf.keras.Input(shape=(input_dim,), name=\"emb\")\n",
        "    x=tf.keras.layers.BatchNormalization()(inp)\n",
        "    x=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(x); x=tf.keras.layers.Dropout(0.5)(x)\n",
        "    x=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(x); x=tf.keras.layers.Dropout(0.4)(x)\n",
        "    out=tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    m=tf.keras.Model(inp,out)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def train_eval_emb(version, Xtr, ytr, Xte, yte, classes, cfg):\n",
        "    res={}\n",
        "    if Xtr.size == 0 or Xte.size == 0 or Xtr.ndim != 2 or Xte.ndim != 2:\n",
        "        raise RuntimeError(\"[임베딩 실패] Xtr/Xte가 비어있거나 차원이 잘못되었습니다.\")\n",
        "    if version[\"classifier\"]==\"mlp\":\n",
        "        clf=build_mlp(Xtr.shape[-1], len(classes), lr=cfg[\"lr\"])\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6),\n",
        "        ]\n",
        "        ytr_cat=tf.keras.utils.to_categorical(ytr, num_classes=len(classes))\n",
        "        yte_cat=tf.keras.utils.to_categorical(yte, num_classes=len(classes))\n",
        "        cnt_tr=Counter(ytr); total=sum(cnt_tr.values())\n",
        "        class_weight={cls: total/(len(cnt_tr)*cnt) for cls,cnt in cnt_tr.items()}\n",
        "        t0=time.time()\n",
        "        clf.fit(Xtr, ytr_cat, validation_data=(Xte, yte_cat),\n",
        "                epochs=cfg[\"epochs\"], batch_size=cfg[\"batch\"], verbose=0,\n",
        "                class_weight=class_weight, callbacks=callbacks)\n",
        "        probs=clf.predict(Xte, verbose=0); pred=probs.argmax(1)\n",
        "        model_path=f\"artifacts/{version['name']}_mlp.keras\"; clf.save(model_path)\n",
        "        res[\"artifact\"]=model_path; res[\"time_sec\"]=time.time()-t0\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.svm import SVC\n",
        "        scaler=StandardScaler().fit(Xtr)\n",
        "        Xtr_s=scaler.transform(Xtr); Xte_s=scaler.transform(Xte)\n",
        "        t0=time.time()\n",
        "        if version[\"classifier\"]==\"logreg\":\n",
        "            clf=LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\n",
        "            clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        else:\n",
        "            clf=SVC(C=2.0, kernel='rbf', probability=True, class_weight='balanced')\n",
        "            clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        res[\"time_sec\"]=time.time()-t0\n",
        "        import joblib\n",
        "        model_path=f\"artifacts/{version['name']}_{version['classifier']}.joblib\"\n",
        "        scaler_path=f\"artifacts/{version['name']}_scaler.joblib\"\n",
        "        joblib.dump(clf, model_path); joblib.dump(scaler, scaler_path)\n",
        "        res[\"artifact\"]=model_path; res[\"scaler\"]=scaler_path\n",
        "\n",
        "    true=yte\n",
        "    res[\"acc\"]=accuracy_score(true, pred)\n",
        "    res[\"bal_acc\"]=balanced_accuracy_score(true, pred)\n",
        "    res[\"macroF1\"]=f1_score(true, pred, average='macro')\n",
        "    try:\n",
        "        yte_cat=tf.keras.utils.to_categorical(true, num_classes=len(classes))\n",
        "        res[\"macroROC\"]=roc_auc_score(yte_cat, probs, average='macro', multi_class='ovr')\n",
        "    except Exception:\n",
        "        res[\"macroROC\"]=np.nan\n",
        "    try:\n",
        "        res[\"topk\"]=top_k_accuracy_score(true, probs, k=BASE_CONFIG[\"topk\"], labels=range(len(classes)))\n",
        "    except Exception:\n",
        "        res[\"topk\"]=np.nan\n",
        "    ap={}\n",
        "    for i,lab in enumerate(classes):\n",
        "        y_bin=(true==i).astype(int)\n",
        "        if 0<y_bin.sum()<len(y_bin): ap[lab]=float(average_precision_score(y_bin, probs[:,i]))\n",
        "        else: ap[lab]=float(\"nan\")\n",
        "    res[\"ap_per_class\"]=ap\n",
        "    res[\"cm\"]=confusion_matrix(true, pred)\n",
        "    # Binary metrics override (if applicable)\n",
        "    if len(classes) == 2:\n",
        "        try:\n",
        "            pos_idx = classes.index(POS_LABEL) if POS_LABEL in classes else 1\n",
        "            res[\"macroROC\"] = roc_auc_score(true, probs[:, pos_idx])\n",
        "        except Exception:\n",
        "            res[\"macroROC\"] = np.nan\n",
        "        res[\"topk\"] = np.nan\n",
        "    return res\n",
        "\n",
        "# ======================================================================\n",
        "# 8) (B) 부분 파인튜닝 — RAM‑safe tf.map_fn (returns fixed D per sample)\n",
        "# ======================================================================\n",
        "\n",
        "def make_wave_ds(infos, y_enc, cfg, batch, shuffle=False, aug=None):\n",
        "    L=int(YAM_SR*cfg[\"seg_dur\"])\n",
        "\n",
        "    def gen():\n",
        "        for info, y in zip(infos, y_enc):\n",
        "            wav=load_segment(info, cfg[\"seg_dur\"], YAM_SR, rms_norm=True)\n",
        "            if aug: wav=augment_wave(wav, YAM_SR, kind=aug)\n",
        "            if wav is None: continue\n",
        "            if len(wav)<L:\n",
        "                pad=np.zeros(L, dtype=np.float32); pad[:len(wav)]=wav; wav=pad\n",
        "            elif len(wav)>L:\n",
        "                wav=wav[:L]\n",
        "            yield wav.astype(np.float32), np.int32(y)\n",
        "\n",
        "    ds=tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(L,), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds=ds.shuffle(buffer_size=min(8000, len(infos)))\n",
        "    # 메모리 안전: 과도한 프리패치 방지\n",
        "    ds=ds.batch(batch, drop_remainder=False).prefetch(2)\n",
        "    return ds\n",
        "\n",
        "class YamnetEmbeddingLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"RAM‑safe: map_fn returns per‑sample pooled vector (1024 or 2048),\n",
        "    avoiding (B,T,1024) materialization. Also defines output shapes for Keras.\"\"\"\n",
        "    def __init__(self, yamnet_url, pooling=\"mean\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if pooling not in [\"mean\", \"meanstd\"]:\n",
        "            raise ValueError(\"pooling must be 'mean' or 'meanstd'\")\n",
        "        self.yamnet_url = yamnet_url\n",
        "        self.pooling = pooling\n",
        "        # YAMNet from TF‑Hub is effectively non‑trainable (frozen graph); keep False to avoid warnings\n",
        "        self.yamnet_layer = hub.KerasLayer(self.yamnet_url, trainable=False, name=\"yamnet_base\")\n",
        "        self.out_dim = 1024 if pooling==\"mean\" else 2048\n",
        "\n",
        "    def _pool_single(self, waveform):\n",
        "        # waveform: (N,)\n",
        "        out = self.yamnet_layer(waveform)\n",
        "        # extract embeddings\n",
        "        if isinstance(out, (list, tuple)) and len(out) > 1:\n",
        "            emb = out[1]  # (T,1024) expected\n",
        "        elif isinstance(out, dict):\n",
        "            emb = out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "            if emb is None and len(out) > 0:\n",
        "                # fallback: take the last value\n",
        "                emb = list(out.values())[-1]\n",
        "        else:\n",
        "            raise RuntimeError(\"Unexpected YAMNet output format\")\n",
        "        emb = tf.convert_to_tensor(emb)\n",
        "        # If a batch dim accidentally appears, drop it using safe gather (no Squeeze)\n",
        "        rank = tf.rank(emb)\n",
        "        emb = tf.cond(tf.equal(rank, 3), lambda: emb[0], lambda: emb)  # (T,1024)\n",
        "        # pool to fixed size\n",
        "        m = tf.reduce_mean(emb, axis=0)  # (1024,)\n",
        "        if self.pooling == \"mean\":\n",
        "            return m\n",
        "        s = tf.math.reduce_std(emb, axis=0)  # (1024,)\n",
        "        return tf.concat([m, s], axis=0)     # (2048,)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (B, N)\n",
        "        outputs = []\n",
        "        for i in tf.range(tf.shape(inputs)[0]):\n",
        "            w = inputs[i]\n",
        "            feat = self._pool_single(w)\n",
        "            outputs.append(tf.expand_dims(feat,0))\n",
        "        return tf.concat(outputs, axis=0)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # input_shape = (B, N)\n",
        "        return (input_shape[0], self.out_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"yamnet_url\": self.yamnet_url, \"pooling\": self.pooling})\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_yamnet_ft_model(num_classes, pooling=\"meanstd\", lr=3e-4):\n",
        "    wave_in = tf.keras.Input(shape=(int(YAM_SR*BASE_CONFIG[\"seg_dur\"]),), dtype=tf.float32, name=\"wave\")\n",
        "    embedding_layer = YamnetEmbeddingLayer(yamnet_url=YAM_URL, pooling=pooling, name=\"yamnet_embedding\")\n",
        "    feat = embedding_layer(wave_in)  # (B, D)\n",
        "\n",
        "    x=tf.keras.layers.BatchNormalization()(feat)\n",
        "    x=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x=tf.keras.layers.Dropout(0.5)(x)\n",
        "    x=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x=tf.keras.layers.Dropout(0.4)(x)\n",
        "    out=tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model=tf.keras.Model(wave_in, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_eval_ft(version, Xtr_infos, ytr, Xte_infos, yte, classes, yam_infer):\n",
        "    \"\"\"Head-only training on frozen YAMNet embeddings.\n",
        "    Rationale: TF-Hub yamnet/1 exposes no trainable vars; true base FT is not\n",
        "    possible here. We stream pooled embeddings and train an MLP head.\n",
        "    \"\"\"\n",
        "    if yam_infer is None:\n",
        "        raise RuntimeError(\"YAMNet infer not initialized.\")\n",
        "\n",
        "    pooling=version.get(\"pooling\", \"meanstd\")\n",
        "    aug=version.get(\"aug\", None)\n",
        "\n",
        "    print(\" - (Head-only) 임베딩(Train) 생성 중 ...\", end=\"\")\n",
        "    Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, BASE_CONFIG, pooling=pooling, aug=aug,\n",
        "                              cache_key=f\"{version['name']}_{pooling}_ft_tr\")\n",
        "    ytr_v=ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "\n",
        "    print(\" - (Head-only) 임베딩(Test) 생성 중 ...\", end=\"\")\n",
        "    Xte, keep_te = embed_many(Xte_infos, yam_infer, BASE_CONFIG, pooling=pooling, aug=None,\n",
        "                              cache_key=f\"{version['name']}_{pooling}_ft_te\")\n",
        "    yte_v=yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "    # 재사용: MLP head 학습/평가\n",
        "    v_copy = dict(version)  # shallow copy to avoid side effects\n",
        "    v_copy[\"classifier\"] = \"mlp\"\n",
        "    v_copy[\"type\"] = \"ft\"\n",
        "    res = train_eval_emb(v_copy, Xtr, ytr_v, Xte, yte_v, classes, cfg=BASE_CONFIG)\n",
        "    return res\n",
        "\n",
        "# ======================================================================\n",
        "# 9) 파이프라인 실행 (한 번 분할 → 모든 버전 공통 비교)\n",
        "# ======================================================================\n",
        "\n",
        "def run_all(config=BASE_CONFIG, versions=VERSIONS):\n",
        "    \"\"\"Single split → compare across versions (V5~V8), head-only for FT.\n",
        "    - V5 uses embedding+MLP.\n",
        "    - V6~V8 run as head-only on frozen YAMNet embeddings (same pipeline as V5),\n",
        "      differing by pooling/augs/schedules keyed via cache.\n",
        "    \"\"\"\n",
        "    # 세그 생성\n",
        "    print(\"[STEP] 세그먼트 생성 중 ...\")\n",
        "    infos, labels, groups, summary, missing = build_segments_ships_ear(SHIPSEAR, config)\n",
        "    print(\" - 클래스별 개수:\", dict(summary), \"| 매핑 실패:\", missing)\n",
        "\n",
        "    if len(infos) < 2 or len(set(labels)) < 2:\n",
        "        raise RuntimeError( \"[데이터 부족] 세그먼트 수가 너무 적거나 클래스가 2종 미만입니다. - SHIPSEAR_DRIVE 경로와 하위 폴더/파일명을 재확인하세요.- 라벨 매핑 규칙(resolve_ships_ear_class)과 실제 폴더명이 맞는지 점검하세요.\"\n",
        "        )\n",
        "\n",
        "    n_files = len(set([i[0] for i in infos]))\n",
        "    n_groups = len(set(groups))\n",
        "    total_h = (len(infos)*config[\"seg_dur\"]) / 3600.0\n",
        "    print(f\" - 세그:{len(infos)} | 파일≈{n_files} | 그룹≈{n_groups} | 총길이≈{total_h:.2f} h\")\n",
        "\n",
        "    if BINARY_MODE:\n",
        "        labels_bin = [\"Ship\" if l in [\"A\",\"B\",\"C\",\"D\"] else \"Noise\" for l in labels]\n",
        "        le = LabelEncoder(); y_all = le.fit_transform(labels_bin)\n",
        "    else:\n",
        "        le = LabelEncoder(); y_all = le.fit_transform(labels)\n",
        "    classes = list(le.classes_)\n",
        "    g_arr = np.array(groups)\n",
        "\n",
        "    tr_idx, te_idx, method = stratified_group_split(y_all, g_arr, config[\"test_size\"])\n",
        "    print(f\"[Split] method={method} | train={len(tr_idx)} | test={len(te_idx)} | 그룹수 train/test={len(set(g_arr[tr_idx]))}/{len(set(g_arr[te_idx]))}\")\n",
        "\n",
        "    Xtr_infos = [infos[i] for i in tr_idx]; ytr = y_all[tr_idx]\n",
        "    Xte_infos = [infos[i] for i in te_idx]; yte = y_all[te_idx]\n",
        "    classes = list(le.classes_)\n",
        "\n",
        "    # 임베딩 필요 여부: emb/ft 모두 infer 필요 (ft도 head-only)\n",
        "    yam_infer = None\n",
        "    if any(v[\"type\"] in [\"emb\", \"ft\"] for v in versions):\n",
        "        print(\"YAMNet infer 준비 중 ...\", end=\"\")\n",
        "        yam_infer = make_yamnet_infer()\n",
        "        try:\n",
        "            test_wav = (np.random.randn(YAM_SR).astype(np.float32) * 1e-3)\n",
        "            feat_mean = yamnet_embed(yam_infer, test_wav, pooling=\"mean\")\n",
        "            feat_ms   = yamnet_embed(yam_infer, test_wav, pooling=\"meanstd\")\n",
        "            print(\" [Sanity] mean:\", (None if feat_mean is None else feat_mean.shape),\n",
        "                  \"| meanstd:\", (None if feat_ms is None else feat_ms.shape))\n",
        "        except Exception as e:\n",
        "            print(f\" [Sanity Failed] - {e}\")\n",
        "            yam_infer = None\n",
        "\n",
        "    all_results = []\n",
        "    for v in versions:\n",
        "        print(f\"================= {v['name']} =================\")\n",
        "        pooling = v.get(\"pooling\", \"mean\")\n",
        "\n",
        "        if v[\"type\"] == \"emb\":\n",
        "            if yam_infer is None:\n",
        "                print(f\"YAMNet infer 문제로 임베딩 기반 버전 {v['name']} 건너뜀.\")\n",
        "                continue\n",
        "            aug = v.get(\"aug\", None)\n",
        "            cache_key = f\"{v['name']}_{pooling}_aug{aug}_{config['seg_dur']}s\"\n",
        "\n",
        "            print(\" - 임베딩(Train) 중 ...\", end=\"\")\n",
        "            Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, config, pooling=pooling, aug=aug, cache_key=cache_key+\"_tr\")\n",
        "            ytr_v = ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "            print(\" - 임베딩(Test) 중 ...\", end=\"\")\n",
        "            Xte, keep_te = embed_many(Xte_infos, yam_infer, config, pooling=pooling, aug=None, cache_key=cache_key+\"_te\")\n",
        "            yte_v = yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "            if Xtr.size == 0 or Xte.size == 0:\n",
        "                raise RuntimeError(f\"[{v['name']}] 임베딩 생성 실패\")\n",
        "\n",
        "            res = train_eval_emb(v, Xtr, ytr_v, Xte, yte_v, classes, cfg=config)\n",
        "\n",
        "        elif v[\"type\"] == \"ft\":\n",
        "            if yam_infer is None:\n",
        "                print(f\"YAMNet infer 문제로 FT 버전 {v['name']} 건너뜀.\")\n",
        "                continue\n",
        "            print(\" - (Head-only) FT 버전 실행 중 ... |\", mem())\n",
        "            # Head-only: 동일 임베딩 생성 → 헤드 MLP 학습\n",
        "            aug = v.get(\"aug\", None)\n",
        "            cache_key = f\"{v['name']}_{pooling}_ft_{config['seg_dur']}s\"\n",
        "\n",
        "            print(\" - (Head-only) 임베딩(Train) 생성 중 ...\", end=\"\")\n",
        "            Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, config, pooling=pooling, aug=aug, cache_key=cache_key+\"_tr\")\n",
        "            ytr_v = ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "            print(\" - (Head-only) 임베딩(Test) 생성 중 ...\", end=\"\")\n",
        "            Xte, keep_te = embed_many(Xte_infos, yam_infer, config, pooling=pooling, aug=None, cache_key=cache_key+\"_te\")\n",
        "            yte_v = yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "            if Xtr.size == 0 or Xte.size == 0:\n",
        "                raise RuntimeError(f\"[{v['name']}] 임베딩 생성 실패\")\n",
        "\n",
        "            v_copy = dict(v)\n",
        "            v_copy[\"classifier\"] = \"mlp\"\n",
        "            v_copy[\"type\"] = \"ft\"\n",
        "            res = train_eval_emb(v_copy, Xtr, ytr_v, Xte, yte_v, classes, cfg=config)\n",
        "\n",
        "        else:\n",
        "            print(f\" - 알 수 없는 버전 타입: {v['type']}. 건너뜀.\")\n",
        "            continue\n",
        "\n",
        "        # 공통: 결과 저장/집계\n",
        "        row = dict(\n",
        "            version=v['name'], type=v['type'],\n",
        "            pooling=v.get('pooling','-'), classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'),\n",
        "            aug=(v.get('aug') or \"none\"),\n",
        "            acc=res[\"acc\"], bal_acc=res[\"bal_acc\"], macroF1=res[\"macroF1\"],\n",
        "            macroROC=res[\"macroROC\"], topk=res[\"topk\"],\n",
        "            time_sec=res[\"time_sec\"], artifact=res.get(\"artifact\",\"\")\n",
        "        )\n",
        "        all_results.append((row, res))\n",
        "\n",
        "        # 혼동행렬 저장\n",
        "        cm = res[\"cm\"]\n",
        "        plt.figure(figsize=(5.5,4.8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.xlabel(\"예측\"); plt.ylabel(\"실제\"); plt.title(f\"CM — {v['name']}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"results/cm_{v['name']}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # AP per class 저장\n",
        "        with open(f\"results/ap_{v['name']}.json\",\"w\") as f:\n",
        "            json.dump(res[\"ap_per_class\"], f, indent=2)\n",
        "\n",
        "    # 요약 표\n",
        "    df = pd.DataFrame([r[0] for r in all_results])\n",
        "    if not df.empty:\n",
        "        df_sorted = df.sort_values([\"macroF1\",\"bal_acc\",\"acc\"], ascending=False)\n",
        "        df_sorted.to_csv(\"results/summary.csv\", index=False)\n",
        "        print(\"[SUMMARY — V5~V8]\")\n",
        "        print(df_sorted.to_string(index=False))\n",
        "\n",
        "        with open(\"results/report.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\" Ship vs Noise — V5~V8 비교 요약\")\n",
        "            f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\")\n",
        "            f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\")\n",
        "            for _,row in df_sorted.iterrows():\n",
        "                f.write(\n",
        "                    f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|\"\n",
        "                    f\"{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{(np.nan if pd.isna(row['macroROC']) else row['macroROC']):.4f}|\"\n",
        "                    f\"{(np.nan if pd.isna(row['topk']) else row['topk']):.4f}|{row['time_sec']:.1f}|{row['artifact']}|\"\n",
        "                )\n",
        "            f.write(\"- 혼동행렬: results/cm_*.png - AP per class: results/ap_*json\")\n",
        "    else:\n",
        "        print(\"No results to summarize (check data path & pipeline).\")\n",
        "\n",
        "    print(\"결과 파일:\")\n",
        "    print(\" - results/summary.csv\")\n",
        "    print(\" - results/report.md\")\n",
        "    print(\" - results/cm_*.png\")\n",
        "    print(\" - results/ap_*.json\")\n",
        "    print(\" - artifacts/* (모델)\")\n",
        "\n",
        "# 실행\n",
        "run_all(BASE_CONFIG, VERSIONS)\n",
        "print(\"\\n🎉 완료 — 버전별 결과는 results/summary.csv, results/report.md, cm_*.png, artifacts/* 에 저장됩니다.\")\n"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963b14be-b3e9-45a4-c07d-87955fa77756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) 환경설정/설치 중 ...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            " - GPU found: 1 | memory growth enabled\n",
            " - 폰트 OK: NanumGothic\n",
            "\n",
            "2) 데이터 확보 중 ...\n",
            " - ShipsEar 이미 존재\n",
            "[STEP] 세그먼트 생성 중 ...\n",
            " - 클래스별 개수: {'C': 5101, 'B': 3139, 'E': 1140, 'A': 1856, 'D': 1513} | 매핑 실패: 0\n",
            " - 세그:12749 | 파일≈85 | 그룹≈85 | 총길이≈3.54 h\n",
            "[Split] method=StratifiedGroupKFold(n_splits=5) | train=9789 | test=2960 | 그룹수 train/test=69/16\n",
            "YAMNet infer 준비 중 ...[YAMNet] backend=hub.load\n",
            " [Sanity] mean: (1024,) | meanstd: (2048,)\n",
            "================= v5_meanstd_mlp_aug =================\n",
            " - 임베딩(Train) 중 ...  ... 5000/9789 | RSS≈3.92 GB\n",
            " - 캐시 저장: cache/emb_v5_meanstd_mlp_aug_meanstd_auglight_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS≈4.04 GB\n",
            " - 임베딩(Test) 중 ... - 캐시 저장: cache/emb_v5_meanstd_mlp_aug_meanstd_auglight_1.0s_te.npz\n",
            " OK (2960, 2048) | RSS≈4.04 GB\n",
            "================= v6_ft_mean_headwarmup_unfreeze =================\n",
            " - (Head-only) FT 버전 실행 중 ... | RSS≈4.24 GB\n",
            " - (Head-only) 임베딩(Train) 생성 중 ...  ... 5000/9789 | RSS≈4.26 GB\n",
            " - 캐시 저장: cache/emb_v6_ft_mean_headwarmup_unfreeze_mean_ft_1.0s_tr.npz\n",
            " OK (9789, 1024) | RSS≈4.21 GB\n",
            " - (Head-only) 임베딩(Test) 생성 중 ... - 캐시 저장: cache/emb_v6_ft_mean_headwarmup_unfreeze_mean_ft_1.0s_te.npz\n",
            " OK (2960, 1024) | RSS≈4.22 GB\n",
            "================= v7_ft_meanstd_headwarmup_unfreeze =================\n",
            " - (Head-only) FT 버전 실행 중 ... | RSS≈4.22 GB\n",
            " - (Head-only) 임베딩(Train) 생성 중 ...  ... 5000/9789 | RSS≈4.25 GB\n",
            " - 캐시 저장: cache/emb_v7_ft_meanstd_headwarmup_unfreeze_meanstd_ft_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS≈4.31 GB\n",
            " - (Head-only) 임베딩(Test) 생성 중 ... - 캐시 저장: cache/emb_v7_ft_meanstd_headwarmup_unfreeze_meanstd_ft_1.0s_te.npz\n",
            " OK (2960, 2048) | RSS≈4.31 GB\n",
            "================= v8_ft_meanstd_fullft_tinyLR =================\n",
            " - (Head-only) FT 버전 실행 중 ... | RSS≈4.29 GB\n",
            " - (Head-only) 임베딩(Train) 생성 중 ...  ... 5000/9789 | RSS≈4.31 GB\n",
            " - 캐시 저장: cache/emb_v8_ft_meanstd_fullft_tinyLR_meanstd_ft_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS≈4.31 GB\n",
            " - (Head-only) 임베딩(Test) 생성 중 ..."
          ]
        }
      ]
    }
  ]
}