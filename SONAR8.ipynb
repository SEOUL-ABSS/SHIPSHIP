{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "history_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb",
      "authorship_tag": "ABX9TyMI6ItAoLhFE7DrgoZ2ngb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#     ShipsEar Ship vs Noise ‚Äî V5~V8 Only (RAM‚Äësafe Fine‚ÄëTuning + Report)\n",
        "#     - Removed V1~V4. Keep V5 (emb) + V6~V8 (fine‚Äëtune) only\n",
        "#     - Major RAM fixes for V6+ (YAMNet map_fn returns pooled vectors, not (B,T,1024))\n",
        "#     - No layer creation inside Layer.call; deterministic tf.data prefetch; GPU mem‚Äëgrowth\n",
        "#     - Summary CSV + per‚Äëversion CM + AP JSON + pretty report printed at the end\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"1) ÌôòÍ≤ΩÏÑ§Ï†ï/ÏÑ§Ïπò Ï§ë ...\")\n",
        "!pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1\n",
        "!pip -q install librosa==0.10.2.post1 soundfile==0.12.1 umap-learn==0.5.6 scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2\n",
        "\n",
        "# (ColabÏùº Îïå) Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Drive mounted.\")\n",
        "except Exception as e:\n",
        "    print(\"ColabÏù¥ ÏïÑÎãàÎùºÎ©¥ Î¨¥Ïãú:\", e)\n",
        "\n",
        "# (ÏÑ†ÌÉù) ÌïúÍ∏Ä Ìè∞Ìä∏\n",
        "!apt -yq install fonts-nanum >/dev/null\n",
        "\n",
        "# ------------------------- Imports & Setup ------------------------------------\n",
        "import os, re, sys, random, math, gc, time, warnings, shutil, glob, json\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import scipy.signal as spsig\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
        "                             roc_auc_score, average_precision_score,\n",
        "                             balanced_accuracy_score, top_k_accuracy_score, accuracy_score)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"]=str(SEED)\n",
        "\n",
        "# GPU Î©îÎ™®Î¶¨ ÏÑ±Ïû• ÌóàÏö© (VRAM OOM Î∞©ÏßÄ)\n",
        "try:\n",
        "    gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    if gpus: print(f\" - GPU found: {len(gpus)} | memory growth enabled\")\n",
        "except Exception as e:\n",
        "    print(\" - GPU memory growth set failed (ok):\", e)\n",
        "\n",
        "def mem(): return f\"RSS‚âà{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# Ìè∞Ìä∏\n",
        "if os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "    print(\" - Ìè∞Ìä∏ OK: NanumGothic\")\n",
        "\n",
        "# ------------------------- Paths & Config -------------------------------------\n",
        "BASE=\"/content\"\n",
        "SHIPSEAR_DRIVE=\"/content/drive/MyDrive/ShipsEar\"   # ‚Üê ÌïÑÏöîÏãú ÏàòÏ†ï\n",
        "SHIPSEAR=f\"{BASE}/ShipsEar_colab\"\n",
        "os.makedirs(\"results\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "YAM_SR=16000\n",
        "\n",
        "# ---- Binary mode (Ship vs Noise) switch ----\n",
        "BINARY_MODE = True\n",
        "POS_LABEL = \"Ship\"\n",
        "# For binary classification, Top-K is not meaningful; keep for compatibility\n",
        "BASE_CONFIG[\"topk\"] = 1\n",
        "BASE_CONFIG=dict(\n",
        "    seg_dur=1.0,               # 5Ï¥à\n",
        "    ship_overlap=0.2,          # A‚ÄìD overlap ÎπÑÏú®(0.2‚Üístride=4s)\n",
        "    noise_overlap=0.0,         # E Ï§ëÎ≥µ ÏµúÏÜåÌôî\n",
        "    vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "    test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "    spec_per_class=2,\n",
        "    umap_max_points=2000,\n",
        "    max_seg_per_group_per_class=500,\n",
        "    noise_jitter_sec=0.5,\n",
        "    topk=2,\n",
        "    cache_emb=True,            # ÏûÑÎ≤†Îî© Ï∫êÏãú\n",
        ")\n",
        "\n",
        "# ------------------------- Î≤ÑÏ†Ñ Ï†ïÏùò (V5~V8Îßå Ïú†ÏßÄ) --------------------------\n",
        "# type: 'emb' (ÏÇ¨Ï†Ñ ÏûÑÎ≤†Îî©+Ìó§Îìú) | 'ft' (end-to-end fine-tuning)\n",
        "VERSIONS = [\n",
        "    # ----- (A) ÏûÑÎ≤†Îî© Í∏∞Î∞ò -----\n",
        "    dict(name=\"v5_meanstd_mlp_aug\", type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "\n",
        "    # ----- (B) Î∂ÄÎ∂Ñ ÌååÏù∏ÌäúÎãù (RAM‚Äësafe) -----\n",
        "    dict(name=\"v6_ft_mean_headwarmup_unfreeze\",    type=\"ft\", pooling=\"mean\",\n",
        "         warmup_epochs=5, ft_epochs=10, base_lr=3e-4, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headwarmup_unfreeze\", type=\"ft\", pooling=\"meanstd\",\n",
        "         warmup_epochs=5, ft_epochs=10, base_lr=3e-4, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_fullft_tinyLR\",       type=\"ft\", pooling=\"meanstd\",\n",
        "         warmup_epochs=0, ft_epochs=12, base_lr=1e-5, ft_lr=1e-5, batch_ft=8, aug=\"light\"),\n",
        "]\n",
        "\n",
        "# ======================================================================\n",
        "# 2) Îç∞Ïù¥ÌÑ∞ ÌôïÎ≥¥\n",
        "# ======================================================================\n",
        "print(\"\\n2) Îç∞Ïù¥ÌÑ∞ ÌôïÎ≥¥ Ï§ë ...\")\n",
        "if os.path.exists(SHIPSEAR_DRIVE):\n",
        "    if not os.path.exists(SHIPSEAR) or not os.listdir(SHIPSEAR):\n",
        "        shutil.copytree(SHIPSEAR_DRIVE, SHIPSEAR, dirs_exist_ok=True)\n",
        "        print(\" - ShipsEar Î≥µÏÇ¨ ÏôÑÎ£å\")\n",
        "    else:\n",
        "        print(\" - ShipsEar Ïù¥ÎØ∏ Ï°¥Ïû¨\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\" - ShipsEar ÎìúÎùºÏù¥Î∏å Í≤ΩÎ°ú ÏóÜÏùå: {SHIPSEAR_DRIVE}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3) ÎùºÎ≤® Îß§Ìïë & Í∑∏Î£π ÌÇ§\n",
        "# ======================================================================\n",
        "A_kw = [\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"]\n",
        "B_kw = [\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"]\n",
        "C_kw = [\"ferry\",\"passenger\"]\n",
        "D_kw = [\"oceanliner\",\"ocean liner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"]\n",
        "E_kw = [\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]\n",
        "\n",
        "def resolve_ships_ear_class(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    parent = os.path.basename(os.path.dirname(path)).lower()\n",
        "    txt = f\"{parent} {name}\"\n",
        "    def has_any(txt, kws): return any(k in txt for k in kws)\n",
        "    if has_any(txt, E_kw): return \"E\"\n",
        "    if has_any(txt, A_kw): return \"A\"\n",
        "    if has_any(txt, B_kw): return \"B\"\n",
        "    if has_any(txt, C_kw): return \"C\"\n",
        "    if has_any(txt, D_kw): return \"D\"\n",
        "    m = re.search(r'\\bclass[_\\s-]*([abcde])\\b', txt)\n",
        "    if m: return m.group(1).upper()\n",
        "    return None\n",
        "\n",
        "def ships_ear_group_key(path):\n",
        "    base = os.path.basename(path)\n",
        "    stem = os.path.splitext(base)[0]\n",
        "    m = re.search(r'(\\d{8}[_-]?\\d{4})', stem) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', stem)\n",
        "    if m: return m.group(1)\n",
        "    parent = os.path.basename(os.path.dirname(path))\n",
        "    toks = re.split(r'[_\\-]+', stem)\n",
        "    prefix = \"_\".join(toks[:3]) if len(toks)>=3 else stem\n",
        "    return f\"{parent}:{prefix}\"\n",
        "\n",
        "# ======================================================================\n",
        "# 4) VAD & ÏÑ∏Í∑∏ ÏÉùÏÑ±\n",
        "# ======================================================================\n",
        "EPS=1e-12\n",
        "\n",
        "def get_activity_intervals_streaming(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr=f.samplerate; n=len(f)\n",
        "            F=max(1,int(round(frame_sec*sr))); H=max(1,int(round(hop_sec*sr)))\n",
        "            # pass1: ÏµúÎåÄ dB\n",
        "            max_db=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS)\n",
        "                db=20*np.log10(rms+EPS)\n",
        "                if db>max_db: max_db=db\n",
        "                pos+=H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "            th = max_db - top_db\n",
        "            # pass2: Î≥ëÌï©\n",
        "            active=[]; in_act=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False)\n",
        "                if y.ndim>1: y=y.mean(axis=1)\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); db=20*np.log10(rms+EPS)\n",
        "                t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not in_act: in_act=True; cur=t0\n",
        "                else:\n",
        "                    if in_act: in_act=False; active.append((cur,t1))\n",
        "                pos+=H\n",
        "            if in_act: active.append((cur,n/sr))\n",
        "            # (Ï∞∏Í≥†) ÎπÑÌôúÏÑ±\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s))\n",
        "                last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except Exception:\n",
        "        return [], []\n",
        "\n",
        "def slice_spans_to_segments(spans, seg_dur, hop):\n",
        "    segs=[]\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st=s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            segs.append((float(st),))\n",
        "            st += hop\n",
        "    return segs\n",
        "\n",
        "def build_segments_ships_ear(root, cfg):\n",
        "    seg_dur=cfg[\"seg_dur\"]\n",
        "    hop_ship = seg_dur*(1-cfg[\"ship_overlap\"])\n",
        "    hop_noise= seg_dur*(1-cfg[\"noise_overlap\"])\n",
        "    noise_jitter=cfg.get(\"noise_jitter_sec\", 0.0)\n",
        "    cap=cfg.get(\"max_seg_per_group_per_class\", None)\n",
        "\n",
        "    infos=[]; labels=[]; groups=[]\n",
        "    missing=0\n",
        "    per_gc_count=defaultdict(int)\n",
        "    summary = defaultdict(int)\n",
        "\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        cls = resolve_ships_ear_class(fp)\n",
        "        if cls is None:\n",
        "            missing+=1; continue\n",
        "        try:\n",
        "            info=sf.info(fp)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        gkey = ships_ear_group_key(fp)\n",
        "        if cls in [\"A\",\"B\",\"C\",\"D\"]:\n",
        "            act,_ = get_activity_intervals_streaming(fp, top_db=cfg[\"vad_top_db\"],\n",
        "                                                     frame_sec=cfg[\"vad_frame_sec\"], hop_sec=cfg[\"vad_hop_sec\"])\n",
        "            spans = act; hop = hop_ship\n",
        "        else: # E\n",
        "            dur = info.frames/info.samplerate\n",
        "            spans = [(0.0, dur)]; hop = hop_noise\n",
        "\n",
        "        segs = slice_spans_to_segments(spans, seg_dur, hop)\n",
        "        random.shuffle(segs)\n",
        "\n",
        "        for (st,) in segs:\n",
        "            if cls == \"E\" and noise_jitter>0:\n",
        "                j = random.uniform(-noise_jitter, noise_jitter)\n",
        "                st = max(0.0, min(st + j, (info.frames/info.samplerate) - seg_dur))\n",
        "            key=(gkey, cls)\n",
        "            if cap is not None and per_gc_count[key] >= cap:\n",
        "                continue\n",
        "            infos.append((fp, float(st), info.samplerate))\n",
        "            labels.append(cls)\n",
        "            groups.append(gkey)\n",
        "            per_gc_count[key]+=1\n",
        "            summary[cls]+=1\n",
        "\n",
        "    return infos, labels, groups, summary, missing\n",
        "\n",
        "# ======================================================================\n",
        "# 5) Ïò§ÎîîÏò§ Î°úÎìú/Ï¶ùÍ∞ï/ÏûÑÎ≤†Îî© (‚òÖ YAMNet Ìå®Ïπò Ìè¨Ìï®)\n",
        "# ======================================================================\n",
        "\n",
        "def load_segment(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, start_time, orig_sr = info\n",
        "    try:\n",
        "        start=int(start_time*orig_sr); num=int(seg_dur*orig_sr)\n",
        "        with sf.SoundFile(fp, 'r') as f:\n",
        "            actual_num_frames = f.frames - start\n",
        "            if actual_num_frames <= 0:\n",
        "                print(f\"WARN: Segment start ({start_time:.2f}s) beyond file end for {os.path.basename(fp)}\")\n",
        "                return None\n",
        "            num = min(num, actual_num_frames)\n",
        "        y,_=sf.read(fp, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y is None: return None\n",
        "        if y.ndim>1: y=y.mean(axis=1)\n",
        "        if orig_sr!=target_sr:\n",
        "            y = safe_resample(y, orig_sr, target_sr)\n",
        "        if y is None:\n",
        "            print(f\"WARN: Resampling returned None for {os.path.basename(fp)}\"); return None\n",
        "        if rms_norm:\n",
        "            rms=np.sqrt(np.mean(y**2))+1e-12\n",
        "            y *= (10**(-20/20))/rms  # -20 dBFS\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to load segment {info} - {e}\")\n",
        "        return None\n",
        "\n",
        "def augment_wave(y, sr, kind=\"light\"):\n",
        "    if y is None: return None\n",
        "    if kind==\"light\":\n",
        "        g_db = random.uniform(-3, 3)\n",
        "        y = y * (10**(g_db/20))\n",
        "        max_shift = int(0.25*sr)\n",
        "        sh = random.randint(-max_shift, max_shift)\n",
        "        if sh>0:\n",
        "            y = np.concatenate([np.zeros(sh, dtype=y.dtype), y[:-sh]])\n",
        "        elif sh<0:\n",
        "            y = np.concatenate([y[-sh:], np.zeros(-sh, dtype=y.dtype)])\n",
        "    return y\n",
        "\n",
        "# ------------------------- Resampling helper (avoid resampy) ------------------\n",
        "# Uses SciPy's polyphase resampler when available; otherwise falls back to\n",
        "# librosa's FFT resampler (no resampy), and finally to linear interpolation.\n",
        "try:\n",
        "    import scipy.signal as _spsig_internal\n",
        "except Exception:\n",
        "    _spsig_internal = None\n",
        "\n",
        "def safe_resample(y, orig_sr, target_sr):\n",
        "    if orig_sr == target_sr:\n",
        "        return y\n",
        "    try:\n",
        "        if _spsig_internal is not None:\n",
        "            g = math.gcd(int(orig_sr), int(target_sr))\n",
        "            up = int(target_sr)//g; down = int(orig_sr)//g\n",
        "            return _spsig_internal.resample_poly(y, up, down).astype(np.float32)\n",
        "        # Fallback: librosa FFT backend (does not require resampy)\n",
        "        return librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"fft\").astype(np.float32)\n",
        "    except Exception:\n",
        "        # Last resort: linear interpolation\n",
        "        new_len = int(round(len(y) * float(target_sr) / float(orig_sr)))\n",
        "        xp = np.arange(len(y))\n",
        "        x_new = np.linspace(0, len(y), new_len, endpoint=False)\n",
        "        return np.interp(x_new, xp, y).astype(np.float32)\n",
        "\n",
        "YAM_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "def make_yamnet_infer():\n",
        "    \"\"\"hub.load ‚Üí Ïã§Ìå® Ïãú hub.KerasLayer Ìè¥Î∞±. infer(y: 1D float32) -> raw outputs\"\"\"\n",
        "    try:\n",
        "        module = hub.load(YAM_URL)\n",
        "        def infer(y):\n",
        "            y = tf.convert_to_tensor(y, tf.float32)     # (N,)\n",
        "            return module(y)                            # tuple or dict\n",
        "        _ = infer(np.zeros(16000, np.float32))\n",
        "        print(\"[YAMNet] backend=hub.load\")\n",
        "        return infer\n",
        "    except Exception as e1:\n",
        "        print(\"[YAMNet] hub.load failed ‚Üí fallback to KerasLayer:\", repr(e1))\n",
        "        layer = hub.KerasLayer(YAM_URL, trainable=False)\n",
        "        def infer(y):\n",
        "            y = tf.convert_to_tensor(y, tf.float32)\n",
        "            try:\n",
        "                return layer(y)                         # ÏùºÎ∂Ä ÌôòÍ≤ΩÏóêÏÑ† Î∞îÎ°ú ÎèôÏûë\n",
        "            except Exception:\n",
        "                return layer(tf.expand_dims(y, 0))      # Î∞∞Ïπò Ï∞®Ïõê Í∞ïÏ†ú (1, N)\n",
        "        _ = infer(np.zeros(16000, np.float32))\n",
        "        print(\"[YAMNet] backend=KerasLayer\")\n",
        "        return infer\n",
        "\n",
        "def _extract_embeddings_from_output(out):\n",
        "    emb = None\n",
        "    if isinstance(out, (list, tuple)):\n",
        "        if len(out) >= 2: emb = out[1]\n",
        "    elif isinstance(out, dict):\n",
        "        emb = out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "        if emb is None:\n",
        "            for v in out.values():\n",
        "                if isinstance(v, dict):\n",
        "                    emb = v.get(\"embeddings\") or v.get(\"embedding\")\n",
        "                    if emb is not None:\n",
        "                        break\n",
        "    if emb is None:\n",
        "        return None\n",
        "\n",
        "    emb = tf.convert_to_tensor(emb)\n",
        "    if emb.shape.rank == 3 and emb.shape[0] == 1:\n",
        "        emb = tf.squeeze(emb, axis=0)\n",
        "    if emb.shape.rank == 1:\n",
        "        emb = tf.expand_dims(emb, 0)\n",
        "    return emb  # (T, 1024)\n",
        "\n",
        "def yamnet_embed(infer, y, pooling=\"meanstd\"):\n",
        "    if y is None:\n",
        "        return None\n",
        "    try:\n",
        "        out = infer(y)\n",
        "        emb = _extract_embeddings_from_output(out)\n",
        "        if emb is None or emb.shape.rank != 2 or int(emb.shape[0]) == 0:\n",
        "            return None\n",
        "        if pooling == \"mean\":\n",
        "            feat = tf.reduce_mean(emb, axis=0)\n",
        "        elif pooling == \"meanstd\":\n",
        "            m = tf.reduce_mean(emb, axis=0)\n",
        "            s = tf.math.reduce_std(emb, axis=0)\n",
        "            feat = tf.concat([m, s], axis=0)\n",
        "        else:\n",
        "            raise ValueError(\"pooling must be 'mean' or 'meanstd'\")\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to embed waveform - {e}\")\n",
        "        return None\n",
        "\n",
        "def embed_many(infos, yam_infer, cfg, pooling=\"mean\", aug=None, cache_key=None, show_every=5000):\n",
        "    cache_path = None\n",
        "    if cfg.get(\"cache_emb\", True) and cache_key:\n",
        "        cache_path = os.path.join(\"cache\", f\"emb_{cache_key}.npz\")\n",
        "        if os.path.exists(cache_path):\n",
        "            try:\n",
        "                z=np.load(cache_path, allow_pickle=True)\n",
        "                print(f\" - Ï∫êÏãú Î°úÎìú: {cache_path} | X:{z['X'].shape} | keep:{z['keep'].shape}\")\n",
        "                return z[\"X\"], z[\"keep\"]\n",
        "            except Exception as e:\n",
        "                print(f\"WARN: Ï∫êÏãú Î°úÎìú Ïã§Ìå® {cache_path} - {e}. Ïû¨ÏÉùÏÑ±Ìï©ÎãàÎã§.\")\n",
        "                if os.path.exists(cache_path): os.remove(cache_path)\n",
        "\n",
        "    X=[]; keep=[]\n",
        "    for i,info in enumerate(infos,1):\n",
        "        y=load_segment(info, cfg[\"seg_dur\"], YAM_SR, rms_norm=True)\n",
        "        if aug:\n",
        "            y=augment_wave(y, YAM_SR, kind=aug)\n",
        "        e=yamnet_embed(yam_infer, y, pooling=pooling)\n",
        "        if e is not None:\n",
        "            X.append(e); keep.append(i-1)\n",
        "        if i%show_every==0:\n",
        "            print(f\"  ... {i}/{len(infos)} | {mem()}\")\n",
        "\n",
        "    X=np.asarray(X, np.float32)\n",
        "    keep=np.array(keep, np.int64)\n",
        "\n",
        "    if X.size == 0:\n",
        "        print(f\"ERROR: Failed to generate any embeddings for {len(infos)} segments.\")\n",
        "\n",
        "    if cache_path is not None and X.size > 0:\n",
        "        try:\n",
        "            np.savez_compressed(cache_path, X=X, keep=keep)\n",
        "            print(f\" - Ï∫êÏãú Ï†ÄÏû•: {cache_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARN: Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå® {cache_path} - {e}\")\n",
        "\n",
        "    return X, keep\n",
        "\n",
        "# ======================================================================\n",
        "# 6) Î∂ÑÌï†(Í∞ÄÎä•ÌïòÎ©¥ Í∑∏Î£π-Í≥ÑÏ∏µ)\n",
        "# ======================================================================\n",
        "\n",
        "def stratified_group_split(y, groups, test_size=0.2, seed=SEED):\n",
        "    n=len(y)\n",
        "    if n < 2:\n",
        "        raise RuntimeError(\"[Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±] ÏÑ∏Í∑∏Î®ºÌä∏Í∞Ä 2Í∞ú ÎØ∏ÎßåÏûÖÎãàÎã§.\")\n",
        "    uniq_groups = len(np.unique(groups))\n",
        "    counts = np.bincount(y) if len(y)>0 else np.array([])\n",
        "    min_per_class = int(counts[counts > 0].min()) if counts.size else 0\n",
        "    desired = max(2, int(round(1.0 / max(1e-9, test_size))))\n",
        "    n_splits = max(2, min(desired, uniq_groups, max(2, min_per_class)))\n",
        "\n",
        "    try:\n",
        "        from sklearn.model_selection import StratifiedGroupKFold\n",
        "        sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "        tr_idx, te_idx = next(sgkf.split(np.zeros(n), y, groups))\n",
        "        method=f\"StratifiedGroupKFold(n_splits={n_splits})\"\n",
        "    except Exception:\n",
        "        gss=GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "        tr_idx, te_idx = next(gss.split(np.arange(n), y, groups))\n",
        "        method=\"GroupShuffleSplit\"\n",
        "    return tr_idx, te_idx, method\n",
        "\n",
        "# ======================================================================\n",
        "# 7) (A) ÏûÑÎ≤†Îî© Í∏∞Î∞ò Î∂ÑÎ•òÍ∏∞(MLP/LogReg/SVM)\n",
        "# ======================================================================\n",
        "\n",
        "def build_mlp(input_dim, num_classes, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    inp=tf.keras.Input(shape=(input_dim,), name=\"emb\")\n",
        "    x=tf.keras.layers.BatchNormalization()(inp)\n",
        "    x=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(x); x=tf.keras.layers.Dropout(0.5)(x)\n",
        "    x=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(x); x=tf.keras.layers.Dropout(0.4)(x)\n",
        "    out=tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    m=tf.keras.Model(inp,out)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def train_eval_emb(version, Xtr, ytr, Xte, yte, classes, cfg):\n",
        "    res={}\n",
        "    if Xtr.size == 0 or Xte.size == 0 or Xtr.ndim != 2 or Xte.ndim != 2:\n",
        "        raise RuntimeError(\"[ÏûÑÎ≤†Îî© Ïã§Ìå®] Xtr/XteÍ∞Ä ÎπÑÏñ¥ÏûàÍ±∞ÎÇò Ï∞®ÏõêÏù¥ ÏûòÎ™ªÎêòÏóàÏäµÎãàÎã§.\")\n",
        "    if version[\"classifier\"]==\"mlp\":\n",
        "        clf=build_mlp(Xtr.shape[-1], len(classes), lr=cfg[\"lr\"])\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6),\n",
        "        ]\n",
        "        ytr_cat=tf.keras.utils.to_categorical(ytr, num_classes=len(classes))\n",
        "        yte_cat=tf.keras.utils.to_categorical(yte, num_classes=len(classes))\n",
        "        cnt_tr=Counter(ytr); total=sum(cnt_tr.values())\n",
        "        class_weight={cls: total/(len(cnt_tr)*cnt) for cls,cnt in cnt_tr.items()}\n",
        "        t0=time.time()\n",
        "        clf.fit(Xtr, ytr_cat, validation_data=(Xte, yte_cat),\n",
        "                epochs=cfg[\"epochs\"], batch_size=cfg[\"batch\"], verbose=0,\n",
        "                class_weight=class_weight, callbacks=callbacks)\n",
        "        probs=clf.predict(Xte, verbose=0); pred=probs.argmax(1)\n",
        "        model_path=f\"artifacts/{version['name']}_mlp.keras\"; clf.save(model_path)\n",
        "        res[\"artifact\"]=model_path; res[\"time_sec\"]=time.time()-t0\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.svm import SVC\n",
        "        scaler=StandardScaler().fit(Xtr)\n",
        "        Xtr_s=scaler.transform(Xtr); Xte_s=scaler.transform(Xte)\n",
        "        t0=time.time()\n",
        "        if version[\"classifier\"]==\"logreg\":\n",
        "            clf=LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\n",
        "            clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        else:\n",
        "            clf=SVC(C=2.0, kernel='rbf', probability=True, class_weight='balanced')\n",
        "            clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        res[\"time_sec\"]=time.time()-t0\n",
        "        import joblib\n",
        "        model_path=f\"artifacts/{version['name']}_{version['classifier']}.joblib\"\n",
        "        scaler_path=f\"artifacts/{version['name']}_scaler.joblib\"\n",
        "        joblib.dump(clf, model_path); joblib.dump(scaler, scaler_path)\n",
        "        res[\"artifact\"]=model_path; res[\"scaler\"]=scaler_path\n",
        "\n",
        "    true=yte\n",
        "    res[\"acc\"]=accuracy_score(true, pred)\n",
        "    res[\"bal_acc\"]=balanced_accuracy_score(true, pred)\n",
        "    res[\"macroF1\"]=f1_score(true, pred, average='macro')\n",
        "    try:\n",
        "        yte_cat=tf.keras.utils.to_categorical(true, num_classes=len(classes))\n",
        "        res[\"macroROC\"]=roc_auc_score(yte_cat, probs, average='macro', multi_class='ovr')\n",
        "    except Exception:\n",
        "        res[\"macroROC\"]=np.nan\n",
        "    try:\n",
        "        res[\"topk\"]=top_k_accuracy_score(true, probs, k=BASE_CONFIG[\"topk\"], labels=range(len(classes)))\n",
        "    except Exception:\n",
        "        res[\"topk\"]=np.nan\n",
        "    ap={}\n",
        "    for i,lab in enumerate(classes):\n",
        "        y_bin=(true==i).astype(int)\n",
        "        if 0<y_bin.sum()<len(y_bin): ap[lab]=float(average_precision_score(y_bin, probs[:,i]))\n",
        "        else: ap[lab]=float(\"nan\")\n",
        "    res[\"ap_per_class\"]=ap\n",
        "    res[\"cm\"]=confusion_matrix(true, pred)\n",
        "    # Binary metrics override (if applicable)\n",
        "    if len(classes) == 2:\n",
        "        try:\n",
        "            pos_idx = classes.index(POS_LABEL) if POS_LABEL in classes else 1\n",
        "            res[\"macroROC\"] = roc_auc_score(true, probs[:, pos_idx])\n",
        "        except Exception:\n",
        "            res[\"macroROC\"] = np.nan\n",
        "        res[\"topk\"] = np.nan\n",
        "    return res\n",
        "\n",
        "# ======================================================================\n",
        "# 8) (B) Î∂ÄÎ∂Ñ ÌååÏù∏ÌäúÎãù ‚Äî RAM‚Äësafe tf.map_fn (returns fixed D per sample)\n",
        "# ======================================================================\n",
        "\n",
        "def make_wave_ds(infos, y_enc, cfg, batch, shuffle=False, aug=None):\n",
        "    L=int(YAM_SR*cfg[\"seg_dur\"])\n",
        "\n",
        "    def gen():\n",
        "        for info, y in zip(infos, y_enc):\n",
        "            wav=load_segment(info, cfg[\"seg_dur\"], YAM_SR, rms_norm=True)\n",
        "            if aug: wav=augment_wave(wav, YAM_SR, kind=aug)\n",
        "            if wav is None: continue\n",
        "            if len(wav)<L:\n",
        "                pad=np.zeros(L, dtype=np.float32); pad[:len(wav)]=wav; wav=pad\n",
        "            elif len(wav)>L:\n",
        "                wav=wav[:L]\n",
        "            yield wav.astype(np.float32), np.int32(y)\n",
        "\n",
        "    ds=tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(L,), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds=ds.shuffle(buffer_size=min(8000, len(infos)))\n",
        "    # Î©îÎ™®Î¶¨ ÏïàÏ†Ñ: Í≥ºÎèÑÌïú ÌîÑÎ¶¨Ìå®Ïπò Î∞©ÏßÄ\n",
        "    ds=ds.batch(batch, drop_remainder=False).prefetch(2)\n",
        "    return ds\n",
        "\n",
        "class YamnetEmbeddingLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"RAM‚Äësafe: map_fn returns per‚Äësample pooled vector (1024 or 2048),\n",
        "    avoiding (B,T,1024) materialization. Also defines output shapes for Keras.\"\"\"\n",
        "    def __init__(self, yamnet_url, pooling=\"mean\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if pooling not in [\"mean\", \"meanstd\"]:\n",
        "            raise ValueError(\"pooling must be 'mean' or 'meanstd'\")\n",
        "        self.yamnet_url = yamnet_url\n",
        "        self.pooling = pooling\n",
        "        # YAMNet from TF‚ÄëHub is effectively non‚Äëtrainable (frozen graph); keep False to avoid warnings\n",
        "        self.yamnet_layer = hub.KerasLayer(self.yamnet_url, trainable=False, name=\"yamnet_base\")\n",
        "        self.out_dim = 1024 if pooling==\"mean\" else 2048\n",
        "\n",
        "    def _pool_single(self, waveform):\n",
        "        # waveform: (N,)\n",
        "        out = self.yamnet_layer(waveform)\n",
        "        # extract embeddings\n",
        "        if isinstance(out, (list, tuple)) and len(out) > 1:\n",
        "            emb = out[1]  # (T,1024) expected\n",
        "        elif isinstance(out, dict):\n",
        "            emb = out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "            if emb is None and len(out) > 0:\n",
        "                # fallback: take the last value\n",
        "                emb = list(out.values())[-1]\n",
        "        else:\n",
        "            raise RuntimeError(\"Unexpected YAMNet output format\")\n",
        "        emb = tf.convert_to_tensor(emb)\n",
        "        # If a batch dim accidentally appears, drop it using safe gather (no Squeeze)\n",
        "        rank = tf.rank(emb)\n",
        "        emb = tf.cond(tf.equal(rank, 3), lambda: emb[0], lambda: emb)  # (T,1024)\n",
        "        # pool to fixed size\n",
        "        m = tf.reduce_mean(emb, axis=0)  # (1024,)\n",
        "        if self.pooling == \"mean\":\n",
        "            return m\n",
        "        s = tf.math.reduce_std(emb, axis=0)  # (1024,)\n",
        "        return tf.concat([m, s], axis=0)     # (2048,)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (B, N)\n",
        "        outputs = []\n",
        "        for i in tf.range(tf.shape(inputs)[0]):\n",
        "            w = inputs[i]\n",
        "            feat = self._pool_single(w)\n",
        "            outputs.append(tf.expand_dims(feat,0))\n",
        "        return tf.concat(outputs, axis=0)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # input_shape = (B, N)\n",
        "        return (input_shape[0], self.out_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"yamnet_url\": self.yamnet_url, \"pooling\": self.pooling})\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_yamnet_ft_model(num_classes, pooling=\"meanstd\", lr=3e-4):\n",
        "    wave_in = tf.keras.Input(shape=(int(YAM_SR*BASE_CONFIG[\"seg_dur\"]),), dtype=tf.float32, name=\"wave\")\n",
        "    embedding_layer = YamnetEmbeddingLayer(yamnet_url=YAM_URL, pooling=pooling, name=\"yamnet_embedding\")\n",
        "    feat = embedding_layer(wave_in)  # (B, D)\n",
        "\n",
        "    x=tf.keras.layers.BatchNormalization()(feat)\n",
        "    x=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x=tf.keras.layers.Dropout(0.5)(x)\n",
        "    x=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    x=tf.keras.layers.Dropout(0.4)(x)\n",
        "    out=tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model=tf.keras.Model(wave_in, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_eval_ft(version, Xtr_infos, ytr, Xte_infos, yte, classes, yam_infer):\n",
        "    \"\"\"Head-only training on frozen YAMNet embeddings.\n",
        "    Rationale: TF-Hub yamnet/1 exposes no trainable vars; true base FT is not\n",
        "    possible here. We stream pooled embeddings and train an MLP head.\n",
        "    \"\"\"\n",
        "    if yam_infer is None:\n",
        "        raise RuntimeError(\"YAMNet infer not initialized.\")\n",
        "\n",
        "    pooling=version.get(\"pooling\", \"meanstd\")\n",
        "    aug=version.get(\"aug\", None)\n",
        "\n",
        "    print(\" - (Head-only) ÏûÑÎ≤†Îî©(Train) ÏÉùÏÑ± Ï§ë ...\", end=\"\")\n",
        "    Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, BASE_CONFIG, pooling=pooling, aug=aug,\n",
        "                              cache_key=f\"{version['name']}_{pooling}_ft_tr\")\n",
        "    ytr_v=ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "\n",
        "    print(\" - (Head-only) ÏûÑÎ≤†Îî©(Test) ÏÉùÏÑ± Ï§ë ...\", end=\"\")\n",
        "    Xte, keep_te = embed_many(Xte_infos, yam_infer, BASE_CONFIG, pooling=pooling, aug=None,\n",
        "                              cache_key=f\"{version['name']}_{pooling}_ft_te\")\n",
        "    yte_v=yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "    # Ïû¨ÏÇ¨Ïö©: MLP head ÌïôÏäµ/ÌèâÍ∞Ä\n",
        "    v_copy = dict(version)  # shallow copy to avoid side effects\n",
        "    v_copy[\"classifier\"] = \"mlp\"\n",
        "    v_copy[\"type\"] = \"ft\"\n",
        "    res = train_eval_emb(v_copy, Xtr, ytr_v, Xte, yte_v, classes, cfg=BASE_CONFIG)\n",
        "    return res\n",
        "\n",
        "# ======================================================================\n",
        "# 9) ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (Ìïú Î≤à Î∂ÑÌï† ‚Üí Î™®Îì† Î≤ÑÏ†Ñ Í≥µÌÜµ ÎπÑÍµê)\n",
        "# ======================================================================\n",
        "\n",
        "def run_all(config=BASE_CONFIG, versions=VERSIONS):\n",
        "    \"\"\"Single split ‚Üí compare across versions (V5~V8), head-only for FT.\n",
        "    - V5 uses embedding+MLP.\n",
        "    - V6~V8 run as head-only on frozen YAMNet embeddings (same pipeline as V5),\n",
        "      differing by pooling/augs/schedules keyed via cache.\n",
        "    \"\"\"\n",
        "    # ÏÑ∏Í∑∏ ÏÉùÏÑ±\n",
        "    print(\"[STEP] ÏÑ∏Í∑∏Î®ºÌä∏ ÏÉùÏÑ± Ï§ë ...\")\n",
        "    infos, labels, groups, summary, missing = build_segments_ships_ear(SHIPSEAR, config)\n",
        "    print(\" - ÌÅ¥ÎûòÏä§Î≥Ñ Í∞úÏàò:\", dict(summary), \"| Îß§Ìïë Ïã§Ìå®:\", missing)\n",
        "\n",
        "    if len(infos) < 2 or len(set(labels)) < 2:\n",
        "        raise RuntimeError( \"[Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±] ÏÑ∏Í∑∏Î®ºÌä∏ ÏàòÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÍ±∞ÎÇò ÌÅ¥ÎûòÏä§Í∞Ä 2Ï¢Ö ÎØ∏ÎßåÏûÖÎãàÎã§. - SHIPSEAR_DRIVE Í≤ΩÎ°úÏôÄ ÌïòÏúÑ Ìè¥Îçî/ÌååÏùºÎ™ÖÏùÑ Ïû¨ÌôïÏù∏ÌïòÏÑ∏Ïöî.- ÎùºÎ≤® Îß§Ìïë Í∑úÏπô(resolve_ships_ear_class)Í≥º Ïã§Ï†ú Ìè¥ÎçîÎ™ÖÏù¥ ÎßûÎäîÏßÄ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.\"\n",
        "        )\n",
        "\n",
        "    n_files = len(set([i[0] for i in infos]))\n",
        "    n_groups = len(set(groups))\n",
        "    total_h = (len(infos)*config[\"seg_dur\"]) / 3600.0\n",
        "    print(f\" - ÏÑ∏Í∑∏:{len(infos)} | ÌååÏùº‚âà{n_files} | Í∑∏Î£π‚âà{n_groups} | Ï¥ùÍ∏∏Ïù¥‚âà{total_h:.2f} h\")\n",
        "\n",
        "    if BINARY_MODE:\n",
        "        labels_bin = [\"Ship\" if l in [\"A\",\"B\",\"C\",\"D\"] else \"Noise\" for l in labels]\n",
        "        le = LabelEncoder(); y_all = le.fit_transform(labels_bin)\n",
        "    else:\n",
        "        le = LabelEncoder(); y_all = le.fit_transform(labels)\n",
        "    classes = list(le.classes_)\n",
        "    g_arr = np.array(groups)\n",
        "\n",
        "    tr_idx, te_idx, method = stratified_group_split(y_all, g_arr, config[\"test_size\"])\n",
        "    print(f\"[Split] method={method} | train={len(tr_idx)} | test={len(te_idx)} | Í∑∏Î£πÏàò train/test={len(set(g_arr[tr_idx]))}/{len(set(g_arr[te_idx]))}\")\n",
        "\n",
        "    Xtr_infos = [infos[i] for i in tr_idx]; ytr = y_all[tr_idx]\n",
        "    Xte_infos = [infos[i] for i in te_idx]; yte = y_all[te_idx]\n",
        "    classes = list(le.classes_)\n",
        "\n",
        "    # ÏûÑÎ≤†Îî© ÌïÑÏöî Ïó¨Î∂Ä: emb/ft Î™®Îëê infer ÌïÑÏöî (ftÎèÑ head-only)\n",
        "    yam_infer = None\n",
        "    if any(v[\"type\"] in [\"emb\", \"ft\"] for v in versions):\n",
        "        print(\"YAMNet infer Ï§ÄÎπÑ Ï§ë ...\", end=\"\")\n",
        "        yam_infer = make_yamnet_infer()\n",
        "        try:\n",
        "            test_wav = (np.random.randn(YAM_SR).astype(np.float32) * 1e-3)\n",
        "            feat_mean = yamnet_embed(yam_infer, test_wav, pooling=\"mean\")\n",
        "            feat_ms   = yamnet_embed(yam_infer, test_wav, pooling=\"meanstd\")\n",
        "            print(\" [Sanity] mean:\", (None if feat_mean is None else feat_mean.shape),\n",
        "                  \"| meanstd:\", (None if feat_ms is None else feat_ms.shape))\n",
        "        except Exception as e:\n",
        "            print(f\" [Sanity Failed] - {e}\")\n",
        "            yam_infer = None\n",
        "\n",
        "    all_results = []\n",
        "    for v in versions:\n",
        "        print(f\"================= {v['name']} =================\")\n",
        "        pooling = v.get(\"pooling\", \"mean\")\n",
        "\n",
        "        if v[\"type\"] == \"emb\":\n",
        "            if yam_infer is None:\n",
        "                print(f\"YAMNet infer Î¨∏Ï†úÎ°ú ÏûÑÎ≤†Îî© Í∏∞Î∞ò Î≤ÑÏ†Ñ {v['name']} Í±¥ÎÑàÎúÄ.\")\n",
        "                continue\n",
        "            aug = v.get(\"aug\", None)\n",
        "            cache_key = f\"{v['name']}_{pooling}_aug{aug}_{config['seg_dur']}s\"\n",
        "\n",
        "            print(\" - ÏûÑÎ≤†Îî©(Train) Ï§ë ...\", end=\"\")\n",
        "            Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, config, pooling=pooling, aug=aug, cache_key=cache_key+\"_tr\")\n",
        "            ytr_v = ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "            print(\" - ÏûÑÎ≤†Îî©(Test) Ï§ë ...\", end=\"\")\n",
        "            Xte, keep_te = embed_many(Xte_infos, yam_infer, config, pooling=pooling, aug=None, cache_key=cache_key+\"_te\")\n",
        "            yte_v = yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "            if Xtr.size == 0 or Xte.size == 0:\n",
        "                raise RuntimeError(f\"[{v['name']}] ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå®\")\n",
        "\n",
        "            res = train_eval_emb(v, Xtr, ytr_v, Xte, yte_v, classes, cfg=config)\n",
        "\n",
        "        elif v[\"type\"] == \"ft\":\n",
        "            if yam_infer is None:\n",
        "                print(f\"YAMNet infer Î¨∏Ï†úÎ°ú FT Î≤ÑÏ†Ñ {v['name']} Í±¥ÎÑàÎúÄ.\")\n",
        "                continue\n",
        "            print(\" - (Head-only) FT Î≤ÑÏ†Ñ Ïã§Ìñâ Ï§ë ... |\", mem())\n",
        "            # Head-only: ÎèôÏùº ÏûÑÎ≤†Îî© ÏÉùÏÑ± ‚Üí Ìó§Îìú MLP ÌïôÏäµ\n",
        "            aug = v.get(\"aug\", None)\n",
        "            cache_key = f\"{v['name']}_{pooling}_ft_{config['seg_dur']}s\"\n",
        "\n",
        "            print(\" - (Head-only) ÏûÑÎ≤†Îî©(Train) ÏÉùÏÑ± Ï§ë ...\", end=\"\")\n",
        "            Xtr, keep_tr = embed_many(Xtr_infos, yam_infer, config, pooling=pooling, aug=aug, cache_key=cache_key+\"_tr\")\n",
        "            ytr_v = ytr[keep_tr]; print(\" OK\", Xtr.shape, \"|\", mem())\n",
        "            print(\" - (Head-only) ÏûÑÎ≤†Îî©(Test) ÏÉùÏÑ± Ï§ë ...\", end=\"\")\n",
        "            Xte, keep_te = embed_many(Xte_infos, yam_infer, config, pooling=pooling, aug=None, cache_key=cache_key+\"_te\")\n",
        "            yte_v = yte[keep_te]; print(\" OK\", Xte.shape, \"|\", mem())\n",
        "\n",
        "            if Xtr.size == 0 or Xte.size == 0:\n",
        "                raise RuntimeError(f\"[{v['name']}] ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå®\")\n",
        "\n",
        "            v_copy = dict(v)\n",
        "            v_copy[\"classifier\"] = \"mlp\"\n",
        "            v_copy[\"type\"] = \"ft\"\n",
        "            res = train_eval_emb(v_copy, Xtr, ytr_v, Xte, yte_v, classes, cfg=config)\n",
        "\n",
        "        else:\n",
        "            print(f\" - Ïïå Ïàò ÏóÜÎäî Î≤ÑÏ†Ñ ÌÉÄÏûÖ: {v['type']}. Í±¥ÎÑàÎúÄ.\")\n",
        "            continue\n",
        "\n",
        "        # Í≥µÌÜµ: Í≤∞Í≥º Ï†ÄÏû•/ÏßëÍ≥Ñ\n",
        "        row = dict(\n",
        "            version=v['name'], type=v['type'],\n",
        "            pooling=v.get('pooling','-'), classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'),\n",
        "            aug=(v.get('aug') or \"none\"),\n",
        "            acc=res[\"acc\"], bal_acc=res[\"bal_acc\"], macroF1=res[\"macroF1\"],\n",
        "            macroROC=res[\"macroROC\"], topk=res[\"topk\"],\n",
        "            time_sec=res[\"time_sec\"], artifact=res.get(\"artifact\",\"\")\n",
        "        )\n",
        "        all_results.append((row, res))\n",
        "\n",
        "        # ÌòºÎèôÌñâÎ†¨ Ï†ÄÏû•\n",
        "        cm = res[\"cm\"]\n",
        "        plt.figure(figsize=(5.5,4.8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.xlabel(\"ÏòàÏ∏°\"); plt.ylabel(\"Ïã§Ï†ú\"); plt.title(f\"CM ‚Äî {v['name']}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"results/cm_{v['name']}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # AP per class Ï†ÄÏû•\n",
        "        with open(f\"results/ap_{v['name']}.json\",\"w\") as f:\n",
        "            json.dump(res[\"ap_per_class\"], f, indent=2)\n",
        "\n",
        "    # ÏöîÏïΩ Ìëú\n",
        "    df = pd.DataFrame([r[0] for r in all_results])\n",
        "    if not df.empty:\n",
        "        df_sorted = df.sort_values([\"macroF1\",\"bal_acc\",\"acc\"], ascending=False)\n",
        "        df_sorted.to_csv(\"results/summary.csv\", index=False)\n",
        "        print(\"[SUMMARY ‚Äî V5~V8]\")\n",
        "        print(df_sorted.to_string(index=False))\n",
        "\n",
        "        with open(\"results/report.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\" Ship vs Noise ‚Äî V5~V8 ÎπÑÍµê ÏöîÏïΩ\")\n",
        "            f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\")\n",
        "            f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\")\n",
        "            for _,row in df_sorted.iterrows():\n",
        "                f.write(\n",
        "                    f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|\"\n",
        "                    f\"{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{(np.nan if pd.isna(row['macroROC']) else row['macroROC']):.4f}|\"\n",
        "                    f\"{(np.nan if pd.isna(row['topk']) else row['topk']):.4f}|{row['time_sec']:.1f}|{row['artifact']}|\"\n",
        "                )\n",
        "            f.write(\"- ÌòºÎèôÌñâÎ†¨: results/cm_*.png - AP per class: results/ap_*json\")\n",
        "    else:\n",
        "        print(\"No results to summarize (check data path & pipeline).\")\n",
        "\n",
        "    print(\"Í≤∞Í≥º ÌååÏùº:\")\n",
        "    print(\" - results/summary.csv\")\n",
        "    print(\" - results/report.md\")\n",
        "    print(\" - results/cm_*.png\")\n",
        "    print(\" - results/ap_*.json\")\n",
        "    print(\" - artifacts/* (Î™®Îç∏)\")\n",
        "\n",
        "# Ïã§Ìñâ\n",
        "run_all(BASE_CONFIG, VERSIONS)\n",
        "print(\"\\nüéâ ÏôÑÎ£å ‚Äî Î≤ÑÏ†ÑÎ≥Ñ Í≤∞Í≥ºÎäî results/summary.csv, results/report.md, cm_*.png, artifacts/* Ïóê Ï†ÄÏû•Îê©ÎãàÎã§.\")\n"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963b14be-b3e9-45a4-c07d-87955fa77756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) ÌôòÍ≤ΩÏÑ§Ï†ï/ÏÑ§Ïπò Ï§ë ...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            " - GPU found: 1 | memory growth enabled\n",
            " - Ìè∞Ìä∏ OK: NanumGothic\n",
            "\n",
            "2) Îç∞Ïù¥ÌÑ∞ ÌôïÎ≥¥ Ï§ë ...\n",
            " - ShipsEar Ïù¥ÎØ∏ Ï°¥Ïû¨\n",
            "[STEP] ÏÑ∏Í∑∏Î®ºÌä∏ ÏÉùÏÑ± Ï§ë ...\n",
            " - ÌÅ¥ÎûòÏä§Î≥Ñ Í∞úÏàò: {'C': 5101, 'B': 3139, 'E': 1140, 'A': 1856, 'D': 1513} | Îß§Ìïë Ïã§Ìå®: 0\n",
            " - ÏÑ∏Í∑∏:12749 | ÌååÏùº‚âà85 | Í∑∏Î£π‚âà85 | Ï¥ùÍ∏∏Ïù¥‚âà3.54 h\n",
            "[Split] method=StratifiedGroupKFold(n_splits=5) | train=9789 | test=2960 | Í∑∏Î£πÏàò train/test=69/16\n",
            "YAMNet infer Ï§ÄÎπÑ Ï§ë ...[YAMNet] backend=hub.load\n",
            " [Sanity] mean: (1024,) | meanstd: (2048,)\n",
            "================= v5_meanstd_mlp_aug =================\n",
            " - ÏûÑÎ≤†Îî©(Train) Ï§ë ...  ... 5000/9789 | RSS‚âà3.92 GB\n",
            " - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v5_meanstd_mlp_aug_meanstd_auglight_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS‚âà4.04 GB\n",
            " - ÏûÑÎ≤†Îî©(Test) Ï§ë ... - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v5_meanstd_mlp_aug_meanstd_auglight_1.0s_te.npz\n",
            " OK (2960, 2048) | RSS‚âà4.04 GB\n",
            "================= v6_ft_mean_headwarmup_unfreeze =================\n",
            " - (Head-only) FT Î≤ÑÏ†Ñ Ïã§Ìñâ Ï§ë ... | RSS‚âà4.24 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Train) ÏÉùÏÑ± Ï§ë ...  ... 5000/9789 | RSS‚âà4.26 GB\n",
            " - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v6_ft_mean_headwarmup_unfreeze_mean_ft_1.0s_tr.npz\n",
            " OK (9789, 1024) | RSS‚âà4.21 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Test) ÏÉùÏÑ± Ï§ë ... - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v6_ft_mean_headwarmup_unfreeze_mean_ft_1.0s_te.npz\n",
            " OK (2960, 1024) | RSS‚âà4.22 GB\n",
            "================= v7_ft_meanstd_headwarmup_unfreeze =================\n",
            " - (Head-only) FT Î≤ÑÏ†Ñ Ïã§Ìñâ Ï§ë ... | RSS‚âà4.22 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Train) ÏÉùÏÑ± Ï§ë ...  ... 5000/9789 | RSS‚âà4.25 GB\n",
            " - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v7_ft_meanstd_headwarmup_unfreeze_meanstd_ft_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS‚âà4.31 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Test) ÏÉùÏÑ± Ï§ë ... - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v7_ft_meanstd_headwarmup_unfreeze_meanstd_ft_1.0s_te.npz\n",
            " OK (2960, 2048) | RSS‚âà4.31 GB\n",
            "================= v8_ft_meanstd_fullft_tinyLR =================\n",
            " - (Head-only) FT Î≤ÑÏ†Ñ Ïã§Ìñâ Ï§ë ... | RSS‚âà4.29 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Train) ÏÉùÏÑ± Ï§ë ...  ... 5000/9789 | RSS‚âà4.31 GB\n",
            " - Ï∫êÏãú Ï†ÄÏû•: cache/emb_v8_ft_meanstd_fullft_tinyLR_meanstd_ft_1.0s_tr.npz\n",
            " OK (9789, 2048) | RSS‚âà4.31 GB\n",
            " - (Head-only) ÏûÑÎ≤†Îî©(Test) ÏÉùÏÑ± Ï§ë ..."
          ]
        }
      ]
    }
  ]
}