{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDjHLmuVomKPtG5gyPNftI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# AI_ÏàòÏ§ëÏùåÌñ•ÌÉêÏßÄ ‚Äî Ïò§ÌîÑÎùºÏù∏(Windows) Î≤àÎì§ ÏÉùÏÑ± + ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (All-in-One)\n",
        "# ===========================================\n",
        "# ‚Äª ColabÏóêÏÑú Ïã§Ìñâ ‚Üí artifacts/offline_bundle_pip_win/ Ìè¥ÎçîÍ∞Ä ÏÉùÏÑ±Îê©ÎãàÎã§.\n",
        "#    Ïù¥ Ìè¥ÎçîÎ•º Windows Ïò§ÌîÑÎùºÏù∏ PCÎ°ú Í∞ÄÏ†∏Í∞ÄÏÑú install_offline.bat, run_offline.bat ÏàúÏúºÎ°ú Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
        "\n",
        "# --- ÌÜ†Í∏Ä ---\n",
        "PACK_OFFLINE = True       # ColabÏóêÏÑú Windows Ïò§ÌîÑÎùºÏù∏ Î≤àÎì§ ÏÉùÏÑ±\n",
        "WIN_PY       = \"3.10\"     # ÌÉÄÍπÉ WindowsÏùò Python Ï£º/Î∂ÄÎ≤ÑÏ†Ñ (3.10 Ï∂îÏ≤ú)\n",
        "GEN_PORTABLE = True       # Ìè¨ÌÑ∞Î∏î(ÏûÑÎ≤†ÎçîÎ∏î) PythonÏö© Î∞∞Ïπò Ïä§ÌÅ¨Î¶ΩÌä∏ÎèÑ ÏÉùÏÑ±\n",
        "\n",
        "print(\"Setup...\")\n",
        "\n",
        "# (Colab Ï†ÑÏö©) ÌïµÏã¨ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò ‚Äî WindowsÏö© Î≤àÎì§Ïóê ÎÑ£ÏùÑ Î≤ÑÏ†ÑÍ≥º ÎèôÏùº\n",
        "try:\n",
        "    import google.colab  # Ï°¥Ïû¨ÌïòÎ©¥ Colab\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # TF 2.20.0: Windows Ìú† ÏûàÍ≥†, GCS ÌîåÎü¨Í∑∏Ïù∏ ÏùòÏ°¥ Ï†úÍ±∞Îê®\n",
        "    !pip -q install \"tensorflow==2.17.1\" tensorflow_hub==0.16.1 librosa==0.10.2.post1 soundfile==0.12.1 \\\n",
        "                    scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2 \\\n",
        "                    numpy==1.26.4 scipy==1.11.4 pandas==2.2.2 matplotlib==3.8.4 audioread==3.0.1\n",
        "    # (ÏòµÏÖò) ÌïúÍ∏ÄÌè∞Ìä∏\n",
        "    !apt -yq install fonts-nanum >/dev/null || true\n",
        "\n",
        "import os, re, random, math, time, json, glob, shutil, warnings, subprocess, pathlib\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import numpy as np, pandas as pd, psutil, soundfile as sf\n",
        "import tensorflow as tf, tensorflow_hub as hub, librosa\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt, seaborn as sns, matplotlib.font_manager as fm\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, average_precision_score, balanced_accuracy_score, top_k_accuracy_score, accuracy_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# ============== Í≥µÌÜµ ÌôòÍ≤Ω ==============\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42; np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "try:\n",
        "    for g in tf.config.experimental.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "except: pass\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Colab ÌïúÍ∏Ä Ìè∞Ìä∏\n",
        "if IN_COLAB and os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "def mem(): return f\"{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# ============== Í≤ΩÎ°ú/Îç∞Ïù¥ÌÑ∞ ÏÑ∏ÌåÖ ==============\n",
        "IS_WIN = (os.name == \"nt\")\n",
        "BASE = os.getcwd() if not IN_COLAB else \"/content\"\n",
        "\n",
        "# TF-Hub Ï∫êÏãú(ColabÏóêÏÑú ÎØ∏Î¶¨ Î∞õÏïÑÏÑú Î≤àÎì§Ïóê ÎèôÎ¥â)\n",
        "TFHUB_CACHE_DIR = os.path.join(\"artifacts\", \"tfhub_cache\")\n",
        "os.makedirs(TFHUB_CACHE_DIR, exist_ok=True)\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = os.path.abspath(TFHUB_CACHE_DIR)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î£®Ìä∏ (Colab: Drive, Windows: ÌôòÍ≤ΩÎ≥ÄÏàò SHIPSEAR_DIR ÏÇ¨Ïö©)\n",
        "if IN_COLAB:\n",
        "    SHIPSEAR_DRIVE = \"/content/drive/MyDrive/ShipsEar\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(\"Not Colab or Drive:\", e)\n",
        "else:\n",
        "    SHIPSEAR_DRIVE = os.getenv(\"SHIPSEAR_DIR\", r\"D:\\Datasets\\ShipsEar\")  # WindowsÏóêÏÑúÎäî Î∞òÎìúÏãú ÌôòÍ≤ΩÎ≥ÄÏàò/Ïã§Í≤ΩÎ°úÎ°ú ÎßûÏ∂îÏÑ∏Ïöî.\n",
        "\n",
        "SHIPSEAR = os.path.join(BASE, \"ShipsEar_local\" if not IN_COLAB else \"ShipsEar_colab\")\n",
        "os.makedirs(\"results\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "print(\"Data...\")\n",
        "if os.path.exists(SHIPSEAR_DRIVE):\n",
        "    if not os.path.exists(SHIPSEAR) or not os.listdir(SHIPSEAR):\n",
        "        shutil.copytree(SHIPSEAR_DRIVE, SHIPSEAR, dirs_exist_ok=True)\n",
        "        print(\" - Copied ShipsEar\")\n",
        "    else:\n",
        "        print(\" - ShipsEar exists\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"ShipsEar path not found: {SHIPSEAR_DRIVE}\")\n",
        "\n",
        "# ============== ÌååÏù¥ÌîÑÎùºÏù∏ ÏÑ§Ï†ï ==============\n",
        "YAM_SR=16000; BINARY_MODE=True; POS_LABEL=\"Ship\"\n",
        "CFG=dict(seg_dur=1.0, ship_overlap=0.2, noise_overlap=0.0,\n",
        "         vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "         test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "         max_seg_per_group_per_class=500, noise_jitter_sec=0.5,\n",
        "         topk=1, cache_emb=True)\n",
        "\n",
        "MAKE_PLOTS=False\n",
        "SAVE_AP=False\n",
        "\n",
        "VERSIONS=[\n",
        "    dict(name=\"v0a_yamnet_zeroshot\", type=\"zero\"),\n",
        "    dict(name=\"v0b_emb_logreg_basic\", type=\"emb\", classifier=\"logreg\", pooling=\"meanstd\", aug=None),\n",
        "    dict(name=\"v5_meanstd_mlp_aug\",  type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v6_ft_mean_headonly\", type=\"ft\",  pooling=\"mean\",    aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headonly\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_headonly_tinyLR\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "]\n",
        "\n",
        "# ============== Ïò§ÎîîÏò§ VAD/ÏÑ∏Í∑∏Î®ºÌä∏/ÏûÖÏ∂úÎ†• ==============\n",
        "KW={\"A\":[\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"],\n",
        "    \"B\":[\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"],\n",
        "    \"C\":[\"ferry\",\"passenger\"],\n",
        "    \"D\":[\"oceanliner\",\"ocean liner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"],\n",
        "    \"E\":[\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]}\n",
        "\n",
        "def resolve_class(path):\n",
        "    txt=(os.path.basename(os.path.dirname(path))+\" \"+os.path.basename(path)).lower()\n",
        "    for c,kws in ((\"E\",KW[\"E\"]),(\"A\",KW[\"A\"]),(\"B\",KW[\"B\"]),(\"C\",KW[\"C\"]),(\"D\",KW[\"D\"])):\n",
        "        if any(k in txt for k in kws): return c\n",
        "    m=re.search(r'\\bclass[_\\s-]*([abcde])\\b', txt); return m.group(1).upper() if m else None\n",
        "\n",
        "def group_key(path):\n",
        "    stem=os.path.splitext(os.path.basename(path))[0]\n",
        "    m=re.search(r'(\\d{8}[_-]?\\d{4})', stem) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', stem)\n",
        "    if m: return m.group(1)\n",
        "    parent=os.path.basename(os.path.dirname(path)); toks=re.split(r'[_\\-]+', stem); pref=\"_\".join(toks[:3]) if len(toks)>=3 else stem\n",
        "    return f\"{parent}:{pref}\"\n",
        "\n",
        "EPS=1e-12\n",
        "def get_activity(file_path, top_db=25.0, frame_sec=0.5, hop_sec=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(file_path) as f:\n",
        "            sr=f.samplerate; n=len(f); F=max(1,int(frame_sec*sr)); H=max(1,int(hop_sec*sr))\n",
        "            max_db=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); max_db=max(max_db, 20*np.log10(rms+EPS)); pos+=H\n",
        "            if not np.isfinite(max_db): return [], []\n",
        "            th=max_db-top_db; active=[]; in_act=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                db=20*np.log10(float(np.sqrt(np.mean(y**2))+EPS))\n",
        "                t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not in_act: in_act=True; cur=t0\n",
        "                else:\n",
        "                    if in_act: in_act=False; active.append((cur,t1))\n",
        "                pos+=H\n",
        "            if in_act: active.append((cur,n/sr))\n",
        "            inactive=[]; last=0.0; dur=n/sr\n",
        "            for s,e in active:\n",
        "                if s>last: inactive.append((last,s)); last=e\n",
        "            if last<dur: inactive.append((last,dur))\n",
        "            return active, inactive\n",
        "    except: return [], []\n",
        "\n",
        "def spans_to_segs(spans, seg_dur, hop):\n",
        "    segs=[]\n",
        "    for s,e in spans:\n",
        "        if e-s < seg_dur: continue\n",
        "        st=s\n",
        "        while st <= e - seg_dur + 1e-9:\n",
        "            segs.append((float(st),)); st += hop\n",
        "    return segs\n",
        "\n",
        "def build_segments(root, cfg):\n",
        "    seg_dur=cfg[\"seg_dur\"]; hop_ship=seg_dur*(1-cfg[\"ship_overlap\"]); hop_noise=seg_dur*(1-cfg[\"noise_overlap\"])\n",
        "    noise_jitter=cfg[\"noise_jitter_sec\"]; cap=cfg[\"max_seg_per_group_per_class\"]\n",
        "    infos=[]; labels=[]; groups=[]; missing=0; per_gc=defaultdict(int); summary=defaultdict(int)\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        c=resolve_class(fp)\n",
        "        if c is None: missing+=1; continue\n",
        "        try: info=sf.info(fp)\n",
        "        except: continue\n",
        "        gk=group_key(fp)\n",
        "        if c in \"ABCD\":\n",
        "            act,_=get_activity(fp, cfg[\"vad_top_db\"], cfg[\"vad_frame_sec\"], cfg[\"vad_hop_sec\"]); spans=act; hop=hop_ship\n",
        "        else:\n",
        "            dur=info.frames/info.samplerate; spans=[(0.0,dur)]; hop=hop_noise\n",
        "        segs=spans_to_segs(spans, seg_dur, hop); random.shuffle(segs)\n",
        "        for (st,) in segs:\n",
        "            if c==\"E\" and noise_jitter>0:\n",
        "                j=random.uniform(-noise_jitter, noise_jitter)\n",
        "                st=max(0.0, min(st+j, (info.frames/info.samplerate) - seg_dur))\n",
        "            key=(gk,c)\n",
        "            if cap and per_gc[key]>=cap: continue\n",
        "            infos.append((fp, float(st), info.samplerate)); labels.append(c); groups.append(gk)\n",
        "            per_gc[key]+=1; summary[c]+=1\n",
        "    return infos, labels, groups, summary, missing\n",
        "\n",
        "# Ï∫êÏãúÌòï ÏÑ∏Í∑∏Î®ºÌä∏ Î°úÎî©/Î¶¨ÏÉòÌîå\n",
        "_WAVE_CACHE=OrderedDict()\n",
        "_WAVE_CACHE_BYTES=0\n",
        "_MAX_CACHE_BYTES=256*1024*1024\n",
        "\n",
        "def _cache_get(fp):\n",
        "    arr=_WAVE_CACHE.get(fp)\n",
        "    if arr is not None:\n",
        "        _WAVE_CACHE.move_to_end(fp)\n",
        "    return arr\n",
        "\n",
        "def _cache_put(fp, arr):\n",
        "    global _WAVE_CACHE_BYTES\n",
        "    size=getattr(arr, \"nbytes\", None)\n",
        "    if size is None:\n",
        "        try: size=arr.size*arr.itemsize\n",
        "        except: size=0\n",
        "    _WAVE_CACHE[fp]=arr\n",
        "    _WAVE_CACHE.move_to_end(fp)\n",
        "    _WAVE_CACHE_BYTES += size\n",
        "    while _WAVE_CACHE_BYTES > _MAX_CACHE_BYTES and len(_WAVE_CACHE)>1:\n",
        "        k,v=_WAVE_CACHE.popitem(last=False)\n",
        "        try: _WAVE_CACHE_BYTES -= v.nbytes\n",
        "        except: pass\n",
        "\n",
        "def safe_resample(y, sr0, sr1):\n",
        "    if sr0==sr1: return y.astype(np.float32)\n",
        "    try:\n",
        "        import scipy.signal as spsig\n",
        "        g=math.gcd(int(sr0),int(sr1)); up=int(sr1)//g; down=int(sr0)//g\n",
        "        return spsig.resample_poly(y, up, down).astype(np.float32)\n",
        "    except Exception:\n",
        "        try: return librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr1, res_type=\"fft\").astype(np.float32)\n",
        "        except Exception:\n",
        "            new_len=int(round(len(y)*float(sr1)/float(sr0)))\n",
        "            xp=np.arange(len(y)); x_new=np.linspace(0,len(y),new_len,endpoint=False)\n",
        "            return np.interp(x_new, xp, y).astype(np.float32)\n",
        "\n",
        "def load_segment_cached(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        y_full = _cache_get(fp)\n",
        "        if y_full is None:\n",
        "            y_full, sr_read = sf.read(fp, dtype='float32', always_2d=False)\n",
        "            if y_full.ndim>1: y_full = y_full.mean(axis=1)\n",
        "            if sr_read != target_sr: y_full = safe_resample(y_full, sr_read, target_sr)\n",
        "            _cache_put(fp, y_full)\n",
        "        L = int(seg_dur*target_sr); start = int(st*target_sr)\n",
        "        if start >= len(y_full): return None\n",
        "        y = y_full[start : min(start+L, len(y_full))]\n",
        "        if len(y) < L: y = np.pad(y, (0, L-len(y)), mode='constant')\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def load_segment(info, seg_dur, target_sr=YAM_SR, rms_norm=True):\n",
        "    fp, st, sr0 = info\n",
        "    try:\n",
        "        start=int(st*sr0); num=int(seg_dur*sr0)\n",
        "        with sf.SoundFile(fp, 'r') as f:\n",
        "            remain=f.frames-start\n",
        "            if remain<=0: return None\n",
        "            num=min(num, remain)\n",
        "        y,_=sf.read(fp, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y is None: return None\n",
        "        if y.ndim>1: y=y.mean(axis=1)\n",
        "        if sr0!=target_sr: y=safe_resample(y, sr0, target_sr)\n",
        "        if rms_norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\", e); return None\n",
        "\n",
        "def augment(y, sr, kind=\"light\"):\n",
        "    if y is None or kind!=\"light\": return y\n",
        "    y = y * (10**(random.uniform(-3,3)/20))\n",
        "    sh = random.randint(-int(0.25*sr), int(0.25*sr))\n",
        "    if sh>0: y=np.concatenate([np.zeros(sh, dtype=y.dtype), y[:-sh]])\n",
        "    elif sh<0: y=np.concatenate([y[-sh:], np.zeros(-sh, dtype=y.dtype)])\n",
        "    return y\n",
        "\n",
        "# ============== YAMNet ÏûÑÎ≤†Îî©/Ï†úÎ°úÏÉ∑ ==============\n",
        "YAM_URL=\"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "def make_yam_infer():\n",
        "    ship_idx=[]\n",
        "    try:\n",
        "        module=hub.load(YAM_URL)  # Ï∫êÏãúÍ∞Ä ÏûàÏúºÎ©¥ Ïò§ÌîÑÎùºÏù∏ OK\n",
        "        def infer(y): return module(tf.convert_to_tensor(y, tf.float32))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] hub.load (cached ok)\")\n",
        "        try:\n",
        "            path=module.class_map_path().numpy().decode(\"utf-8\")\n",
        "            df=pd.read_csv(path); col='display_name' if 'display_name' in df.columns else df.columns[-1]\n",
        "            names=df[col].astype(str).str.lower().tolist()\n",
        "            subs=[\"boat\",\"ship\",\"sail\",\"sailing\",\"ferry\",\"cargo\",\"tanker\",\"submarine\",\"motorboat\",\"watercraft\",\"water vehicle\",\"ocean liner\",\"yacht\",\"kayak\",\"canoe\",\"rowboat\",\"row\",\"fishing\"]\n",
        "            ship_idx=[i for i,n in enumerate(names) if any(s in n for s in subs)]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return infer, ship_idx\n",
        "    except Exception:\n",
        "        layer=hub.KerasLayer(YAM_URL, trainable=False)\n",
        "        def infer(y):\n",
        "            t=tf.convert_to_tensor(y, tf.float32)\n",
        "            try: return layer(t)\n",
        "            except: return layer(tf.expand_dims(t,0))\n",
        "        _=infer(np.zeros(16000, np.float32)); print(\"[YAMNet] KerasLayer\")\n",
        "        return infer, ship_idx\n",
        "\n",
        "def _emb_from_out(out):\n",
        "    emb=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=2: emb=out[1]\n",
        "    elif isinstance(out,dict):\n",
        "        emb=out.get(\"embeddings\") or out.get(\"embedding\")\n",
        "        if emb is None:\n",
        "            for v in out.values():\n",
        "                if isinstance(v,dict):\n",
        "                    emb=v.get(\"embeddings\") or v.get(\"embedding\")\n",
        "                    if emb is not None: break\n",
        "    if emb is None: return None\n",
        "    t=tf.convert_to_tensor(emb)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: t=tf.expand_dims(t,0)\n",
        "    return t\n",
        "\n",
        "def embed_one(infer, y, pooling=\"meanstd\"):\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        t=_emb_from_out(infer(y))\n",
        "        if t is None or t.shape.rank!=2 or int(t.shape[0])==0: return None\n",
        "        if pooling==\"mean\":\n",
        "            feat=tf.reduce_mean(t,axis=0)\n",
        "        else:\n",
        "            m=tf.reduce_mean(t,axis=0); s=tf.math.reduce_std(t,axis=0); feat=tf.concat([m,s],axis=0)\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR embed:\", e); return None\n",
        "\n",
        "def embed_many(infos, infer, cfg, pooling=\"meanstd\", aug=None, cache_key=None, show_every=4000):\n",
        "    cache=None\n",
        "    if cfg[\"cache_emb\"] and cache_key:\n",
        "        cache=f\"cache/emb_{cache_key}.npz\"\n",
        "        if os.path.exists(cache):\n",
        "            z=np.load(cache, allow_pickle=True); print(f\" - cache {cache} | X:{z['X'].shape} keep:{z['keep'].shape}\"); return z[\"X\"], z[\"keep\"]\n",
        "    X=[]; keep=[]\n",
        "    for i,info in enumerate(infos,1):\n",
        "        y=load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "        if aug: y=augment(y, YAM_SR, aug)\n",
        "        e=embed_one(infer, y, pooling)\n",
        "        if e is not None: X.append(e); keep.append(i-1)\n",
        "        if i%show_every==0: print(f\"  ... {i}/{len(infos)} (mem {mem()})\")\n",
        "    X=np.asarray(X,np.float32); keep=np.array(keep,np.int64)\n",
        "    if cache and X.size>0: np.savez_compressed(cache, X=X, keep=keep)\n",
        "    if X.size==0: print(f\"ERR: no embeddings for {len(infos)} segs\")\n",
        "    return X, keep\n",
        "\n",
        "def yam_scores(infer, y):\n",
        "    out=infer(y); sc=None\n",
        "    if isinstance(out,(list,tuple)) and len(out)>=1: sc=out[0]\n",
        "    elif isinstance(out,dict): sc=out.get('scores') or out.get('predictions')\n",
        "    if sc is None: return None\n",
        "    t=tf.convert_to_tensor(sc)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: return t.numpy().astype(np.float32)\n",
        "    return tf.reduce_mean(t,axis=0).numpy().astype(np.float32)\n",
        "\n",
        "# ============== Î∂ÑÎ•òÍ∏∞ (Emb/MLP/LogReg) ==============\n",
        "def build_mlp(in_dim, n_cls, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    x=tf.keras.Input(shape=(in_dim,)); h=tf.keras.layers.BatchNormalization()(x)\n",
        "    h=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.5)(h)\n",
        "    h=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.4)(h)\n",
        "    y=tf.keras.layers.Dense(n_cls, activation='softmax')(h)\n",
        "    m=tf.keras.Model(x,y); m.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy']); return m\n",
        "\n",
        "def train_eval_emb(version, Xtr, ytr, Xte, yte, classes, cfg):\n",
        "    res={}\n",
        "    if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"[emb] empty features\")\n",
        "    if version.get(\"classifier\")==\"mlp\":\n",
        "        clf=build_mlp(Xtr.shape[-1], len(classes), cfg[\"lr\"])\n",
        "        cb=[tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)]\n",
        "        ytrc=tf.keras.utils.to_categorical(ytr, num_classes=len(classes)); ytec=tf.keras.utils.to_categorical(yte, num_classes=len(classes))\n",
        "        cw={c:len(ytr)/ (len(np.unique(ytr))*cnt) for c,cnt in Counter(ytr).items()}\n",
        "        t0=time.time(); clf.fit(Xtr, ytrc, validation_data=(Xte,ytec), epochs=cfg[\"epochs\"], batch_size=cfg[\"batch\"], verbose=0, class_weight=cw, callbacks=cb)\n",
        "        probs=clf.predict(Xte, verbose=0).astype(np.float32); pred=probs.argmax(1); path=f\"artifacts/{version['name']}_mlp.keras\"; clf.save(path); res[\"artifact\"]=path; res[\"time_sec\"]=time.time()-t0\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression; from sklearn.svm import SVC; import joblib\n",
        "        sc=StandardScaler().fit(Xtr); Xtr_s=sc.transform(Xtr); Xte_s=sc.transform(Xte); t0=time.time()\n",
        "        if version.get(\"classifier\")==\"logreg\":\n",
        "            clf=LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=1); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        else:\n",
        "            clf=SVC(C=2.0, kernel='rbf', probability=True, class_weight='balanced'); clf.fit(Xtr_s, ytr); probs=clf.predict_proba(Xte_s); pred=probs.argmax(1)\n",
        "        res[\"time_sec\"]=time.time()-t0; joblib.dump(clf, f\"artifacts/{version['name']}_{version['classifier']}.joblib\"); joblib.dump(sc, f\"artifacts/{version['name']}_scaler.joblib\"); res[\"artifact\"]=\"artifacts/*\"\n",
        "    true=yte; res[\"acc\"]=accuracy_score(true,pred); res[\"bal_acc\"]=balanced_accuracy_score(true,pred); res[\"macroF1\"]=f1_score(true,pred,average='macro')\n",
        "    try:\n",
        "        res[\"macroROC\"]=roc_auc_score(tf.keras.utils.to_categorical(true, len(classes)), probs, average='macro', multi_class='ovr')\n",
        "    except: res[\"macroROC\"]=np.nan\n",
        "    try: res[\"topk\"]=top_k_accuracy_score(true, probs, k=cfg['topk'], labels=range(len(classes)))\n",
        "    except: res[\"topk\"]=np.nan\n",
        "    ap={};\n",
        "    for i,lab in enumerate(classes):\n",
        "        yb=(true==i).astype(int)\n",
        "        ap[lab]=float(average_precision_score(yb, probs[:,i])) if 0<yb.sum()<len(yb) else float(\"nan\")\n",
        "    res[\"ap_per_class\"]=ap; res[\"cm\"]=confusion_matrix(true,pred)\n",
        "    if len(classes)==2:\n",
        "        try: pos_idx=classes.index(POS_LABEL) if POS_LABEL in classes else 1; res[\"macroROC\"]=roc_auc_score(true, probs[:,pos_idx])\n",
        "        except: res[\"macroROC\"]=np.nan\n",
        "        res[\"topk\"]=np.nan\n",
        "    return res\n",
        "\n",
        "# ============== Split ==============\n",
        "def strat_group_split(y, groups, test_size=0.2, seed=SEED):\n",
        "    n=len(y)\n",
        "    if n<2 or len(set(y))<2:\n",
        "        raise RuntimeError(\"[Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±] ÏÑ∏Í∑∏Î®ºÌä∏ ÏàòÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÍ±∞ÎÇò ÌÅ¥ÎûòÏä§Í∞Ä 2Ï¢Ö ÎØ∏ÎßåÏûÖÎãàÎã§.\\n- Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ•º Ïû¨ÌôïÏù∏ÌïòÏÑ∏Ïöî.\\n- ÎùºÎ≤®ÎßÅ Í∑úÏπô(resolve_class)Í≥º Ïã§Ï†ú Ìè¥ÎçîÎ™ÖÏù¥ ÎßûÎäîÏßÄ Ï†êÍ≤ÄÌïòÏÑ∏Ïöî.\")\n",
        "    gss=GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    tr,te=next(gss.split(np.arange(n), y, groups)); return tr,te,\"GroupShuffleSplit\"\n",
        "\n",
        "# ============== Pipeline ==============\n",
        "def run_all(cfg=CFG, versions=VERSIONS):\n",
        "    print(\"Build segments...\")\n",
        "    infos,labels,groups,summary,missing=build_segments(SHIPSEAR,cfg)\n",
        "    print(f\" - per-class: {dict(summary)} | missing: {missing}\")\n",
        "    if BINARY_MODE: labels=[\"Ship\" if l in \"ABCD\" else \"Noise\" for l in labels]\n",
        "    le=LabelEncoder(); y=le.fit_transform(labels); classes=list(le.classes_); g=np.array(groups)\n",
        "    tr,te,method=strat_group_split(y,g,cfg[\"test_size\"]); print(f\"[Split] {method} | train={len(tr)} test={len(te)} groups {len(set(g[tr]))}/{len(set(g[te]))}\")\n",
        "    Xtr_i=[infos[i] for i in tr]; ytr=y[tr]; Xte_i=[infos[i] for i in te]; yte=y[te]\n",
        "    print(\"YAMNet infer...\", end=\"\"); infer, ship_idx = make_yam_infer(); print(f\" OK (ship_idx={len(ship_idx)})\")\n",
        "\n",
        "    feature_bank = {}\n",
        "    def get_feats(tag, infos, pooling, aug):\n",
        "        key = (tag, pooling, aug or 'none', cfg['seg_dur'], len(infos))\n",
        "        if key not in feature_bank:\n",
        "            X, keep = embed_many(\n",
        "                infos, infer, cfg, pooling, aug,\n",
        "                cache_key=f\"{tag}_pool={pooling}_aug={(aug or 'none')}_seg={cfg['seg_dur']}s\"\n",
        "            )\n",
        "            feature_bank[key] = (X, keep)\n",
        "        return feature_bank[key]\n",
        "\n",
        "    all_rows = []\n",
        "    for v in versions:\n",
        "        print(f\"\\n==== {v['name']} ====\")\n",
        "        if v[\"type\"] in (\"emb\",\"ft\"):\n",
        "            pooling = v.get(\"pooling\",\"meanstd\")\n",
        "            aug = v.get(\"aug\", None)\n",
        "            print(\" - embeds (train)...\", end=\"\"); Xtr, kt = get_feats(\"train\", Xtr_i, pooling, aug); ytr_v = ytr[kt]; print(f\" OK {Xtr.shape} (mem {mem()})\")\n",
        "            print(\" - embeds (test)...\",  end=\"\"); Xte, ke = get_feats(\"test\",  Xte_i, pooling, None); yte_v = yte[ke]; print(f\" OK {Xte.shape} (mem {mem()})\")\n",
        "            if Xtr.size == 0 or Xte.size == 0: raise RuntimeError(f\"[{v['name']}] ÏûÑÎ≤†Îî© Ïã§Ìå®\")\n",
        "            if v[\"type\"] == \"emb\":\n",
        "                res = train_eval_emb(v, Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "            else:\n",
        "                res = train_eval_emb(dict(v, classifier=\"mlp\"), Xtr, ytr_v, Xte, yte_v, classes, cfg)\n",
        "\n",
        "        elif v[\"type\"] == \"zero\":\n",
        "            if not ship_idx:\n",
        "                print(\" - zero-shot skipped (no ship idx)\")\n",
        "                continue\n",
        "            print(\" - zero-shot scoring...\", end=\"\")\n",
        "            def score_list(infos):\n",
        "                s = []\n",
        "                for info in infos:\n",
        "                    yseg = load_segment_cached(info, cfg[\"seg_dur\"], YAM_SR, True)\n",
        "                    if yseg is None: continue\n",
        "                    sc = yam_scores(infer, yseg)\n",
        "                    if sc is None: continue\n",
        "                    s.append(float(1.0 - np.prod(1.0 - sc[ship_idx])))\n",
        "                return np.array(s, np.float32)\n",
        "\n",
        "            s_tr = score_list(Xtr_i); s_te = score_list(Xte_i)\n",
        "            keep_tr = np.where(~np.isnan(s_tr))[0]; keep_te = np.where(~np.isnan(s_te))[0]\n",
        "            s_tr = s_tr[keep_tr]; ytr_v = ytr[keep_tr]; s_te = s_te[keep_te]; yte_v = yte[keep_te]\n",
        "            pos_idx = classes.index(POS_LABEL) if POS_LABEL in classes else 1\n",
        "            ytr_bin = (ytr_v == pos_idx).astype(int); yte_bin = (yte_v == pos_idx).astype(int)\n",
        "            t_best = 0.5; f_best = -1.0\n",
        "            for t in np.linspace(0, 1, 21):\n",
        "                f = f1_score(ytr_bin, (s_tr >= t).astype(int), average='binary', zero_division=0)\n",
        "                if f > f_best: f_best = f; t_best = float(t)\n",
        "            pred = (s_te >= t_best).astype(int)\n",
        "            res = dict(\n",
        "                artifact=\"\",\n",
        "                time_sec=0.0,\n",
        "                acc=accuracy_score(yte_bin, pred),\n",
        "                bal_acc=balanced_accuracy_score(yte_bin, pred),\n",
        "                macroF1=f1_score(yte_bin, pred, average='macro'),\n",
        "                macroROC=(roc_auc_score(yte_bin, s_te) if len(np.unique(yte_bin)) == 2 else np.nan),\n",
        "                topk=np.nan,\n",
        "                ap_per_class={POS_LABEL: float(average_precision_score(yte_bin, s_te))},\n",
        "                cm=confusion_matrix(yte_bin, pred),\n",
        "            )\n",
        "            print(\" OK\")\n",
        "\n",
        "        else:\n",
        "            print(\" - unknown type; skip\")\n",
        "            continue\n",
        "\n",
        "        row = dict(\n",
        "            version=v['name'], type=v['type'],\n",
        "            pooling=v.get('pooling','-'),\n",
        "            classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'),\n",
        "            aug=(v.get('aug') or 'none'),\n",
        "            acc=res[\"acc\"], bal_acc=res[\"bal_acc\"], macroF1=res[\"macroF1\"],\n",
        "            macroROC=res[\"macroROC\"], topk=res[\"topk\"],\n",
        "            time_sec=res[\"time_sec\"], artifact=res.get(\"artifact\",\"\")\n",
        "        )\n",
        "        all_rows.append((row, res))\n",
        "\n",
        "        if MAKE_PLOTS:\n",
        "            cm = res[\"cm\"]; plt.figure(figsize=(5.2, 4.5))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "            plt.xlabel(\"ÏòàÏ∏°\"); plt.ylabel(\"Ïã§Ï†ú\"); plt.title(f\"CM ‚Äî {v['name']}\"); plt.tight_layout()\n",
        "            plt.savefig(f\"results/cm_{v['name']}.png\", dpi=150); plt.close()\n",
        "\n",
        "        if SAVE_AP:\n",
        "            with open(f\"results/ap_{v['name']}.json\", \"w\") as f:\n",
        "                json.dump(res[\"ap_per_class\"], f, indent=2)\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No results. Check data path.\"); return\n",
        "    df=pd.DataFrame([r[0] for r in all_rows]).sort_values([\"macroF1\",\"bal_acc\",\"acc\"], ascending=False)\n",
        "    df.to_csv(\"results/summary.csv\", index=False)\n",
        "    print(\"\\n[SUMMARY]\"); print(df.to_string(index=False))\n",
        "    with open(\"results/report.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Ship vs Noise ‚Äî V0a/V0b/V5~V8 ÎπÑÍµê ÏöîÏïΩ\\n\")\n",
        "        f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\\n\")\n",
        "        f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\\n\")\n",
        "        for _,row in df.iterrows():\n",
        "            macroROC = np.nan if pd.isna(row['macroROC']) else row['macroROC']\n",
        "            topk = np.nan if pd.isna(row['topk']) else row['topk']\n",
        "            f.write(f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{macroROC:.4f}|{topk:.4f}|{row['time_sec']:.1f}|{row['artifact']}|\\n\")\n",
        "        f.write(\"- ÌòºÎèôÌñâÎ†¨: results/cm_*.png\\n\\n- AP per class: results/ap_*.json\\n\")\n",
        "    print(\"\\nÍ≤∞Í≥º ÌååÏùº: results/summary.csv, results/report.md, results/cm_*.json, artifacts/*\")\n",
        "\n",
        "# ============== Windows Ïò§ÌîÑÎùºÏù∏ Î≤àÎì§ ÏÉùÏÑ±Í∏∞ ==============\n",
        "def _run(cmd: str):\n",
        "    print(\"+\", cmd)\n",
        "    return subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "def prepare_offline_windows_bundle(win_py=\"3.10\", gen_portable=True, verbose=False, use_minimal_reqs=True):\n",
        "    \"\"\"\n",
        "    artifacts/offline_bundle_pip_win/ Ïóê Windows Ïò§ÌîÑÎùºÏù∏ Î≤àÎì§ÏùÑ ÏÉùÏÑ±.\n",
        "      - wheelhouse/: Windows(win_amd64) Ìú†Îßå ÏàòÏßë (sdist Ï†úÏô∏)\n",
        "      - models/tfhub_cache/: TF-Hub(YAMNet) Ï∫êÏãú Î≥µÏÇ¨ (Ïò§ÌîÑÎùºÏù∏ ÌóàÎ∏å Î°úÎìú)\n",
        "      - install_offline.bat / run_offline.bat ÏÉùÏÑ± (UTF-8 Í≥†Ï†ï Ìè¨Ìï®)\n",
        "      - (ÏòµÏÖò) portable_install.bat / run_offline_portable.bat ÏÉùÏÑ±\n",
        "    \"\"\"\n",
        "    out_dir = pathlib.Path(\"artifacts/offline_bundle_pip_win\")\n",
        "    wheelhouse = out_dir / \"wheelhouse\"\n",
        "    models_dir = out_dir / \"models\"\n",
        "    tfhub_out = models_dir / \"tfhub_cache\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    wheelhouse.mkdir(parents=True, exist_ok=True)\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # requirements ÏûëÏÑ± (ÏµúÏÜå ÏÖã: ÏúàÎèÑÏö∞ Ìú† ÌôïÏù∏Îêú Ï°∞Ìï©)\n",
        "    req = out_dir / \"requirements.txt\"\n",
        "    if use_minimal_reqs:\n",
        "        base_reqs = [\n",
        "            \"tensorflow==2.20.0\",\n",
        "            \"tensorflow-hub==0.16.1\",\n",
        "            \"numpy==1.26.4\",\n",
        "            \"scipy==1.11.4\",\n",
        "            \"pandas==2.2.2\",\n",
        "            \"matplotlib==3.8.4\",\n",
        "            \"seaborn==0.13.2\",\n",
        "            \"librosa==0.10.2.post1\",\n",
        "            \"soundfile==0.12.1\",\n",
        "            \"scikit-learn==1.5.2\",\n",
        "            \"joblib==1.4.2\",\n",
        "            \"psutil==5.9.8\",\n",
        "            \"packaging\",\n",
        "            \"pooch>=1.0.0\",\n",
        "            \"lazy-loader>=0.3\",\n",
        "            \"decorator>=5.0.0\",\n",
        "            \"threadpoolctl>=3.1.0\",\n",
        "        ]\n",
        "        req.write_text(\"\\n\".join(base_reqs) + \"\\n\", encoding=\"utf-8\")\n",
        "        print(f\"[OK] Wrote MINIMAL {req} (lines: {len(base_reqs)})\")\n",
        "    else:\n",
        "        # (ÎåÄÏ≤¥ Í≤ΩÎ°ú) freeze ÌõÑ Ïû°Ìå®ÌÇ§ÏßÄ Ï†úÍ±∞ Î∞è ÌïÑÏàò ÌïÄ Ï∂îÍ∞Ä ‚Äî ÌïÑÏöî Ïãú ÏÇ¨Ïö©\n",
        "        req_full = out_dir / \"requirements_full.txt\"\n",
        "        _run(f\"python -m pip freeze > {req_full}\")\n",
        "        drop_patterns = [\n",
        "            r\"^ipykernel==\", r\"^ipython==\", r\"^jupyter\", r\"^notebook==\", r\"^qtconsole==\", r\"^jedi==\",\n",
        "            r\"^google-colab==\", r\"^matplotlib-inline==\", r\"^tornado==\", r\"^pyzmq==\", r\"^debugpy==\",\n",
        "            r\"^tensorboard==\", r\"^grpcio==.*\", r\"^google-.*==\",\n",
        "            r\"^tensorflow-io-gcs-filesystem==\",\n",
        "        ]\n",
        "        kept=[]\n",
        "        for ln in req_full.read_text(encoding=\"utf-8\").splitlines():\n",
        "            if any(re.search(p, ln) for p in drop_patterns): continue\n",
        "            kept.append(ln)\n",
        "        pins = [\n",
        "            \"tensorflow==2.20.0\",\"tensorflow-hub==0.16.1\",\n",
        "            \"librosa==0.10.2.post1\",\"soundfile==0.12.1\",\n",
        "            \"scikit-learn==1.5.2\",\"psutil==5.9.8\",\"seaborn==0.13.2\",\n",
        "            \"joblib==1.4.2\",\"numpy==1.26.4\",\"scipy\",\"matplotlib\",\"pandas\"\n",
        "        ]\n",
        "        def has(lines, name):\n",
        "            n=name.split(\"==\")[0].lower().replace(\"_\",\"-\")\n",
        "            return any((l.split(\"==\")[0].lower().replace(\"_\",\"-\")==n) for l in lines if l.strip())\n",
        "        base = list(dict.fromkeys([k.strip() for k in kept if k.strip()]))\n",
        "        for p in pins:\n",
        "            if not has(base, p): base.append(p)\n",
        "        req.write_text(\"\\n\".join(base) + \"\\n\", encoding=\"utf-8\")\n",
        "        print(f\"[OK] Wrote TRIMMED {req} (lines: {len(base)})\")\n",
        "\n",
        "    # Windows Ìú† ÍµêÏ∞® Îã§Ïö¥Î°úÎìú\n",
        "    py_major, py_minor = win_py.split(\".\")\n",
        "    plat = \"win_amd64\"\n",
        "    common = f\"--platform {plat} --python-version {py_major}{py_minor} --only-binary=:all: --prefer-binary\"\n",
        "    vflag = \"-v\" if verbose else \"\"\n",
        "    try:\n",
        "        _run(f\"python -m pip download -r {req} -d {wheelhouse} {common} {vflag}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        logf = out_dir / \"download_fail.log\"\n",
        "        os.system(f\"python -m pip download -r {req} -d {wheelhouse} {common} -v > {logf} 2>&1\")\n",
        "        print(\"\\n[WARN] ÏùºÎ∂Ä Ìå®ÌÇ§ÏßÄÏùò Windows Ìú†Ïù¥ ÏóÜÏñ¥ Ïã§Ìå®ÌñàÏäµÎãàÎã§.\")\n",
        "        print(f\" - ÏûêÏÑ∏Ìïú Î°úÍ∑∏: {logf} (ÎßàÏßÄÎßâ 'No matching distribution found for ...' ÌôïÏù∏)\")\n",
        "        raise\n",
        "\n",
        "    # TF-Hub Ï∫êÏãú Î≥µÏÇ¨\n",
        "    if os.path.isdir(TFHUB_CACHE_DIR):\n",
        "        if os.path.isdir(tfhub_out): shutil.rmtree(tfhub_out)\n",
        "        shutil.copytree(TFHUB_CACHE_DIR, tfhub_out)\n",
        "        print(f\"[OK] Copied TF-Hub cache ‚Üí {tfhub_out}\")\n",
        "\n",
        "    # Î∞∞Ïπò Ïä§ÌÅ¨Î¶ΩÌä∏ (UTF-8 Í∞ïÏ†ú Ìè¨Ìï®)\n",
        "    (out_dir / \"install_offline.bat\").write_text(fr\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "where python >nul 2>&1 || (echo [ERROR] Python {win_py}+ required & exit /b 1)\n",
        "python -c \"import sys;mi=list(map(int,'{win_py}'.split('.')));v=sys.version_info;exit(0 if (v.major>mi[0] or (v.major==mi[0] and v.minor>=mi[1])) else 1)\" || (echo [ERROR] Python >= {win_py} required & exit /b 1)\n",
        "python -m venv .venv\n",
        "call .venv\\Scripts\\activate\n",
        "python -m pip install --upgrade pip\n",
        "python -X utf8 -m pip install --no-index --find-links=wheelhouse -r requirements.txt\n",
        "echo [OK] Installed from local wheelhouse.\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    (out_dir / \"run_offline.bat\").write_text(rf\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set TFHUB_CACHE_DIR=%~dp0models\\tfhub_cache\n",
        "call .venv\\Scripts\\activate\n",
        "python -X utf8 -c \"import os;print('TFHUB_CACHE_DIR=', r'%TFHUB_CACHE_DIR%')\"\n",
        "echo Ready. Activate venv and run your script, e.g.:\n",
        "echo     call .venv\\Scripts\\activate\n",
        "echo     python -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    (out_dir / \"README-Windows.txt\").write_text(f\"\"\"Ïò§ÌîÑÎùºÏù∏ ÏÑ§Ïπò/Ïã§Ìñâ ÏïàÎÇ¥ (Windows)\n",
        "\n",
        "1) Python {win_py} (64-bit) ÏÑ§Ïπò + PATH Ï∂îÍ∞Ä\n",
        "2) Ïù¥ Ìè¥ÎçîÏóêÏÑú install_offline.bat Ïã§Ìñâ ‚Üí wheelhouseÏóêÏÑú Ïò§ÌîÑÎùºÏù∏ ÏÑ§Ïπò\n",
        "3) run_offline.bat Ïã§Ìñâ ‚Üí TF-Hub Ï∫êÏãú Î≥ÄÏàò ÏÑ∏ÌåÖ\n",
        "4) Í∞ÄÏÉÅÌôòÍ≤Ω ÌôúÏÑ±Ìôî ÌõÑ ÌååÏù¥Ïç¨ Ïã§Ìñâ:\n",
        "   > call .venv\\\\Scripts\\\\activate\n",
        "   > python -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    if gen_portable:\n",
        "        (out_dir / \"portable_install.bat\").write_text(r\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set BASE=%~dp0\n",
        "set EMBED=%BASE%py_embed\n",
        "\n",
        "if not exist \"%EMBED%\" mkdir \"%EMBED%\"\n",
        "for %%Z in (\"%BASE%python-*-embed-amd64.zip\") do (\n",
        "  powershell -NoP -C \"Expand-Archive -Path '%%~fZ' -DestinationPath '%EMBED%' -Force\"\n",
        ")\n",
        "for %%F in (\"%EMBED%\\python*.pth\") do (\n",
        "  powershell -NoP -C \"(Get-Content '%%~fF') -replace '^\\s*#\\s*import site','import site' | Set-Content '%%~fF'\"\n",
        ")\n",
        "\"%EMBED%\\python.exe\" \"%BASE%get-pip.py\" --no-index --find-links=\"%BASE%wheelhouse\" pip setuptools wheel\n",
        "\"%EMBED%\\python.exe\" -X utf8 -m pip install --no-index --find-links=\"%BASE%wheelhouse\" -r \"%BASE%requirements.txt\"\n",
        "echo [OK] Portable Python + packages installed.\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "        (out_dir / \"run_offline_portable.bat\").write_text(r\"\"\"@echo off\n",
        "chcp 65001 >nul\n",
        "set PYTHONUTF8=1\n",
        "set PYTHONIOENCODING=utf-8\n",
        "set BASE=%~dp0\n",
        "set EMBED=%BASE%py_embed\n",
        "set TFHUB_CACHE_DIR=%BASE%models\\tfhub_cache\n",
        "\"%EMBED%\\python.exe\" -X utf8 -c \"import sys,os;print('Python:',sys.version);print('TFHUB_CACHE_DIR=',os.getenv('TFHUB_CACHE_DIR'))\"\n",
        "echo Ready. Use:\n",
        "echo    \"%EMBED%\\python.exe\" -X utf8 main.py\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"\\n[OFFLINE BUNDLE READY] {out_dir.resolve()}\")\n",
        "\n",
        "# ============== Î≤àÎì§ ÏÉùÏÑ± + Ïã§Ìñâ ==============\n",
        "if PACK_OFFLINE and IN_COLAB:\n",
        "    print(\"[PREP] Pre-fetch TF-Hub YAMNet to cache...\")\n",
        "    _ = make_yam_infer()  # hub.load ‚Üí TFHUB_CACHE_DIRÏóê Ï∫êÏãú ÏÉùÏÑ±\n",
        "    print(\"[PREP] Building Windows offline bundle...\")\n",
        "    prepare_offline_windows_bundle(WIN_PY, gen_portable=GEN_PORTABLE, verbose=False, use_minimal_reqs=True)\n",
        "\n",
        "# ColabÏóêÏÑúÎèÑ Î∞îÎ°ú ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Í∞ÄÎä•\n",
        "run_all(CFG, VERSIONS)\n",
        "print(\"\\nüéâ ÏôÑÎ£å\")"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4a7b12-a666-4d59-caf8-b4e416333a13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup...\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Data...\n",
            " - ShipsEar exists\n",
            "[PREP] Pre-fetch TF-Hub YAMNet to cache...\n",
            "[YAMNet] hub.load (cached ok)\n",
            "[PREP] Building Windows offline bundle...\n",
            "[OK] Wrote MINIMAL artifacts/offline_bundle_pip_win/requirements.txt (lines: 17)\n",
            "+ python -m pip download -r artifacts/offline_bundle_pip_win/requirements.txt -d artifacts/offline_bundle_pip_win/wheelhouse --platform win_amd64 --python-version 310 --only-binary=:all: --prefer-binary \n",
            "[OK] Copied TF-Hub cache ‚Üí artifacts/offline_bundle_pip_win/models/tfhub_cache\n",
            "\n",
            "[OFFLINE BUNDLE READY] /content/artifacts/offline_bundle_pip_win\n",
            "Build segments...\n",
            " - per-class: {'C': 5085, 'E': 1140, 'D': 1513, 'B': 3134, 'A': 1855} | missing: 0\n",
            "[Split] GroupShuffleSplit | train=10227 test=2500 groups 68/17\n",
            "YAMNet infer...[YAMNet] hub.load (cached ok)\n",
            " OK (ship_idx=11)\n",
            "\n",
            "==== v0a_yamnet_zeroshot ====\n",
            " - zero-shot scoring... OK\n",
            "\n",
            "==== v0b_emb_logreg_basic ====\n",
            " - embeds (train)... - cache cache/emb_train_pool=meanstd_aug=none_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 2.72 GB)\n",
            " - embeds (test)... - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 2.72 GB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  # that has no feature names.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  # that has no feature names.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== v5_meanstd_mlp_aug ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 2.77 GB)\n",
            "  ... 8000/10227 (mem 2.88 GB)\n",
            " OK (10227, 2048) (mem 2.99 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 2.99 GB)\n",
            "\n",
            "==== v6_ft_mean_headonly ====\n",
            " - embeds (train)...  ... 4000/10227 (mem 3.18 GB)\n",
            "  ... 8000/10227 (mem 3.17 GB)\n",
            " OK (10227, 1024) (mem 3.21 GB)\n",
            " - embeds (test)... OK (2500, 1024) (mem 3.21 GB)\n",
            "\n",
            "==== v7_ft_meanstd_headonly ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 3.14 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 3.14 GB)\n",
            "\n",
            "==== v8_ft_meanstd_headonly_tinyLR ====\n",
            " - embeds (train)... OK (10227, 2048) (mem 3.27 GB)\n",
            " - embeds (test)... OK (2500, 2048) (mem 3.27 GB)\n",
            "\n",
            "[SUMMARY]\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9916 0.966473 0.969877  0.994891   NaN 21.624325        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9892 0.948268 0.960499  0.995247   NaN 21.779872 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9876 0.973969 0.957394  0.995494   NaN 21.942016            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9836 0.974220 0.945168  0.997685   NaN 20.872626           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9828 0.908578 0.934808  0.985578   NaN  2.795108                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.471962   NaN  0.000000                                                  \n",
            "\n",
            "Í≤∞Í≥º ÌååÏùº: results/summary.csv, results/report.md, results/cm_*.json, artifacts/*\n",
            "\n",
            "üéâ ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD ÌèâÍ∞Ä Î™®Îìà =================================\n",
        "# Ïù¥ Î∏îÎ°ùÏùÄ Í∏∞Ï°¥ ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú ÌïôÏäµÏù¥ ÎÅùÎÇú ÌõÑÏóê Î∂ôÏó¨ Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
        "# ÌïÑÏöî Ï†ÑÏó≠: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(ÌïôÏäµÎêú Î∂ÑÎ•òÍ∏∞), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE Í≤ΩÎ°ú\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) GitÏóêÏÑú OOD ÏÉòÌîå Ïò§ÎîîÏò§ Í∞ÄÎ≥çÍ≤å ÏàòÏßë ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # ÏÜåÌòï ÏòàÏ†ú/ÌÖåÏä§Ìä∏ Ïò§ÎîîÏò§Í∞Ä ÎπÑÍµêÏ†Å Îì§Ïñ¥ÏûàÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏùå\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] Î¶¨Ìè¨ÏßÄÌÜ†Î¶¨ ÏàòÏßë ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# Ïò§ÎîîÏò§ ÌôïÏû•Ïûê Ìå®ÌÑ¥(ÎÑìÍ≤å Ïû°Îêò Í∞úÏàò Ï†úÌïú)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # ÎÑàÎ¨¥ ÎßéÏùÄ Í≤ΩÏö∞ ÏÉòÌîåÎßÅ\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - ÏàòÏßëÎêú OOD ÏõêÎ≥∏ ÌååÏùº: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD ÏÑ∏Í∑∏Î®ºÌä∏(5Ï¥à) Ïä§Ìä∏Î¶¨Î∞ç ÏÉùÏÑ± ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load ÏóÜÏù¥ Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú 5Ï¥à Íµ¨Í∞ÑÏùÑ Í∑†Ïùº Ïä§Ìä∏ÎùºÏù¥ÎìúÎ°ú ÏµúÎåÄ capÎßå Ï∂îÏ∂ú\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # Í∑†Ïùº Ïä§Ìä∏ÎùºÏù¥ÎìúÎ°ú ÏãúÏûëÏ†ê ÌõÑÎ≥¥ ÏÉùÏÑ±\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# ÎÑàÎ¨¥ ÎßéÏù¥ ÎΩëÏßÄ ÏïäÎèÑÎ°ù Ï†ÑÏ≤¥ cap (Ïòà: 800 ÏÑ∏Í∑∏Î®ºÌä∏)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - ÏÉùÏÑ±Îêú OOD ÏÑ∏Í∑∏Î®ºÌä∏: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD ÏûÑÎ≤†Îî© ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(Ï†ïÍ∑úÌôî Ï†ÑÏóê)ÎèÑ Ï†ÄÏû•Ìï¥ ÏóêÎÑàÏßÄ Ìé∏Ìñ• Î∂ÑÏÑù\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD ÏûÑÎ≤†Îî© {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] ÏûÑÎ≤†Îî© Ï∂îÏ∂ú ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"Í≤ΩÍ≥†: OOD ÏûÑÎ≤†Îî©Ïù¥ ÎπÑÏóàÏäµÎãàÎã§. Î¶¨Ìè¨ ÏÜåÏä§ÎÇò max_total, capÏùÑ Ï°∞Ï†ïÌï¥Î≥¥ÏÑ∏Ïöî.\")\n",
        "\n",
        "# ---------- 4) ÏûÑÍ≥ÑÍ∞í ÏÑ†ÌÉù(Í≤ÄÏ¶ùÏÖã TPR=95%) & ID/OOD FPR ÎπÑÍµê ----------\n",
        "# ÌïôÏäµÏóê ÏÇ¨Ïö©Ìïú trainÏóêÏÑú validationÏùÑ Î∂ÑÎ¶¨(Í∞ÑÎã®Ìûà 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# Ïû¨ÌïôÏäµ ÏóÜÏù¥ clfÎ•º Ïû¨ÏÇ¨Ïö©ÌïòÎêò, val ÌôïÎ•†Îßå ÏÉàÎ°ú Ï∂îÏ†ï\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPRÏù¥ targetÏóê Í∞ÄÏû• Í∑ºÏ†ëÌïú Ï†êÏùò threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[ÏûÑÍ≥ÑÍ∞í] TPR@val‚âà95% ‚Üí œÑ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-ÌÖåÏä§Ìä∏ FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@œÑ={fpr_id:.4f} | OOD FPR@œÑ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) ÏãúÍ∞ÅÌôî: ÌôïÎ•† Î∂ÑÌè¨ / ROC-PR / ÏóêÎÑàÏßÄ Ìé∏Ìñ• ----------\n",
        "# (a) ÌôïÎ•† ÌûàÏä§ÌÜ†Í∑∏Îû®\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"œÑ={tau:.2f}\")\n",
        "plt.title(\"Ship ÌôïÎ•† Î∂ÑÌè¨(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID Í∏∞Ï§Ä)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) ÏóêÎÑàÏßÄ decile Î≥Ñ FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMSÏôÄ ÌôïÎ•†\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (ÎÇÆÏùå‚ÜíÎÜíÏùå)\")\n",
        "    plt.ylabel(f\"FPR@œÑ\"); plt.title(\"ÏóêÎÑàÏßÄ Íµ¨Í∞ÑÎ≥Ñ FPR (ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile Î∂ÑÏÑùÏùÑ ÏúÑÌïú Ïú†Ìö® ÌëúÎ≥∏Ïù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§.\")\n",
        "\n",
        "print(\"\\n[ÏöîÏïΩ]\")\n",
        "print(f\" - ÏûÑÍ≥ÑÍ∞í œÑ(Val TPR‚âà95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@œÑ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@œÑ: {fpr_ood:.4f} (ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - Í∑∏ÎûòÌîÑ: ÌôïÎ•†Î∂ÑÌè¨/ROC/PR/ÏóêÎÑàÏßÄ-ÎîîÏÇ¨Ïùº FPRÏúºÎ°ú, ÏóêÎÑàÏßÄ-Ìé∏Ìñ• Ïó¨Î∂ÄÎ•º Ìï®Íªò Ï†êÍ≤Ä\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10cba29f",
        "outputId": "b6e9ceeb-77f1-4bb4-fcbd-da9710bd86d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "output_filename = 'offline_bundle_pip_win.zip'\n",
        "directory_to_zip = 'artifacts/offline_bundle_pip_win'\n",
        "\n",
        "# Create a zip archive of the directory\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', directory_to_zip)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(output_filename)\n",
        "\n",
        "print(f\"'{directory_to_zip}' Ìè¥ÎçîÍ∞Ä '{output_filename}'ÏúºÎ°ú ÏïïÏ∂ïÎêòÏñ¥ Îã§Ïö¥Î°úÎìúÎê©ÎãàÎã§.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a8259313-299b-456c-9830-2407a170e9b9\", \"offline_bundle_pip_win.zip\", 532653704)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'artifacts/offline_bundle_pip_win' Ìè¥ÎçîÍ∞Ä 'offline_bundle_pip_win.zip'ÏúºÎ°ú ÏïïÏ∂ïÎêòÏñ¥ Îã§Ïö¥Î°úÎìúÎê©ÎãàÎã§.\n"
          ]
        }
      ]
    }
  ]
}