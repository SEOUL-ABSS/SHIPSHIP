{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb",
      "authorship_tag": "ABX9TyN5hBPTG3RB41ULOq8Stgeu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/5Model_XAI_ADDED\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ShipsEar – Part 1 v2-mini (Colab)\n",
        "# 데이터 준비 · 학습(임베딩/MLP/제로샷) · 시각화 + 스펙트럼 근거(시간/주파수 오클루전) + 전역 밴드 요약\n",
        "\n",
        "print(\"Setup…\")\n",
        "!pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1 librosa==0.10.2.post1 soundfile==0.12.1 scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2 scipy==1.13.1\n",
        "!apt -yq install fonts-nanum >/dev/null\n",
        "\n",
        "import os, re, random as rd, math, time, json, glob, shutil, warnings\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import numpy as np, pandas as pd, psutil, soundfile as sf\n",
        "import tensorflow as tf, tensorflow_hub as hub, librosa\n",
        "from tensorflow.keras import mixed_precision as mp\n",
        "import matplotlib.pyplot as plt, seaborn as sns, matplotlib.font_manager as fm\n",
        "from sklearn.preprocessing import LabelEncoder as LE, StandardScaler as SS\n",
        "from sklearn.metrics import (confusion_matrix as CM, f1_score as F1, roc_auc_score as AUC,\n",
        "                             average_precision_score as AP, balanced_accuracy_score as BACC,\n",
        "                             top_k_accuracy_score as TOPK, accuracy_score as ACC,\n",
        "                             roc_curve, precision_recall_curve)\n",
        "from sklearn.model_selection import GroupShuffleSplit as GSS\n",
        "import scipy.signal as sig\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "SEED=42; np.random.seed(SEED); rd.seed(SEED); tf.random.set_seed(SEED)\n",
        "try:\n",
        "    for g in tf.config.experimental.list_physical_devices('GPU'):\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "except: pass\n",
        "mp.set_global_policy(\"mixed_float16\")\n",
        "if os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# --- PATH/CFG ---\n",
        "B=\"/content\"; DSH=f\"{B}/drive/MyDrive/ShipsEar\"; DROOT=f\"{B}/ShipsEar_colab\"\n",
        "os.makedirs(\"results/part1\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True); os.makedirs(\"state\", exist_ok=True)\n",
        "try:\n",
        "    from google.colab import drive; drive.mount('/content/drive', force_remount=False); print(\"Drive mounted.\")\n",
        "except Exception as e: print(\"Not Colab or Drive:\", e)\n",
        "\n",
        "SR=16000; BIN=True; POS=\"Ship\"\n",
        "C=dict(seg_dur=1.0, ship_overlap=0.2, noise_overlap=0.0,\n",
        "       vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "       test_size=0.2, epochs=40, batch=32, lr=5e-4,\n",
        "       max_seg_per_group_per_class=500, noise_jitter_sec=0.5,\n",
        "       topk=1, cache_emb=True)\n",
        "\n",
        "# toggles\n",
        "P_DATA=True; P_HIST=True; P_CM=True; P_ROCPR=True; MAKE_XAI=True\n",
        "MAKE_OCC=True; PER_CAT=2\n",
        "O_TMS=80; O_THOP=20; O_FMIN=125; O_FMAX=7500; O_NB=32; O_ORD=801\n",
        "G_SUM=True; G_SPLIT='test'; G_MAX=200\n",
        "\n",
        "VS=[\n",
        "    dict(name=\"v0a_yamnet_zeroshot\", type=\"zero\"),\n",
        "    dict(name=\"v0b_emb_logreg_basic\", type=\"emb\", classifier=\"logreg\", pooling=\"meanstd\", aug=None),\n",
        "    dict(name=\"v5_meanstd_mlp_aug\",  type=\"emb\", classifier=\"mlp\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v6_ft_mean_headonly\", type=\"ft\",  pooling=\"mean\",    aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headonly\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_headonly_tinyLR\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "]\n",
        "\n",
        "# --- DATA ---\n",
        "print(\"Data…\")\n",
        "if os.path.exists(DSH):\n",
        "    if not os.path.exists(DROOT) or not os.listdir(DROOT):\n",
        "        shutil.copytree(DSH, DROOT, dirs_exist_ok=True); print(\" - Copied ShipsEar\")\n",
        "    else: print(\" - ShipsEar exists\")\n",
        "else: raise FileNotFoundError(f\"ShipsEar not found: {DSH}\")\n",
        "\n",
        "EPS=1e-12\n",
        "KW={\"A\":[\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"],\n",
        "    \"B\":[\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"],\n",
        "    \"C\":[\"ferry\",\"passenger\"],\n",
        "    \"D\":[\"oceanliner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"],\n",
        "    \"E\":[\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]}\n",
        "\n",
        "mem=lambda: f\"{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "rc=lambda p:(os.path.basename(os.path.dirname(p))+\" \"+os.path.basename(p)).lower()\n",
        "\n",
        "def cls_of(p):\n",
        "    t=rc(p)\n",
        "    for c,k in ((\"E\",KW[\"E\"]),(\"A\",KW[\"A\"]),(\"B\",KW[\"B\"]),(\"C\",KW[\"C\"]),(\"D\",KW[\"D\"])):\n",
        "        if any(s in t for s in k): return c\n",
        "    m=re.search(r'\\bclass[_\\s-]*([abcde])\\b', t); return m.group(1).upper() if m else None\n",
        "\n",
        "def grp_of(p):\n",
        "    s=os.path.splitext(os.path.basename(p))[0]\n",
        "    m=re.search(r'(\\d{8}[_-]?\\d{4})', s) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', s)\n",
        "    if m: return m.group(1)\n",
        "    par=os.path.basename(os.path.dirname(p)); tok=re.split(r'[_\\-]+', s); pre=\"_\".join(tok[:3]) if len(tok)>=3 else s\n",
        "    return f\"{par}:{pre}\"\n",
        "\n",
        "def act_of(fp, top_db=25.0, fr=0.5, hop=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(fp) as f:\n",
        "            sr=f.samplerate; n=len(f); F=max(1,int(fr*sr)); H=max(1,int(hop*sr))\n",
        "            mx=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); mx=max(mx,20*np.log10(rms+EPS)); pos+=H\n",
        "            if not np.isfinite(mx): return [], []\n",
        "            th=mx-top_db; A=[]; on=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                db=20*np.log10(float(np.sqrt(np.mean(y**2))+EPS)); t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not on: on=True; cur=t0\n",
        "                else:\n",
        "                    if on: on=False; A.append((cur,t1))\n",
        "                pos+=H\n",
        "            if on: A.append((cur,n/sr))\n",
        "            I=[]; last=0.0; dur=n/sr\n",
        "            for s,e in A:\n",
        "                if s>last: I.append((last,s)); last=e\n",
        "            if last<dur: I.append((last,dur))\n",
        "            return A,I\n",
        "    except: return [], []\n",
        "\n",
        "spans=lambda S,d,h:[(float(st),) for s,e in S for st in np.arange(s, e-d+1e-9, h)]\n",
        "\n",
        "def build_segs(root,C):\n",
        "    d=C['seg_dur']; hop_s=d*(1-C['ship_overlap']); hop_n=d*(1-C['noise_overlap'])\n",
        "    jit=C['noise_jitter_sec']; cap=C['max_seg_per_group_per_class']\n",
        "    infos,labels,groups,miss,pc=[],[],[],0,defaultdict(int); summ=defaultdict(int)\n",
        "    for fp in glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True):\n",
        "        c=cls_of(fp)\n",
        "        if c is None: miss+=1; continue\n",
        "        try: inf=sf.info(fp)\n",
        "        except: continue\n",
        "        gk=grp_of(fp)\n",
        "        if c in \"ABCD\":\n",
        "            A,_=act_of(fp,C['vad_top_db'],C['vad_frame_sec'],C['vad_hop_sec']); S=spans(A,d,hop_s)\n",
        "        else:\n",
        "            S=spans([(0.0, inf.frames/inf.samplerate)], d, hop_n)\n",
        "        rd.shuffle(S)\n",
        "        for (st,) in S:\n",
        "            if c==\"E\" and jit>0:\n",
        "                j=rd.uniform(-jit, jit); st=max(0.0, min(st+j,(inf.frames/inf.samplerate)-d))\n",
        "            key=(gk,c)\n",
        "            if cap and pc[key]>=cap: continue\n",
        "            infos.append((fp,float(st),inf.samplerate)); labels.append(c); groups.append(gk); pc[key]+=1; summ[c]+=1\n",
        "    return infos,labels,groups,summ,miss\n",
        "\n",
        "# --- IO/EMB ---\n",
        "_WC=OrderedDict(); _WC_BYTES=0; _WC_MAX=256*1024*1024\n",
        "\n",
        "_cget=lambda k: (_WC.get(k) if not _WC.get(k) is None else None)\n",
        "\n",
        "def _cput(k,arr):\n",
        "    global _WC_BYTES; sz=getattr(arr,'nbytes',None) or (getattr(arr,'size',0)*getattr(arr,'itemsize',0)); _WC[k]=arr; _WC.move_to_end(k); _WC_BYTES+=sz\n",
        "    while _WC_BYTES>_WC_MAX and len(_WC)>1:\n",
        "        kk,v=_WC.popitem(last=False)\n",
        "        try: _WC_BYTES-=v.nbytes\n",
        "        except: pass\n",
        "\n",
        "def resamp(y,sr0,sr1):\n",
        "    if sr0==sr1: return y.astype(np.float32)\n",
        "    try:\n",
        "        g=math.gcd(int(sr0),int(sr1)); up=int(sr1)//g; dn=int(sr0)//g\n",
        "        return sig.resample_poly(y, up, dn).astype(np.float32)\n",
        "    except Exception:\n",
        "        try: return librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr1, res_type='fft').astype(np.float32)\n",
        "        except Exception:\n",
        "            n=int(round(len(y)*float(sr1)/float(sr0))); xp=np.arange(len(y)); x=np.linspace(0,len(y),n,endpoint=False); return np.interp(x,xp,y).astype(np.float32)\n",
        "\n",
        "def load_seg(info,dur,tar_sr=SR,norm=True):\n",
        "    fp,st,sr0=info\n",
        "    try:\n",
        "        yF=_cget(fp)\n",
        "        if yF is None:\n",
        "            yF,sr=sf.read(fp,dtype='float32',always_2d=False); yF=yF.mean(axis=1) if yF.ndim>1 else yF\n",
        "            if sr!=tar_sr: yF=resamp(yF,sr,tar_sr); _cput(fp,yF)\n",
        "        L=int(dur*tar_sr); s=int(st*tar_sr)\n",
        "        if s>=len(yF): return None\n",
        "        y=yF[s:min(s+L,len(yF))]\n",
        "        if len(y)<L: y=np.pad(y,(0,L-len(y)))\n",
        "        if norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y*=(10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\",e); return None\n",
        "\n",
        "# --- YAMNet ---\n",
        "YURL=\"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "def mk_infer():\n",
        "    mod=hub.load(YURL)\n",
        "    def f(y): return mod(tf.convert_to_tensor(y,tf.float32))\n",
        "    _=f(np.zeros(SR,np.float32))\n",
        "    shp=[]\n",
        "    try:\n",
        "        p=mod.class_map_path().numpy().decode('utf-8'); df=pd.read_csv(p); col='display_name' if 'display_name' in df.columns else df.columns[-1]\n",
        "        names=df[col].astype(str).str.lower().tolist(); subs=[\"boat\",\"ship\",\"sail\",\"sailing\",\"ferry\",\"cargo\",\"tanker\",\"submarine\",\"motorboat\",\"watercraft\",\"water vehicle\",\"ocean liner\",\"yacht\",\"kayak\",\"canoe\",\"rowboat\",\"row\",\"fishing\"]\n",
        "        shp=[i for i,n in enumerate(names) if any(s in n for s in subs)]\n",
        "    except Exception: pass\n",
        "    return f, shp, mod\n",
        "\n",
        "def _emb_out(o):\n",
        "    e=None\n",
        "    if isinstance(o,(list,tuple)) and len(o)>=2: e=o[1]\n",
        "    elif isinstance(o,dict):\n",
        "        e=o.get('embeddings') or o.get('embedding')\n",
        "        if e is None:\n",
        "            for v in o.values():\n",
        "                if isinstance(v,dict):\n",
        "                    e=v.get('embeddings') or v.get('embedding');\n",
        "                    if e is not None: break\n",
        "    if e is None: return None\n",
        "    t=tf.convert_to_tensor(e)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: t=tf.expand_dims(t,0)\n",
        "    return t\n",
        "\n",
        "def emb1(infer,y,pool=\"meanstd\"):\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        t=_emb_out(infer(y))\n",
        "        if t is None or t.shape.rank!=2 or int(t.shape[0])==0: return None\n",
        "        if pool==\"mean\": feat=tf.reduce_mean(t,0)\n",
        "        else: m=tf.reduce_mean(t,0); s=tf.math.reduce_std(t,0); feat=tf.concat([m,s],0)\n",
        "        return feat.numpy().astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR emb:\",e); return None\n",
        "\n",
        "def embN(infos,infer,C,pool=\"meanstd\",aug=None,ck=None,show=4000):\n",
        "    path=None\n",
        "    if C['cache_emb'] and ck:\n",
        "        path=f\"cache/emb_{ck}.npz\"\n",
        "        if os.path.exists(path):\n",
        "            z=np.load(path,allow_pickle=True); print(f\" - cache {path} | X:{z['X'].shape} keep:{z['keep'].shape}\"); return z['X'], z['keep']\n",
        "    X=[]; keep=[]\n",
        "    for i,inf in enumerate(infos,1):\n",
        "        y=load_seg(inf,C['seg_dur'],SR,True)\n",
        "        if aug:\n",
        "            y=y*(10**(rd.uniform(-3,3)/20)); sh=rd.randint(-int(0.25*SR),int(0.25*SR))\n",
        "            if sh>0: y=np.concatenate([np.zeros(sh,dtype=y.dtype),y[:-sh]])\n",
        "            elif sh<0: y=np.concatenate([y[-sh:],np.zeros(-sh,dtype=y.dtype)])\n",
        "        e=emb1(infer,y,pool);\n",
        "        if e is not None: X.append(e); keep.append(i-1)\n",
        "        if i%show==0: print(f\"  ... {i}/{len(infos)} (mem {mem()})\")\n",
        "    X=np.asarray(X,np.float32); keep=np.array(keep,np.int64)\n",
        "    if path and X.size>0: np.savez_compressed(path,X=X,keep=keep)\n",
        "    if X.size==0: print(\"ERR: no embeddings\")\n",
        "    return X,keep\n",
        "\n",
        "def y_sc(infer,y):\n",
        "    o=infer(y); s=None\n",
        "    if isinstance(o,(list,tuple)) and len(o)>=1: s=o[0]\n",
        "    elif isinstance(o,dict): s=o.get('scores') or o.get('predictions')\n",
        "    if s is None: return None\n",
        "    t=tf.convert_to_tensor(s)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: return t.numpy().astype(np.float32)\n",
        "    return tf.reduce_mean(t,0).numpy().astype(np.float32)\n",
        "\n",
        "# --- MODELS/plots ---\n",
        "\n",
        "def mlp(in_dim,n_cls,lr):\n",
        "    r=tf.keras.regularizers.l2(1e-4)\n",
        "    x=tf.keras.Input((in_dim,)); h=tf.keras.layers.BatchNormalization()(x)\n",
        "    h=tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=r)(h); h=tf.keras.layers.Dropout(0.5)(h)\n",
        "    h=tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=r)(h); h=tf.keras.layers.Dropout(0.4)(h)\n",
        "    y=tf.keras.layers.Dense(n_cls,activation='softmax')(h)\n",
        "    m=tf.keras.Model(x,y); m.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='categorical_crossentropy', metrics=['accuracy']); return m\n",
        "\n",
        "p_hist=lambda h,p: (plt.figure(figsize=(8,3.2)) or plt.plot(h.epoch,h.history.get('loss',[])) or (('val_loss' in h.history) and plt.plot(h.epoch,h.history['val_loss'])) or (('accuracy' in h.history) and plt.plot(h.epoch,h.history['accuracy'])) or (('val_accuracy' in h.history) and plt.plot(h.epoch,h.history['val_accuracy'])) or plt.xlabel('epoch') or plt.legend(['loss','val_loss','acc','val_acc']) or plt.title('Training') or plt.tight_layout() or plt.savefig(p,dpi=150) or plt.close())\n",
        "\n",
        "def p_cm(cm,cls,p,title='CM'):\n",
        "    plt.figure(figsize=(5.2,4.5)); sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',xticklabels=cls,yticklabels=cls)\n",
        "    plt.xlabel('Pred'); plt.ylabel('True'); plt.title(title); plt.tight_layout(); plt.savefig(p,dpi=150); plt.close()\n",
        "\n",
        "def p_rocpr(y,pp,pre):\n",
        "    fpr,tpr,_=roc_curve(y,pp); pr,rc,_=precision_recall_curve(y,pp)\n",
        "    plt.figure(figsize=(4.2,3.6)); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.tight_layout(); plt.savefig(pre+\"_roc.png\",dpi=150); plt.close()\n",
        "    plt.figure(figsize=(4.2,3.6)); plt.plot(rc,pr); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR'); plt.tight_layout(); plt.savefig(pre+\"_pr.png\",dpi=150); plt.close()\n",
        "\n",
        "def p_dist(d,p):\n",
        "    it=sorted(d.items()); lab=[k for k,_ in it]; v=[x for _,x in it]\n",
        "    plt.figure(figsize=(6,3.2)); sns.barplot(x=lab,y=v); plt.title('세그먼트 수'); plt.tight_layout(); plt.savefig(p,dpi=150); plt.close()\n",
        "\n",
        "# --- XAI (Grad×Input on log-mel) ---\n",
        "\n",
        "def grad_sal(mod,w,ship_idx,sm_n=0,sm_s=0.001):\n",
        "    def one(z):\n",
        "        with tf.GradientTape() as t:\n",
        "            zt=tf.convert_to_tensor(z,tf.float32); sc,emb,sp=mod(zt); t.watch(sp)\n",
        "            p=tf.gather(sc,ship_idx,axis=1); agg=1.0-tf.reduce_prod(1.0-p,axis=1); tgt=tf.reduce_mean(agg)\n",
        "        g=t.gradient(tgt,sp); s=tf.nn.relu(g*sp); return s.numpy()\n",
        "    if sm_n>0:\n",
        "        rms=float(np.sqrt(np.mean(w*w))+1e-12); std=sm_s*(rms if rms>0 else 1.0); acc=None\n",
        "        for _ in range(sm_n):\n",
        "            wn=w+np.random.normal(0.0,std,size=w.shape).astype(np.float32); s=one(wn); acc=s if acc is None else acc+s\n",
        "        sal=acc/float(sm_n)\n",
        "    else: sal=one(w)\n",
        "    return np.maximum(0.0, sal)\n",
        "\n",
        "def p_sal(t,mel,sal,out,a=0.6,pr=95.0):\n",
        "    v=np.percentile(sal,pr) if np.isfinite(sal).any() else 1.0; v=1.0 if v<=0 else v; s=np.clip(sal/v,0.0,1.0)\n",
        "    plt.figure(figsize=(10,3.8)); dt=(t[1]-t[0]) if len(t)>1 else 0.48\n",
        "    if mel is not None:\n",
        "        plt.imshow(mel.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+dt,0,s.shape[1]])\n",
        "        plt.imshow(s.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+dt,0,s.shape[1]],alpha=a); plt.title('Spec+Saliency')\n",
        "    else:\n",
        "        plt.imshow(s.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+dt,0,s.shape[1]]); plt.title('Saliency')\n",
        "    plt.xlabel('Time(s)'); plt.ylabel('Mel bin'); plt.colorbar(); plt.tight_layout(); plt.savefig(out,dpi=150); plt.close()\n",
        "\n",
        "# --- Occlusion utils ---\n",
        "ship_p=lambda sc,idx: (float(1.0-np.prod(1.0-np.mean(sc,0)[idx])) if sc is not None and len(idx)>0 else float('nan'))\n",
        "pship=lambda f,y,idx: ship_p(y_sc(f,y), idx)\n",
        "\n",
        "def occ_t(f,y,sr,idx,win=80,hop=20):\n",
        "    base=pship(f,y,idx);\n",
        "    if not np.isfinite(base): return None,None,None\n",
        "    w=int(max(1,round(win/1000.0*sr))); h=int(max(1,round(hop/1000.0*sr))); L=len(y)\n",
        "    ctr,dp,pr=[],[],[]\n",
        "    for st in range(0,max(1,L-w+1),h):\n",
        "        yy=y.copy(); yy[st:st+w]=0.0; p=pship(f,yy,idx); pr.append(p); dp.append(max(0.0,base-p) if np.isfinite(p) else 0.0); ctr.append((st+min(w//2,L-1))/sr)\n",
        "    return np.array(ctr,np.float32), np.array(dp,np.float32), float(base)\n",
        "\n",
        "def bs_taps(sr,f1,f2,ord=801):\n",
        "    ny=sr/2.0; f1=max(1.0,min(f1,ny*0.999)); f2=max(f1+1.0,min(f2,ny*0.999)); return sig.firwin(ord,[f1,f2],pass_zero='bandstop',fs=sr)\n",
        "\n",
        "def bs_apply(y,sr,f1,f2,ord=801):\n",
        "    taps=bs_taps(sr,f1,f2,ord)\n",
        "    try: return sig.filtfilt(taps,[1.0],y,method='pad').astype(np.float32)\n",
        "    except Exception: return sig.lfilter(taps,[1.0],y).astype(np.float32)\n",
        "\n",
        "def occ_f(f,y,sr,idx,fmin=125,fmax=7500,nb=32,ord=801):\n",
        "    base=pship(f,y,idx);\n",
        "    if not np.isfinite(base): return None,None,None\n",
        "    ed=np.geomspace(max(1.0,fmin),min(fmax,sr/2*0.999),num=nb+1); cen=np.sqrt(ed[:-1]*ed[1:])\n",
        "    dp=[];\n",
        "    for a,b in zip(ed[:-1],ed[1:]):\n",
        "        yy=bs_apply(y,sr,a,b,ord); p=pship(f,yy,idx); dp.append(max(0.0,base-p) if np.isfinite(p) else 0.0)\n",
        "    return cen.astype(np.float32), np.array(dp,np.float32), float(base)\n",
        "\n",
        "pt_imp=lambda t,dp,base,out: (plt.figure(figsize=(10,3.0)) or plt.plot(t,dp) or plt.ylabel('ΔP_ship') or plt.xlabel('Time(s)') or plt.title(f'Time Occlusion (base={base:.3f})') or plt.tight_layout() or plt.savefig(out,dpi=150) or plt.close())\n",
        "\n",
        "pf_imp=lambda c,dp,base,out: (plt.figure(figsize=(10,3.0)) or plt.bar(c/1000.0,dp,width=(c*0.12)/1000.0) or plt.xlabel('Freq(kHz)') or plt.ylabel('ΔP_ship') or plt.title(f'Freq Occlusion (base={base:.3f})') or plt.tight_layout() or plt.savefig(out,dpi=150) or plt.close())\n",
        "\n",
        "# --- Split ---\n",
        "\n",
        "def split(y,g,t=0.2,seed=SEED):\n",
        "    if len(y)<2 or len(set(y))<2:\n",
        "        raise RuntimeError(\"[데이터 부족] 세그먼트 수가 너무 적거나 클래스가 2종 미만\")\n",
        "    s=GSS(n_splits=1, test_size=t, random_state=seed); tr,te=next(s.split(np.arange(len(y)),y,g)); return tr,te\n",
        "\n",
        "# --- Train/Eval ---\n",
        "\n",
        "def fit_eval(v,Xtr,ytr,Xte,yte,cls,C,pre):\n",
        "    if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"[emb] empty\")\n",
        "    if v.get('classifier')=='mlp':\n",
        "        m=mlp(Xtr.shape[-1],len(cls),C['lr']); cb=[tf.keras.callbacks.EarlyStopping(patience=8,restore_best_weights=True,monitor='val_loss'), tf.keras.callbacks.ReduceLROnPlateau(patience=4,factor=0.5,min_lr=1e-6)]\n",
        "        ytrc=tf.keras.utils.to_categorical(ytr,len(cls)); ytec=tf.keras.utils.to_categorical(yte,len(cls)); cw={c:len(ytr)/(len(np.unique(ytr))*cnt) for c,cnt in Counter(ytr).items()}\n",
        "        t0=time.time(); hist=m.fit(Xtr,ytrc,validation_data=(Xte,ytec),epochs=C['epochs'],batch_size=C['batch'],verbose=0,class_weight=cw,callbacks=cb)\n",
        "        P=m.predict(Xte,verbose=0).astype(np.float32); pr=P.argmax(1); path=f\"artifacts/{v['name']}_mlp.keras\"; m.save(path); T=time.time()-t0\n",
        "        if P_HIST: p_hist(hist, pre+\"_history.png\")\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression as LR; from sklearn.svm import SVC; import joblib\n",
        "        sc=SS().fit(Xtr); Xtr=sc.transform(Xtr); Xte=sc.transform(Xte); t0=time.time()\n",
        "        if v.get('classifier')=='logreg': clf=LR(max_iter=2000,class_weight='balanced',n_jobs=-1).fit(Xtr,ytr); P=clf.predict_proba(Xte); pr=P.argmax(1)\n",
        "        else: clf=SVC(C=2.0,kernel='rbf',probability=True,class_weight='balanced').fit(Xtr,ytr); P=clf.predict_proba(Xte); pr=P.argmax(1)\n",
        "        T=time.time()-t0; joblib.dump(clf,f\"artifacts/{v['name']}_{v.get('classifier','svm')}.joblib\"); joblib.dump(sc,f\"artifacts/{v['name']}_scaler.joblib\"); path=\"artifacts/*\"\n",
        "    yt=yte; res=dict(artifact=path,time_sec=T, acc=ACC(yt,pr), bal_acc=BACC(yt,pr), macroF1=F1(yt,pr,average='macro'))\n",
        "    try: res['macroROC']=AUC(tf.keras.utils.to_categorical(yt,len(cls)), P, average='macro', multi_class='ovr')\n",
        "    except: res['macroROC']=np.nan\n",
        "    try: res['topk']=TOPK(yt,P,k=C['topk'],labels=range(len(cls)))\n",
        "    except: res['topk']=np.nan\n",
        "    ap={};\n",
        "    for i,lb in enumerate(cls):\n",
        "        yb=(yt==i).astype(int); ap[lb]=float(AP(yb,P[:,i])) if 0<yb.sum()<len(yb) else float('nan')\n",
        "    res['ap_per_class']=ap; cm=CM(yt,pr); res['cm']=cm\n",
        "    if P_CM: p_cm(cm,cls, pre+\"_cm.png\", title=f\"CM — {os.path.basename(pre)}\")\n",
        "    if BIN and P_ROCPR and len(cls)==2:\n",
        "        pi=cls.index(POS) if POS in cls else 1; p_ = P[:,pi]\n",
        "        try: res['macroROC']=AUC((yt==pi).astype(int), p_)\n",
        "        except: pass\n",
        "        p_rocpr((yt==pi).astype(int), p_, pre)\n",
        "    res['probs']=P; res['pred']=pr; res['y_true']=yt\n",
        "    return res\n",
        "\n",
        "# --- RUN ---\n",
        "print(\"Build segs…\")\n",
        "INF,LBL,GRP,SUM,MS=build_segs(DROOT,C)\n",
        "print(f\" - per-class: {dict(SUM)} | missing: {MS}\")\n",
        "if BIN: LBL=[\"Ship\" if l in \"ABCD\" else \"Noise\" for l in LBL]\n",
        "le=LE(); y=le.fit_transform(LBL); CLS=list(le.classes_); g=np.array(GRP)\n",
        "tr,te=split(y,g,C['test_size']); print(f\"[Split] train={len(tr)} test={len(te)} grp {len(set(g[tr]))}/{len(set(g[te]))}\")\n",
        "Xtr_i=[INF[i] for i in tr]; ytr=y[tr]; Xte_i=[INF[i] for i in te]; yte=y[te]\n",
        "if P_DATA:\n",
        "    dtr=Counter([LBL[i] for i in tr]); dte=Counter([LBL[i] for i in te])\n",
        "    p_dist(dtr, \"results/part1/train_class_dist.png\"); p_dist(dte, \"results/part1/test_class_dist.png\")\n",
        "\n",
        "print(\"YAMNet…\", end=\"\")\n",
        "infer, ship_idx, MOD = mk_infer(); print(f\" OK (ship_idx={len(ship_idx)})\")\n",
        "\n",
        "FB={}\n",
        "getF=lambda tag,infos,pool,aug: (FB.setdefault((tag,pool,aug or 'none',C['seg_dur'],len(infos)), embN(infos,infer,C,pool,aug, ck=f\"{tag}_pool={pool}_aug={(aug or 'none')}_seg={C['seg_dur']}s\")))\n",
        "\n",
        "rows=[]; PV={}\n",
        "for v in VS:\n",
        "    print(f\"\\n==== {v['name']} ====\")\n",
        "    pre=f\"results/part1/{v['name']}\"\n",
        "    if v['type'] in ('emb','ft'):\n",
        "        pool=v.get('pooling','meanstd'); aug=v.get('aug',None)\n",
        "        print(\" - emb train…\", end=\"\"); Xtr,kt=getF('train',Xtr_i,pool,aug); ytr_v=ytr[kt]; print(f\" OK {Xtr.shape} (mem {mem()})\")\n",
        "        print(\" - emb test …\", end=\"\"); Xte,ke=getF('test', Xte_i,pool,None); yte_v=yte[ke]; print(f\" OK {Xte.shape} (mem {mem()})\")\n",
        "        if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"emb fail\")\n",
        "        res=fit_eval((dict(v, classifier='mlp') if v['type']=='ft' else v), Xtr,ytr_v,Xte,yte_v,CLS,C,pre)\n",
        "    elif v['type']=='zero':\n",
        "        if not ship_idx: print(\" - skip zero\"); continue\n",
        "        print(\" - zero…\", end=\"\")\n",
        "        def scoreL(infos):\n",
        "            s=[]; idx=[]\n",
        "            for i,inf in enumerate(infos):\n",
        "                yv=load_seg(inf,C['seg_dur'],SR,True)\n",
        "                if yv is None: continue\n",
        "                sc=y_sc(infer,yv)\n",
        "                if sc is None: continue\n",
        "                s.append(float(1.0-np.prod(1.0-sc[ship_idx]))); idx.append(i)\n",
        "            return np.array(s,np.float32), np.array(idx,np.int64)\n",
        "        s_tr,ktr=scoreL(Xtr_i); s_te,kte=scoreL(Xte_i); ytr_v=ytr[ktr]; yte_v=yte[kte]\n",
        "        pi=CLS.index(POS) if POS in CLS else 1; ytr_b=(ytr_v==pi).astype(int); yte_b=(yte_v==pi).astype(int)\n",
        "        t_best=0.5; f_best=-1.0\n",
        "        for t in np.linspace(0,1,21):\n",
        "            f=F1(ytr_b,(s_tr>=t).astype(int),average='binary',zero_division=0)\n",
        "            if f>f_best: f_best=f; t_best=float(t)\n",
        "        pr=(s_te>=t_best).astype(int)\n",
        "        res=dict(artifact=\"\",time_sec=0.0, acc=ACC(yte_b,pr), bal_acc=BACC(yte_b,pr), macroF1=F1(yte_b,pr,average='macro'), macroROC=(AUC(yte_b,s_te) if len(np.unique(yte_b))==2 else np.nan), topk=np.nan, ap_per_class={POS: float(AP(yte_b,s_te))}, cm=CM(yte_b,pr), probs=np.vstack([1.0-s_te,s_te]).T, pred=pr, y_true=yte_v)\n",
        "        print(\" OK\");\n",
        "        if P_CM: p_cm(res['cm'], CLS, pre+\"_cm.png\", title=f\"CM — {v['name']}\")\n",
        "        if BIN and P_ROCPR and len(CLS)==2: p_rocpr((yte_v==pi).astype(int), s_te, pre)\n",
        "    else:\n",
        "        print(\" - unknown; skip\"); continue\n",
        "\n",
        "    rows.append((dict(version=v['name'], type=v['type'], pooling=v.get('pooling','-'), classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'), aug=(v.get('aug') or 'none'), acc=res['acc'], bal_acc=res['bal_acc'], macroF1=res['macroF1'], macroROC=res['macroROC'], topk=res['topk'], time_sec=res['time_sec'], artifact=res.get('artifact','')), res))\n",
        "    PV[v['name']]={\"cm_png\": pre+\"_cm.png\" if P_CM else None, \"history_png\": pre+\"_history.png\" if (P_HIST and v.get('classifier')=='mlp') else None, \"roc_png\": pre+\"_roc.png\" if (BIN and P_ROCPR and len(CLS)==2) else None, \"pr_png\": pre+\"_pr.png\" if (BIN and P_ROCPR and len(CLS)==2) else None}\n",
        "\n",
        "if not rows:\n",
        "    print(\"No results. Check data path.\")\n",
        "else:\n",
        "    DF=pd.DataFrame([r[0] for r in rows]).sort_values([\"macroF1\",\"bal_acc\",\"acc\"],ascending=False)\n",
        "    DF.to_csv(\"results/part1/summary_part1.csv\", index=False)\n",
        "    print(\"\\n[SUMMARY]\"); print(DF.to_string(index=False))\n",
        "\n",
        "# --- SPEC/XAI for best ---\n",
        "spec_dir=None\n",
        "if (MAKE_XAI or MAKE_OCC) and rows:\n",
        "    best=DF.iloc[0]['version'] if 'DF' in globals() else rows[0][0]['version']\n",
        "    rb=None; rres=None\n",
        "    for r,res in rows:\n",
        "        if r['version']==best: rb=r; rres=res; break\n",
        "    if rres is not None:\n",
        "        print(f\"\\n[XAI/SPEC] best: {best}\")\n",
        "        pi=CLS.index(POS) if POS in CLS else 1\n",
        "        P=rres['probs']; yt=rres['y_true']; pr=rres['pred']\n",
        "        pp=P[:,pi] if P.ndim==2 else P; yb=(yt==pi).astype(int)\n",
        "        tp=np.where((yb==1)&(pr==pi))[0]; fn=np.where((yb==1)&(pr!=pi))[0]; bd=np.where((pp>=0.45)&(pp<=0.55))[0]\n",
        "        pick=lambda a: a[:PER_CAT] if len(a)>PER_CAT else a\n",
        "        cats={\"tp\":pick(tp),\"fn\":pick(fn),\"bd\":pick(bd)}\n",
        "        spec_dir=f\"results/part1/xai_spec_v2_{best}\"; os.makedirs(spec_dir, exist_ok=True)\n",
        "        idx_to_info=lambda idx: Xte_i[idx] if idx < len(Xte_i) else Xte_i[-1]\n",
        "        for tag,arr in cats.items():\n",
        "            for j,idx in enumerate(arr):\n",
        "                info=idx_to_info(idx); yv=load_seg(info, C['seg_dur'], SR, True)\n",
        "                if yv is None: continue\n",
        "                try:\n",
        "                    sc,emb,sp=MOD(tf.convert_to_tensor(yv,tf.float32)); T=int(sc.shape[0]); t=np.arange(T,dtype=np.float32)*0.48; mel=sp.numpy() if sp is not None else None\n",
        "                    sal=grad_sal(MOD,yv,ship_idx,sm_n=4,sm_s=0.002); p_sal(t,mel,sal, os.path.join(spec_dir,f\"{tag}_{j}_spec_grad.png\"))\n",
        "                except Exception as e:\n",
        "                    print(\"[WARN] Grad×Input skip:\",e)\n",
        "                if MAKE_OCC:\n",
        "                    try:\n",
        "                        tt,dp,base=occ_t(infer,yv,SR,ship_idx,O_TMS,O_THOP)\n",
        "                        if tt is not None:\n",
        "                            pt_imp(tt,dp,base, os.path.join(spec_dir,f\"{tag}_{j}_time_occ.png\"))\n",
        "                            with open(os.path.join(spec_dir,f\"{tag}_{j}_time_occ.csv\"),'w') as f: f.write(\"time_sec,delta_p\\n\"+\"\\n\".join([f\"{a:.6f},{b:.6f}\" for a,b in zip(tt,dp)]))\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] time occ skip:\",e)\n",
        "                    try:\n",
        "                        cen,dfp,base2=occ_f(infer,yv,SR,ship_idx,O_FMIN,O_FMAX,O_NB,O_ORD)\n",
        "                        if cen is not None:\n",
        "                            pf_imp(cen,dfp,base2, os.path.join(spec_dir,f\"{tag}_{j}_freq_occ.png\"))\n",
        "                            with open(os.path.join(spec_dir,f\"{tag}_{j}_freq_occ.csv\"),'w') as f: f.write(\"center_hz,delta_p\\n\"+\"\\n\".join([f\"{a:.2f},{b:.6f}\" for a,b in zip(cen,dfp)]))\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] freq occ skip:\",e)\n",
        "\n",
        "# --- GLOBAL band summary ---\n",
        "\n",
        "def _edges(sr,fmin,fmax,nb):\n",
        "    ed=np.geomspace(max(1.0,fmin), min(fmax, sr/2*0.999), num=nb+1); return ed, np.sqrt(ed[:-1]*ed[1:])\n",
        "\n",
        "def gb_sum(f, infos, y, pi, sr, fmin,fmax,nb,ord, m=None, seed=SEED):\n",
        "    rng=np.random.RandomState(seed); idx=np.arange(len(infos))\n",
        "    if (m is not None) and (m < len(idx)): idx=rng.choice(idx,size=m,replace=False)\n",
        "    ed,cen=_edges(sr,fmin,fmax,nb); S_all=np.zeros(nb,np.float64); S_s=np.zeros(nb,np.float64); S_n=np.zeros(nb,np.float64); c_all=c_s=c_n=0\n",
        "    for i in idx:\n",
        "        yv=load_seg(infos[i], C['seg_dur'], sr, True)\n",
        "        if yv is None: continue\n",
        "        base=pship(f,yv,ship_idx)\n",
        "        if not np.isfinite(base): continue\n",
        "        d=[]\n",
        "        for a,b in zip(ed[:-1],ed[1:]):\n",
        "            yy=bs_apply(yv,sr,a,b,ord); p=pship(f,yy,ship_idx); d.append(max(0.0,base-p) if np.isfinite(p) else 0.0)\n",
        "        d=np.asarray(d,np.float64); S_all+=d; c_all+=1\n",
        "        if y[i]==pi: S_s+=d; c_s+=1\n",
        "        else: S_n+=d; c_n+=1\n",
        "    safe=lambda s,c: (s/max(1,c)).astype(np.float32)\n",
        "    return dict(centers=cen.astype(np.float32), mean_all=safe(S_all,c_all), mean_ship=safe(S_s,c_s), mean_noise=safe(S_n,c_n), n_total=int(c_all), n_ship=int(c_s), n_noise=int(c_n))\n",
        "\n",
        "def gb_fig(d,out):\n",
        "    c=d['centers']/1000.0; plt.figure(figsize=(9.5,3.2)); plt.plot(c,d['mean_all'],label='Overall'); plt.plot(c,d['mean_ship'],label='Ship(true)'); plt.plot(c,d['mean_noise'],label='Noise(true)')\n",
        "    plt.xlabel('Freq(kHz)'); plt.ylabel('ΔP(avg)'); plt.title('Dataset-level Band Importance'); plt.legend(); plt.tight_layout(); plt.savefig(out,dpi=150); plt.close()\n",
        "\n",
        "def gb_csv(d,out):\n",
        "    pd.DataFrame({'center_hz':d['centers'],'delta_p_overall':d['mean_all'],'delta_p_ship_true':d['mean_ship'],'delta_p_noise_true':d['mean_noise']}).to_csv(out,index=False)\n",
        "\n",
        "gb=None\n",
        "if MAKE_OCC and G_SUM and rows:\n",
        "    print(\"\\n[GLOBAL] Band importance…\")\n",
        "    if G_SPLIT=='train': infos_ref,y_ref=Xtr_i,ytr\n",
        "    elif G_SPLIT=='all': infos_ref,y_ref= Xtr_i+Xte_i, np.concatenate([ytr,yte],0)\n",
        "    else: infos_ref,y_ref=Xte_i,yte\n",
        "    pi=CLS.index(POS) if POS in CLS else 1\n",
        "    gb=gb_sum(infer, infos_ref, y_ref, pi, SR, O_FMIN,O_FMAX,O_NB,O_ORD, m=G_MAX, seed=SEED)\n",
        "    if gb is not None:\n",
        "        png=\"results/part1/global_band_importance.png\"; csv_=\"results/part1/global_band_importance.csv\"; gb_fig(gb,png); gb_csv(gb,csv_)\n",
        "        print(f\" - saved: {png}, {csv_} | N={gb['n_total']} (Ship {gb['n_ship']}, Noise {gb['n_noise']})\")\n",
        "\n",
        "# --- MANIFEST ---\n",
        "mani={\n",
        "    \"summary_csv\": \"results/part1/summary_part1.csv\" if os.path.exists(\"results/part1/summary_part1.csv\") else None,\n",
        "    \"per_version\": PV,\n",
        "    \"class_dist\": {\n",
        "        \"train\": \"results/part1/train_class_dist.png\" if os.path.exists(\"results/part1/train_class_dist.png\") else None,\n",
        "        \"test\":  \"results/part1/test_class_dist.png\" if os.path.exists(\"results/part1/test_class_dist.png\") else None,\n",
        "    },\n",
        "    \"xai_dir\": spec_dir,\n",
        "    \"xai_spec_v2\": spec_dir,\n",
        "    \"config\": C,\n",
        "    \"classes\": CLS,\n",
        "    \"occlusion_cfg\": {\"time_win_ms\": O_TMS, \"time_hop_ms\": O_THOP, \"fmin\": O_FMIN, \"fmax\": O_FMAX, \"bands\": O_NB, \"fir_order\": O_ORD},\n",
        "    \"global_band_summary\": {\n",
        "        \"png\": (\"results/part1/global_band_importance.png\" if gb is not None else None),\n",
        "        \"csv\": (\"results/part1/global_band_importance.csv\" if gb is not None else None),\n",
        "        \"n_total\": (gb['n_total'] if gb is not None else 0),\n",
        "        \"n_ship\": (gb['n_ship'] if gb is not None else 0),\n",
        "        \"n_noise\": (gb['n_noise'] if gb is not None else 0)\n",
        "    }\n",
        "}\n",
        "with open(\"state/part1_manifest.json\",\"w\",encoding=\"utf-8\") as f: json.dump(mani,f,ensure_ascii=False,indent=2)\n",
        "print(\"\\n[Part 1 v2-mini 완료]\")\n",
        "print(json.dumps(mani, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e756942d-0247-4f84-aaf4-0251db309b3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup…\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Data…\n",
            " - ShipsEar exists\n",
            "Build segs…\n",
            " - per-class: {'D': 1513, 'B': 3134, 'C': 5085, 'A': 1855, 'E': 1140} | missing: 0\n",
            "[Split] train=10227 test=2500 grp 68/17\n",
            "YAMNet… OK (ship_idx=11)\n",
            "\n",
            "==== v0a_yamnet_zeroshot ====\n",
            " - zero… OK\n",
            "\n",
            "==== v0b_emb_logreg_basic ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=none_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.18 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.18 GB)\n",
            "\n",
            "==== v5_meanstd_mlp_aug ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.26 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.26 GB)\n",
            "\n",
            "==== v6_ft_mean_headonly ====\n",
            " - emb train… - cache cache/emb_train_pool=mean_aug=light_seg=1.0s.npz | X:(10227, 1024) keep:(10227,)\n",
            " OK (10227, 1024) (mem 4.38 GB)\n",
            " - emb test … - cache cache/emb_test_pool=mean_aug=none_seg=1.0s.npz | X:(2500, 1024) keep:(2500,)\n",
            " OK (2500, 1024) (mem 4.38 GB)\n",
            "\n",
            "==== v7_ft_meanstd_headonly ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.49 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.49 GB)\n",
            "\n",
            "==== v8_ft_meanstd_headonly_tinyLR ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.53 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.53 GB)\n",
            "\n",
            "[SUMMARY]\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9936 0.974801 0.977105  0.998855   NaN 21.577783        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9928 0.967122 0.973990  0.995650   NaN 22.475796           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9920 0.964274 0.971100  0.998184   NaN 18.786420 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9892 0.962759 0.961644  0.997774   NaN 20.762111            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9892 0.960344 0.961459  0.994124   NaN  3.347056                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.524714   NaN  0.000000                                                  \n",
            "\n",
            "[XAI/SPEC] best: v7_ft_meanstd_headonly\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "[WARN] Grad×Input skip: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n",
            "[WARN] time occ skip: invalid index to scalar variable.\n",
            "[WARN] freq occ skip: invalid index to scalar variable.\n",
            "\n",
            "[GLOBAL] Band importance…\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2061492645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minfos_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXte_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPOS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCLS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgb_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_FMIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO_FMAX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO_NB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO_ORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG_MAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mpng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"results/part1/global_band_importance.png\"\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcsv_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"results/part1/global_band_importance.csv\"\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgb_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgb_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsv_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2061492645.py\u001b[0m in \u001b[0;36mgb_sum\u001b[0;34m(f, infos, y, pi, sr, fmin, fmax, nb, ord, m, seed)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0myv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_seg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seg_dur'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0myv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpship\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mship_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2061492645.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, y, idx)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;31m# --- Occlusion utils ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0mship_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0mpship\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mship_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mocc_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2061492645.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sc, idx)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;31m# --- Occlusion utils ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0mship_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0mpship\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mship_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x320 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x320 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x320 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x320 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##// 결과데이터 시각화모듈!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# %% [markdown]\n",
        "# # ShipsEar – Part 2 (Colab)\n",
        "# ### 결과 집계 · 전문가용 리포트 생성(HTML/MD) · 자산 정리\n",
        "#\n",
        "# - 이 노트는 **Part 1**에서 생성된 산출물(`state/part1_manifest.json`, `results/part1/*`)을 읽어\n",
        "#   **전문가용 보고서**를 자동 생성합니다.\n",
        "# - 산출물:\n",
        "#   - `results/part2/report.html` — **자체 포함(이미지 Base64)** 단일 HTML 리포트\n",
        "#   - `results/part2/report.md` — 깔끔한 마크다운 요약(상대경로 이미지 링크)\n",
        "#   - `results/part2/summary_plots.png` — 버전별 핵심 지표 묶음 그래프\n",
        "#   - `results/part2/assets/` — 이미지/표 자산 복사본(Part 2용)\n",
        "#   - `state/part2_manifest.json` — Part 2 메타\n",
        "#\n",
        "# ---\n",
        "# ## 기능\n",
        "# - Part 1 요약 CSV 로드 → **최적 버전 선정**(macroF1 → bal_acc → acc)\n",
        "# - 버전별 지표 테이블 및 막대 플롯(정렬/하이라이트)\n",
        "# - CM/ROC/PR 이미지 자동 포함(존재 시)\n",
        "# - (선택) XAI 샘플 오버레이 자동 포함(존재 시)\n",
        "# - 환경/설정 요약(Config/Classes) + 재현을 위한 경로/파일 테이블\n",
        "#\n",
        "# ## 요구사항\n",
        "# - Part 1이 **완료된 동일 런타임/경로**에서 실행\n",
        "# - 추가 설치 불필요(표준 라이브러리 + matplotlib/seaborn/pandas)\n",
        "\n",
        "# %%\n",
        "import os, json, glob, shutil, base64, io, textwrap, sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "BASE = Path('/content')\n",
        "P1_STATE = BASE / 'state/part1_manifest.json'\n",
        "P2_DIR = BASE / 'results/part2'\n",
        "P1_DIR = BASE / 'results/part1'\n",
        "ASSETS = P2_DIR / 'assets'\n",
        "P2_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ASSETS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# %%\n",
        "# ---------------------------- 유틸 ----------------------------\n",
        "\n",
        "def _read_json(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def _to_b64_image(path: Path) -> str:\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            data = f.read()\n",
        "        b64 = base64.b64encode(data).decode('ascii')\n",
        "        ext = path.suffix.lower().lstrip('.')\n",
        "        mime = 'image/png' if ext == 'png' else 'image/jpeg'\n",
        "        return f\"data:{mime};base64,{b64}\"\n",
        "    except Exception:\n",
        "        return ''\n",
        "\n",
        "\n",
        "def _copy_if_exists(src: Path, dst_dir: Path) -> Path | None:\n",
        "    if src and src.exists():\n",
        "        dst = dst_dir / src.name\n",
        "        try:\n",
        "            shutil.copy2(src, dst)\n",
        "            return dst\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# %%\n",
        "# ---------------------------- Part 1 산출물 로드 ----------------------------\n",
        "if not P1_STATE.exists():\n",
        "    raise SystemExit('Part 1 manifest가 없습니다: state/part1_manifest.json')\n",
        "\n",
        "man = _read_json(P1_STATE)\n",
        "summary_csv = man.get('summary_csv') or str(P1_DIR / 'summary_part1.csv')\n",
        "summary_csv_path = Path(summary_csv)\n",
        "if not summary_csv_path.exists():\n",
        "    raise SystemExit(f'요약 CSV가 없습니다: {summary_csv_path}')\n",
        "\n",
        "df = pd.read_csv(summary_csv_path)\n",
        "metric_cols = ['macroF1','bal_acc','acc','macroROC']\n",
        "for c in metric_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "# 최적 버전 선정\n",
        "_df_sorted = df.sort_values(['macroF1','bal_acc','acc'], ascending=False).reset_index(drop=True)\n",
        "best_row = _df_sorted.iloc[0].to_dict()\n",
        "best_version = best_row['version']\n",
        "\n",
        "classes = man.get('classes', [])\n",
        "config = man.get('config', {})\n",
        "per_version = man.get('per_version', {})\n",
        "xai_dir = man.get('xai_dir')\n",
        "\n",
        "print('[Part 2] Loaded summary:')\n",
        "print(_df_sorted.to_string(index=False))\n",
        "print(f\"\\n[Best] {best_version}\")\n",
        "\n",
        "# %%\n",
        "# ---------------------------- 요약 플롯 ----------------------------\n",
        "plt.figure(figsize=(8, 4))\n",
        "show_cols = [c for c in ['macroF1','bal_acc','acc'] if c in df.columns]\n",
        "for i, c in enumerate(show_cols):\n",
        "    sns.barplot(data=_df_sorted, x='version', y=c)\n",
        "    plt.xticks(rotation=30, ha='right')\n",
        "    plt.title(f'{c} by version (sorted by macroF1)')\n",
        "    plt.tight_layout()\n",
        "    out_png = P2_DIR / f'summary_{c}.png'\n",
        "    plt.savefig(out_png, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# 하나의 캔버스에 묶기\n",
        "fig, axes = plt.subplots(1, len(show_cols), figsize=(6*len(show_cols), 3.6), squeeze=False)\n",
        "for ax, c in zip(axes[0], show_cols):\n",
        "    sns.barplot(data=_df_sorted, x='version', y=c, ax=ax)\n",
        "    ax.set_title(c)\n",
        "    ax.tick_params(axis='x', rotation=30)\n",
        "fig.suptitle('Key metrics per version')\n",
        "fig.tight_layout()\n",
        "SUM_PNG = P2_DIR / 'summary_plots.png'\n",
        "fig.savefig(SUM_PNG, dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# %%\n",
        "# ---------------------------- 자산 수집(CM/ROC/PR/XAI) ----------------------------\n",
        "assets = {\n",
        "    'summary_plots': _copy_if_exists(SUM_PNG, ASSETS),\n",
        "    'class_dist_train': _copy_if_exists(Path(man['class_dist']['train']) if man.get('class_dist') and man['class_dist'].get('train') else None, ASSETS),\n",
        "    'class_dist_test': _copy_if_exists(Path(man['class_dist']['test'])  if man.get('class_dist') and man['class_dist'].get('test')  else None, ASSETS),\n",
        "}\n",
        "\n",
        "per_ver_cards = {}\n",
        "for ver, paths in per_version.items():\n",
        "    card = {}\n",
        "    for k in ['cm_png','history_png','roc_png','pr_png']:\n",
        "        p = paths.get(k)\n",
        "        if p:\n",
        "            c = _copy_if_exists(Path(p), ASSETS)\n",
        "            if c: card[k] = str(c)\n",
        "    if card:\n",
        "        per_ver_cards[ver] = card\n",
        "\n",
        "xai_images = []\n",
        "if xai_dir:\n",
        "    xai_dir_path = Path(xai_dir)\n",
        "    if xai_dir_path.exists():\n",
        "        for p in sorted(xai_dir_path.glob('*.png')):\n",
        "            cp = _copy_if_exists(p, ASSETS)\n",
        "            if cp: xai_images.append(str(cp))\n",
        "\n",
        "# %%\n",
        "# ---------------------------- HTML 리포트 생성 ----------------------------\n",
        "\n",
        "def _html_escape(s: str) -> str:\n",
        "    return (s.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;'))\n",
        "\n",
        "\n",
        "def make_html_report():\n",
        "    css = \"\"\"\n",
        "    body {font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans KR', Arial, sans-serif; margin: 24px;}\n",
        "    h1 {margin-bottom: 0.2em}\n",
        "    h2 {margin-top: 1.6em}\n",
        "    table {border-collapse: collapse; width: 100%; margin: 12px 0}\n",
        "    th, td {border: 1px solid #ddd; padding: 6px 8px; font-size: 13px}\n",
        "    th {background: #f5f5f5;}\n",
        "    .badge {display:inline-block; padding:2px 8px; border-radius:10px; background:#e7f3ff; color:#0366d6; font-size:12px; margin-left:8px}\n",
        "    .grid {display:grid; grid-template-columns: repeat(auto-fill, minmax(360px, 1fr)); gap: 12px}\n",
        "    .imgbox {border:1px solid #eee; padding:8px; background:#fafafa}\n",
        "    .kv {margin: 2px 0}\n",
        "    .kv span{display:inline-block; min-width:160px; color:#555}\n",
        "    .best {background:#eaffea}\n",
        "    .small{font-size:12px; color:#666}\n",
        "    \"\"\"\n",
        "\n",
        "    # 표(숫자 4자리)\n",
        "    df_show = _df_sorted.copy()\n",
        "    for c in metric_cols:\n",
        "        if c in df_show.columns:\n",
        "            df_show[c] = df_show[c].map(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"\")\n",
        "    html_table = df_show.to_html(index=False, escape=False)\n",
        "\n",
        "    # 요약 카드\n",
        "    cfg_kv = ''.join([f\"<div class='kv'><span>{_html_escape(str(k))}</span> {_html_escape(str(v))}</div>\" for k,v in config.items()])\n",
        "    cls_str = ', '.join([_html_escape(c) for c in classes])\n",
        "\n",
        "    # 이미지 인라인(필수 요약만 Base64, 나머지는 파일 링크)\n",
        "    b64_summary = _to_b64_image(Path(assets['summary_plots'])) if assets.get('summary_plots') else ''\n",
        "\n",
        "    # 버전별 카드(이미지 링크)\n",
        "    ver_cards_html = []\n",
        "    for ver, card in per_ver_cards.items():\n",
        "        items=[]\n",
        "        for k,label in [('cm_png','Confusion Matrix'),('history_png','Training History'),('roc_png','ROC'),('pr_png','PR')]:\n",
        "            if card.get(k):\n",
        "                rel = Path(card[k]).relative_to(P2_DIR)\n",
        "                items.append(f\"<div class='imgbox'><div class='small'>{label}</div><img src='{rel.as_posix()}' style='width:100%'></div>\")\n",
        "        if items:\n",
        "            ver_cards_html.append(f\"<h3 id='{_html_escape(ver)}'>{_html_escape(ver)}</h3><div class='grid'>{''.join(items)}</div>\")\n",
        "\n",
        "    # XAI 섹션\n",
        "    xai_html = ''\n",
        "    if xai_images:\n",
        "        ims = []\n",
        "        for p in xai_images:\n",
        "            rel = Path(p).relative_to(P2_DIR)\n",
        "            ims.append(f\"<div class='imgbox'><img src='{rel.as_posix()}' style='width:100%'></div>\")\n",
        "        xai_html = f\"<h2>🔍 XAI Samples</h2><div class='grid'>{''.join(ims)}</div>\"\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <html><head><meta charset='utf-8'><title>ShipsEar Report – Part 2</title>\n",
        "    <style>{css}</style></head><body>\n",
        "    <h1>ShipsEar Report – Part 2 <span class='badge'>auto-generated</span></h1>\n",
        "    <p class='small'>이 리포트는 Part 1 산출물을 바탕으로 자동 생성되었습니다.</p>\n",
        "\n",
        "    <h2>1) Executive Summary</h2>\n",
        "    <p><b>최적 버전:</b> { _html_escape(best_version) }</p>\n",
        "    <div class='grid'>\n",
        "      <div class='imgbox'>\n",
        "        <div class='small'>Key metrics</div>\n",
        "        { f\"<img src='{b64_summary}' style='width:100%'>\" if b64_summary else '<em>summary_plots.png 없음</em>' }\n",
        "      </div>\n",
        "      <div class='imgbox'>\n",
        "        <div class='small'>Config</div>\n",
        "        {cfg_kv}\n",
        "        <div class='kv'><span>Classes</span> {cls_str}</div>\n",
        "      </div>\n",
        "    </div>\n",
        "\n",
        "    <h2>2) Versions & Metrics</h2>\n",
        "    {html_table}\n",
        "\n",
        "    <h2>3) Per-Version Figures</h2>\n",
        "    {''.join(ver_cards_html) if ver_cards_html else '<p>버전별 이미지가 없습니다.</p>'}\n",
        "\n",
        "    {xai_html}\n",
        "\n",
        "    <h2>4) Artifacts</h2>\n",
        "    <ul>\n",
        "      <li><code>{_html_escape(str(summary_csv_path))}</code></li>\n",
        "      <li><code>results/part2/summary_plots.png</code></li>\n",
        "      <li><code>results/part2/assets/</code> (이미지 자산)</li>\n",
        "    </ul>\n",
        "\n",
        "    <hr>\n",
        "    <p class='small'>Generated by Part 2 module · Reproducible with Part 1 + this script.</p>\n",
        "    </body></html>\n",
        "    \"\"\"\n",
        "    out = P2_DIR / 'report.html'\n",
        "    with open(out, 'w', encoding='utf-8') as f:\n",
        "        f.write(html)\n",
        "    return out\n",
        "\n",
        "html_path = make_html_report()\n",
        "print(f\"[OK] HTML report → {html_path}\")\n",
        "\n",
        "# %%\n",
        "# ---------------------------- Markdown 리포트 생성 ----------------------------\n",
        "\n",
        "def make_md_report():\n",
        "    lines = []\n",
        "    lines.append('# ShipsEar – Part 2 Report')\n",
        "    lines.append('')\n",
        "    lines.append('## 1) Executive Summary')\n",
        "    lines.append(f'- **Best Version:** `{best_version}`')\n",
        "    if assets.get('summary_plots'):\n",
        "        rel = Path(assets['summary_plots']).relative_to(P2_DIR)\n",
        "        lines.append(f'![summary]({rel.as_posix()})')\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## 2) Versions & Metrics')\n",
        "    # 표: 숫자 포맷\n",
        "    df_md = _df_sorted.copy()\n",
        "    for c in metric_cols:\n",
        "        if c in df_md.columns:\n",
        "            df_md[c] = df_md[c].map(lambda x: f\"{x:.4f}\" if pd.notna(x) else '')\n",
        "    lines.append(df_md.to_markdown(index=False))\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## 3) Per-Version Figures')\n",
        "    for ver, card in per_ver_cards.items():\n",
        "        lines.append(f'### {ver}')\n",
        "        for k,label in [('cm_png','Confusion Matrix'),('history_png','Training History'),('roc_png','ROC'),('pr_png','PR')]:\n",
        "            if card.get(k):\n",
        "                rel = Path(card[k]).relative_to(P2_DIR)\n",
        "                lines.append(f'**{label}**')\n",
        "                lines.append(f'![{label}]({rel.as_posix()})')\n",
        "        lines.append('')\n",
        "\n",
        "    if xai_images:\n",
        "        lines.append('## 4) XAI Samples')\n",
        "        for p in xai_images:\n",
        "            rel = Path(p).relative_to(P2_DIR)\n",
        "            lines.append(f'![]({rel.as_posix()})')\n",
        "        lines.append('')\n",
        "\n",
        "    lines.append('## 5) Artifacts')\n",
        "    lines.append(f'- `{summary_csv_path}`')\n",
        "    lines.append('- `results/part2/summary_plots.png`')\n",
        "    lines.append('- `results/part2/assets/`')\n",
        "\n",
        "    out = P2_DIR / 'report.md'\n",
        "    with open(out, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "    return out\n",
        "\n",
        "md_path = make_md_report()\n",
        "print(f\"[OK] Markdown report → {md_path}\")\n",
        "\n",
        "# %%\n",
        "# ---------------------------- Part 2 매니페스트 ----------------------------\n",
        "part2_manifest = {\n",
        "    'report_html': str(html_path),\n",
        "    'report_md': str(md_path),\n",
        "    'summary_png': str(SUM_PNG),\n",
        "    'assets_dir': str(ASSETS),\n",
        "    'best_version': best_version,\n",
        "    'source_summary_csv': str(summary_csv_path),\n",
        "}\n",
        "with open(BASE / 'state/part2_manifest.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(part2_manifest, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print('\\n[Part 2 완료] → 요약:')\n",
        "print(json.dumps(part2_manifest, ensure_ascii=False, indent=2))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9tMLJKgpqHym",
        "outputId": "ee6ae957-c2d2-4bfa-d1c9-0973d0cb97d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Part 2] Loaded summary:\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9928 0.964707 0.973862  0.998603   NaN 18.732005        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9924 0.966906 0.972613  0.998884   NaN 20.479734           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9912 0.971087 0.968822  0.995689   NaN 21.119885 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9900 0.968022 0.964656  0.995748   NaN 32.206018            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9892 0.960344 0.961459  0.994124   NaN  3.414936                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.525136   NaN  0.000000                                               NaN\n",
            "\n",
            "[Best] v7_ft_meanstd_headonly\n",
            "[OK] HTML report → /content/results/part2/report.html\n",
            "[OK] Markdown report → /content/results/part2/report.md\n",
            "\n",
            "[Part 2 완료] → 요약:\n",
            "{\n",
            "  \"report_html\": \"/content/results/part2/report.html\",\n",
            "  \"report_md\": \"/content/results/part2/report.md\",\n",
            "  \"summary_png\": \"/content/results/part2/summary_plots.png\",\n",
            "  \"assets_dir\": \"/content/results/part2/assets\",\n",
            "  \"best_version\": \"v7_ft_meanstd_headonly\",\n",
            "  \"source_summary_csv\": \"results/part1/summary_part1.csv\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "YAMNet Ship Detection – Visualization Toolkit\n",
        "Version: v3.1-colab-xai-solo (2025-11-11)\n",
        "\n",
        "What this script does (single-audio, no-beam)\n",
        "--------------------------------------------\n",
        "1) Runs YAMNet on a mono audio file (16 kHz assumed; auto-resample).\n",
        "2) Aggregates ship-related probabilities (labels CSV keyword match or explicit indices).\n",
        "3) Visuals & reports:\n",
        "   - Fig1: YAMNet Log-Mel Spectrogram\n",
        "   - Fig2: Ship probability over time\n",
        "   - Fig5: Spectrogram + gradient saliency overlay (Grad×Input on log-mel)\n",
        "   - Fig6: Frequency-band importance (time-averaged saliency, Hz labels) + CSV\n",
        "   - Fig7: (optional) Time–Frequency Patch Occlusion heatmap + CSV (model-agnostic)\n",
        "   - Fig4: (optional) Time occlusion saliency (1D) + CSV\n",
        "   - Curves CSV: times, P_ship, MSP-like, entropy(ship)\n",
        "\n",
        "Colab-friendly\n",
        "--------------\n",
        "- Interactive wizard (file upload) when no args are provided in Colab.\n",
        "- Auto-install lightweight deps (tensorflow_hub, soundfile). Uses Colab's TensorFlow.\n",
        "- Auto-fetches `yamnet_class_map.csv` if missing.\n",
        "\n",
        "Removed\n",
        "-------\n",
        "- **All beam/DOA features removed** (no multi-file, no beam heatmap, no angles).\n",
        "\n",
        "CHANGELOG\n",
        "---------\n",
        "2025-11-11 v3.1-colab-xai-solo:\n",
        "- FIX: Added missing `occlusion_time_saliency` function (1D time occlusion).\n",
        "- FIX: Restored proper mel→Hz mapping for frequency-importance; CSV now includes Hz.\n",
        "- FIX: More robust gradient path (explicitly watch waveform tensor); clear error if grad None.\n",
        "- Minor: Cleaned labels, comments, ensured consistent plotting extents.\n",
        "\n",
        "2025-11-11 v3-colab-xai-solo:\n",
        "- Removed all beam/DOA options and code paths (project-wide clean).\n",
        "- Added reliability tracks: MSP-like & binary entropy of P_ship.\n",
        "- Added optional time–frequency patch occlusion heatmap (+ CSV export).\n",
        "- Kept IG-style saliency on spectrogram + frequency-importance CSV.\n",
        "\n",
        "2025-11-11 v2-colab-xai:\n",
        "- Gradient-based spectrogram XAI (saliency on YAMNet log-mel), SmoothGrad option\n",
        "- Overlay figure, frequency-importance bar + CSV\n",
        "\n",
        "2025-11-11 v1-colab:\n",
        "- Colab interactive mode, auto deps/labels, inline display\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Sequence\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Audio I/O & DSP\n",
        "try:\n",
        "    import soundfile as sf\n",
        "    _HAVE_SF = True\n",
        "except Exception:\n",
        "    _HAVE_SF = False\n",
        "\n",
        "from scipy.signal import resample_poly, stft, istft\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "\n",
        "# ML\n",
        "import tensorflow as tf\n",
        "try:\n",
        "    import tensorflow_hub as hub\n",
        "    _HAVE_TFHUB = True\n",
        "except Exception:\n",
        "    _HAVE_TFHUB = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
        "YAMNET_SR = 16000\n",
        "DEFAULT_HOP_SEC = 0.48\n",
        "DEFAULT_WIN_SEC = 0.96\n",
        "\n",
        "LABELS_DEFAULT_URL = \"https://storage.googleapis.com/audioset/yamnet/yamnet_class_map.csv\"\n",
        "\n",
        "# ------------------------------ Colab helpers -------------------------------\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def _pip_install_if_missing(mod_name: str, pip_pkg: str):\n",
        "    try:\n",
        "        __import__(mod_name)\n",
        "    except Exception:\n",
        "        try:\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_pkg])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to install {pip_pkg}: {e}\")\n",
        "\n",
        "\n",
        "def ensure_colab_deps():\n",
        "    _pip_install_if_missing(\"tensorflow_hub\", \"tensorflow_hub\")\n",
        "    _pip_install_if_missing(\"soundfile\", \"soundfile\")\n",
        "\n",
        "\n",
        "def colab_upload_one() -> Optional[str]:\n",
        "    from google.colab import files  # type: ignore\n",
        "    print(\"\n",
        "[Colab] Select ONE audio file to upload…\")\n",
        "    up = files.upload()\n",
        "    for name, data in up.items():\n",
        "        path = f\"/content/{name}\"\n",
        "        with open(path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        print(f\"[Colab] Saved: {path}\")\n",
        "        return path\n",
        "    print(\"[Colab] No file uploaded.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def maybe_colab_mount_drive():\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        ans = input(\"Mount Google Drive? [y/N]: \").strip().lower()\n",
        "        if ans == 'y':\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"[Colab] Drive mounted at /content/drive\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def auto_fetch_labels_csv(dest_dir: str = \"/content\") -> Optional[str]:\n",
        "    dest = os.path.join(dest_dir, \"yamnet_class_map.csv\")\n",
        "    try:\n",
        "        import urllib.request\n",
        "        print(f\"[Info] Downloading label CSV → {dest}\")\n",
        "        urllib.request.urlretrieve(LABELS_DEFAULT_URL, dest)\n",
        "        return dest\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not download labels CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "# --------------------------------- Core -------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class InferenceResult:\n",
        "    times_sec: np.ndarray            # [T]\n",
        "    scores: np.ndarray               # [T, 521]\n",
        "    embeddings: Optional[np.ndarray] # [T, 1024]\n",
        "    log_mel: Optional[np.ndarray]    # [T, 64]\n",
        "\n",
        "\n",
        "def load_audio(path: str, target_sr: int = YAMNET_SR) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load mono audio at target_sr. Returns (waveform_float32, sr).\"\"\"\n",
        "    if _HAVE_SF:\n",
        "        data, sr = sf.read(path, always_2d=False)\n",
        "        if data.ndim == 2:\n",
        "            data = np.mean(data, axis=1)\n",
        "    else:\n",
        "        from scipy.io import wavfile\n",
        "        sr, data = wavfile.read(path)\n",
        "        if data.dtype.kind in (\"i\", \"u\"):\n",
        "            peak = np.iinfo(data.dtype).max\n",
        "            data = data.astype(np.float32) / peak\n",
        "        elif data.dtype.kind == \"f\":\n",
        "            data = data.astype(np.float32)\n",
        "        if data.ndim == 2:\n",
        "            data = np.mean(data, axis=1)\n",
        "    if sr != target_sr:\n",
        "        g = math.gcd(sr, target_sr)\n",
        "        up = target_sr // g\n",
        "        down = sr // g\n",
        "        data = resample_poly(data, up, down).astype(np.float32)\n",
        "        sr = target_sr\n",
        "    if not np.isfinite(data).all():\n",
        "        data = np.nan_to_num(data).astype(np.float32)\n",
        "    return data.astype(np.float32), sr\n",
        "\n",
        "\n",
        "def _load_yamnet(yamnet_savedmodel: Optional[str] = None):\n",
        "    if yamnet_savedmodel:\n",
        "        model = tf.saved_model.load(yamnet_savedmodel)\n",
        "    else:\n",
        "        if not _HAVE_TFHUB:\n",
        "            raise RuntimeError(\"tensorflow_hub not available. Provide --yamnet_savedmodel for offline use.\")\n",
        "        model = hub.load(YAMNET_HANDLE)\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_yamnet(wav: np.ndarray, model, frame_hop_sec: float = DEFAULT_HOP_SEC) -> InferenceResult:\n",
        "    wav_tf = tf.convert_to_tensor(wav, dtype=tf.float32)\n",
        "    scores, embeddings, spectrogram = model(wav_tf)\n",
        "    scores_np = scores.numpy()\n",
        "    T = scores_np.shape[0]\n",
        "    times = np.arange(T, dtype=np.float32) * float(frame_hop_sec)\n",
        "    log_mel = spectrogram.numpy() if spectrogram is not None else None\n",
        "    embeds = embeddings.numpy() if embeddings is not None else None\n",
        "    return InferenceResult(times, scores_np, embeds, log_mel)\n",
        "\n",
        "\n",
        "def load_labels(labels_csv: Optional[str]) -> Optional[List[str]]:\n",
        "    if labels_csv is None:\n",
        "        return None\n",
        "    labels: List[str] = []\n",
        "    with open(labels_csv, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0 and (\",\" in line and (\"display_name\" in line or \"label\" in line)):\n",
        "                pass\n",
        "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
        "            if not parts:\n",
        "                continue\n",
        "            label = parts[-1]\n",
        "            labels.append(label)\n",
        "    labels = [l.strip().strip('\"') for l in labels if l.strip()]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def match_ship_indices(labels: Optional[List[str]], keywords: Sequence[str]) -> List[int]:\n",
        "    if not labels:\n",
        "        return []\n",
        "    lw = [l.lower() for l in labels]\n",
        "    idxs: List[int] = []\n",
        "    for i, l in enumerate(lw):\n",
        "        if any(kw.lower() in l for kw in keywords):\n",
        "            idxs.append(i)\n",
        "    return sorted(set(idxs))\n",
        "\n",
        "\n",
        "def ship_probability(scores: np.ndarray, ship_indices: Sequence[int]) -> np.ndarray:\n",
        "    if len(ship_indices) == 0:\n",
        "        raise ValueError(\"No ship_indices provided. Use --labels_csv + --ship_keywords or --ship_indices.\")\n",
        "    p = np.clip(scores[:, ship_indices], 0.0, 1.0)\n",
        "    agg = 1.0 - np.prod(1.0 - p, axis=1)\n",
        "    return agg\n",
        "\n",
        "\n",
        "def msp_like(scores: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Max over 521 sigmoid-like outputs (confidence proxy).\"\"\"\n",
        "    return np.max(np.clip(scores, 0.0, 1.0), axis=1)\n",
        "\n",
        "\n",
        "def binary_entropy(p: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
        "    p = np.clip(p, eps, 1.0 - eps)\n",
        "    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n",
        "\n",
        "# ----------------------------- Occlusion (1D) --------------------------------\n",
        "\n",
        "def occlusion_time_saliency(wav: np.ndarray,\n",
        "                            model,\n",
        "                            ship_indices: Sequence[int],\n",
        "                            base_prob: Optional[np.ndarray] = None,\n",
        "                            window_sec: float = 0.48,\n",
        "                            stride_sec: float = 0.24,\n",
        "                            frame_hop_sec: float = DEFAULT_HOP_SEC) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Time-wise occlusion: zero-out windows and observe drop in P_ship.\n",
        "    Returns (centers_sec, importance) where importance >= 0.\n",
        "    \"\"\"\n",
        "    if base_prob is None:\n",
        "        base_scores = run_yamnet(wav, model, frame_hop_sec).scores\n",
        "        base_prob = ship_probability(base_scores, ship_indices)\n",
        "    L = len(wav)\n",
        "    win = int(round(window_sec * YAMNET_SR))\n",
        "    hop = int(round(stride_sec * YAMNET_SR))\n",
        "    if win <= 0 or hop <= 0:\n",
        "        raise ValueError(\"Invalid occlusion window/stride.\")\n",
        "    centers = []\n",
        "    importances = []\n",
        "    for start in range(0, max(1, L - win), hop):\n",
        "        w = wav.copy()\n",
        "        w[start:start+win] = 0.0\n",
        "        sc = run_yamnet(w, model, frame_hop_sec).scores\n",
        "        p = ship_probability(sc, ship_indices)\n",
        "        M = min(len(base_prob), len(p))\n",
        "        drop = np.maximum(0.0, base_prob[:M] - p[:M])\n",
        "        centers.append((start + win/2) / float(YAMNET_SR))\n",
        "        importances.append(float(np.mean(drop)))\n",
        "    return np.array(centers, dtype=np.float32), np.array(importances, dtype=np.float32)\n",
        "\n",
        "# ----------------------- XAI: Spectrogram Gradients --------------------------\n",
        "\n",
        "def _hz_to_mel(hz: np.ndarray) -> np.ndarray:\n",
        "    return 2595.0 * np.log10(1.0 + hz / 700.0)\n",
        "\n",
        "def _mel_to_hz(mel: np.ndarray) -> np.ndarray:\n",
        "    return 700.0 * (10**(mel / 2595.0) - 1.0)\n",
        "\n",
        "def mel_center_frequencies(n_mels: int, fmin: float = 125.0, fmax: float = 7500.0) -> np.ndarray:\n",
        "    mel_min = _hz_to_mel(np.array([fmin]))[0]\n",
        "    mel_max = _hz_to_mel(np.array([fmax]))[0]\n",
        "    mels = np.linspace(mel_min, mel_max, num=n_mels)\n",
        "    return _mel_to_hz(mels)\n",
        "\n",
        "\n",
        "def compute_spec_grad_saliency(model,\n",
        "                               wav: np.ndarray,\n",
        "                               ship_indices: Sequence[int],\n",
        "                               smooth_samples: int = 0,\n",
        "                               smooth_sigma: float = 0.001) -> np.ndarray:\n",
        "    \"\"\"Return saliency map d target / d spectrogram (T×64), using ReLU(grad * spec).\n",
        "    - smooth_samples: number of noisy repetitions (0=off)\n",
        "    - smooth_sigma: noise std as fraction of waveform RMS\n",
        "    \"\"\"\n",
        "    def _single(w: np.ndarray) -> np.ndarray:\n",
        "        with tf.GradientTape() as tape:\n",
        "            wtf = tf.convert_to_tensor(w, dtype=tf.float32)\n",
        "            tape.watch(wtf)  # ensure path tracked\n",
        "            scores, embeddings, spectrogram = model(wtf)\n",
        "            # Compute target from scores\n",
        "            p = tf.gather(scores, ship_indices, axis=1)     # [T, K]\n",
        "            agg = 1.0 - tf.reduce_prod(1.0 - p, axis=1)     # [T]\n",
        "            target = tf.reduce_mean(agg)                    # scalar\n",
        "        # Grad w.r.t spectrogram output\n",
        "        grad = tape.gradient(target, spectrogram)\n",
        "        if grad is None:\n",
        "            raise RuntimeError(\"Gradient w.r.t spectrogram is None. Try --xai_smooth_samples > 0 or use --do_patch_occlusion.\")\n",
        "        sal = tf.nn.relu(grad * spectrogram)                # Grad×Input, positive\n",
        "        return sal.numpy()\n",
        "\n",
        "    if smooth_samples and smooth_samples > 0:\n",
        "        rms = float(np.sqrt(np.mean(np.square(wav)) + 1e-12))\n",
        "        std = smooth_sigma * (rms if rms > 0 else 1.0)\n",
        "        acc = None\n",
        "        for _ in range(smooth_samples):\n",
        "            wn = wav + np.random.normal(0.0, std, size=wav.shape).astype(np.float32)\n",
        "            s = _single(wn)\n",
        "            acc = s if acc is None else (acc + s)\n",
        "        sal = acc / float(smooth_samples)\n",
        "    else:\n",
        "        sal = _single(wav)\n",
        "    sal = np.maximum(0.0, sal)\n",
        "    return sal  # [T, 64]\n",
        "\n",
        "# -------------------- XAI: Patch Occlusion (Time–Frequency) ------------------\n",
        "\n",
        "def patch_occlusion_heatmap(wav: np.ndarray,\n",
        "                            model,\n",
        "                            ship_indices: Sequence[int],\n",
        "                            time_bins: int = 24,\n",
        "                            freq_bins: int = 16,\n",
        "                            nperseg: int = 1024,\n",
        "                            noverlap: int = 512) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Coarse 2D occlusion by zeroing STFT patches and measuring drop in P_ship.\n",
        "    Returns (heat[f_bins, t_bins], f_edges_Hz[f_bins+1], t_edges_sec[t_bins+1]).\n",
        "    Note: This is approximate and compute-heavy; use small grids.\n",
        "    \"\"\"\n",
        "    # Baseline\n",
        "    base_scores = run_yamnet(wav, model).scores\n",
        "    base_ship = ship_probability(base_scores, ship_indices)\n",
        "\n",
        "    # STFT\n",
        "    f, t, Z = stft(wav, fs=YAMNET_SR, nperseg=nperseg, noverlap=noverlap, window='hann', boundary='zeros', padded=True)\n",
        "    F, Tt = Z.shape\n",
        "\n",
        "    # Grid edges\n",
        "    t_edges = np.linspace(0, Tt, num=time_bins+1, dtype=int)\n",
        "    f_edges = np.linspace(0, F, num=freq_bins+1, dtype=int)\n",
        "\n",
        "    heat = np.zeros((freq_bins, time_bins), dtype=np.float32)\n",
        "\n",
        "    for ti in range(time_bins):\n",
        "        t0, t1 = t_edges[ti], t_edges[ti+1]\n",
        "        if t1 <= t0: continue\n",
        "        for fi in range(freq_bins):\n",
        "            f0, f1 = f_edges[fi], f_edges[fi+1]\n",
        "            if f1 <= f0: continue\n",
        "            Zm = Z.copy()\n",
        "            Zm[f0:f1, t0:t1] = 0.0\n",
        "            _, x = istft(Zm, fs=YAMNET_SR, nperseg=nperseg, noverlap=noverlap, window='hann')\n",
        "            x = x.astype(np.float32)\n",
        "            sc = run_yamnet(x, model).scores\n",
        "            shp = ship_probability(sc, ship_indices)\n",
        "            M = min(len(base_ship), len(shp))\n",
        "            drop = np.maximum(0.0, base_ship[:M] - shp[:M])\n",
        "            heat[fi, ti] = float(np.mean(drop))\n",
        "\n",
        "    # Convert STFT edges to Hz/sec\n",
        "    f_edges_hz = np.interp(f_edges, np.arange(F), f)\n",
        "    t_edges_sec = (t_edges / float(Tt)) * (len(wav) / float(YAMNET_SR))\n",
        "\n",
        "    return heat, f_edges_hz, t_edges_sec\n",
        "\n",
        "# ------------------------------ Plot helpers ---------------------------------\n",
        "\n",
        "def _maybe_display_image(path: str):\n",
        "    if in_colab():\n",
        "        try:\n",
        "            from IPython.display import Image, display\n",
        "            display(Image(filename=path))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def plot_log_mel(times: np.ndarray, log_mel: Optional[np.ndarray], outpath: str):\n",
        "    if log_mel is None:\n",
        "        return\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    dt = times[1]-times[0] if len(times) > 1 else DEFAULT_HOP_SEC\n",
        "    plt.imshow(log_mel.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, log_mel.shape[1]])\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Mel bin'); plt.title('YAMNet Log-Mel Spectrogram')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_ship_prob(times: np.ndarray, prob: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.plot(times, prob)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('P(ship)'); plt.title('Ship Probability over Time')\n",
        "    plt.grid(True); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_time_saliency(centers_sec: np.ndarray, importance: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.stem(centers_sec, importance, use_line_collection=True)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Importance (ΔP)'); plt.title('Time Occlusion Saliency')\n",
        "    plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_spec_with_saliency(times: np.ndarray,\n",
        "                            log_mel: Optional[np.ndarray],\n",
        "                            saliency: np.ndarray,\n",
        "                            outpath: str,\n",
        "                            alpha: float = 0.6,\n",
        "                            clip_percentile: float = 95.0):\n",
        "    sal = saliency.copy()\n",
        "    vmax = np.percentile(sal, clip_percentile) if np.isfinite(sal).any() else 1.0\n",
        "    if vmax <= 0:\n",
        "        vmax = 1.0\n",
        "    sal = np.clip(sal / vmax, 0.0, 1.0)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    dt = times[1]-times[0] if len(times) > 1 else DEFAULT_HOP_SEC\n",
        "    if log_mel is not None:\n",
        "        plt.imshow(log_mel.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]])\n",
        "        plt.imshow(sal.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]], alpha=alpha)\n",
        "        plt.title('Spectrogram + XAI Saliency Overlay')\n",
        "    else:\n",
        "        plt.imshow(sal.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]])\n",
        "        plt.title('XAI Saliency (no spectrogram available)')\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Mel bin')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_freq_importance(freq_hz: np.ndarray, saliency: np.ndarray, outpath: str):\n",
        "    imp = np.mean(saliency, axis=0)  # [64]\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.bar(np.arange(len(imp)), imp)\n",
        "    # Sparse tick labels in kHz\n",
        "    idxs = np.linspace(0, len(imp)-1, num=8).astype(int)\n",
        "    labels = [f\"{freq_hz[i]/1000:.1f}k\" for i in idxs]\n",
        "    plt.xticks(idxs, labels)\n",
        "    plt.xlabel('Frequency (Hz)'); plt.ylabel('Importance (avg)')\n",
        "    plt.title('Frequency-band Importance (from saliency)')\n",
        "    plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_patch_occlusion(heat: np.ndarray, f_edges_hz: np.ndarray, t_edges_sec: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    extent = [t_edges_sec[0], t_edges_sec[-1], f_edges_hz[0], f_edges_hz[-1]]\n",
        "    plt.imshow(heat, aspect='auto', origin='lower', extent=extent)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Frequency (Hz)'); plt.title('Patch Occlusion Heatmap (ΔP_ship)')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "# ---------------------------------- CLI --------------------------------------\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description='YAMNet Ship Detection – Visualization Toolkit (Colab-ready, single-audio, XAI)')\n",
        "    p.add_argument('--audio', type=str, help='Audio file path (single). If omitted in Colab, an upload wizard appears.')\n",
        "    p.add_argument('--yamnet_savedmodel', type=str, default=os.getenv('YAMNET_SAVEDMODEL'), help='Local SavedModel dir for offline use.')\n",
        "    p.add_argument('--labels_csv', type=str, default=None, help='Path to yamnet_class_map.csv (auto-fetched in Colab if missing).')\n",
        "    p.add_argument('--ship_keywords', type=str, default='boat,ship,water vehicle,marine engine,ferry', help='Comma-separated keywords for label matching.')\n",
        "    p.add_argument('--ship_indices', type=str, default=None, help='Comma-separated explicit class indices (overrides keywords).')\n",
        "    p.add_argument('--frame_hop_sec', type=float, default=DEFAULT_HOP_SEC, help='Frame hop seconds for timeline.')\n",
        "    p.add_argument('--thr', type=float, default=0.5, help='Event threshold on aggregated ship prob.')\n",
        "    p.add_argument('--min_dur', type=float, default=1.0, help='Minimum event duration (s).')\n",
        "    p.add_argument('--smooth', type=float, default=1.0, help='Smoothing window (s) before thresholding.')\n",
        "    p.add_argument('--outdir', type=str, default='viz_out', help='Output directory for images & CSV.')\n",
        "    p.add_argument('--do_saliency', action='store_true', help='Compute time-occlusion saliency (1D)')\n",
        "    p.add_argument('--sal_win', type=float, default=0.48, help='Saliency occlusion window (s).')\n",
        "    p.add_argument('--sal_hop', type=float, default=0.24, help='Saliency occlusion stride (s).')\n",
        "    # XAI over spectrogram\n",
        "    p.add_argument('--xai_smooth_samples', type=int, default=0, help='SmoothGrad samples (0=off).')\n",
        "    p.add_argument('--xai_smooth_sigma', type=float, default=0.001, help='SmoothGrad noise std as fraction of waveform RMS.')\n",
        "    # Patch occlusion (2D)\n",
        "    p.add_argument('--do_patch_occlusion', action='store_true', help='Compute 2D time–frequency patch occlusion (slow).')\n",
        "    p.add_argument('--po_time_bins', type=int, default=24, help='Occlusion grid time bins.')\n",
        "    p.add_argument('--po_freq_bins', type=int, default=16, help='Occlusion grid freq bins.')\n",
        "    p.add_argument('--po_nperseg', type=int, default=1024, help='STFT nperseg for occlusion.')\n",
        "    p.add_argument('--po_noverlap', type=int, default=512, help='STFT noverlap for occlusion.')\n",
        "    return p.parse_args()\n",
        "\n",
        "# ------------------------------ Main / Entrypoint ----------------------------\n",
        "\n",
        "def colab_wizard(args):\n",
        "    print(\"\n",
        "[Colab] Interactive mode enabled.\")\n",
        "    ensure_colab_deps()\n",
        "    maybe_colab_mount_drive()\n",
        "\n",
        "    audio_path = args.audio\n",
        "    if not audio_path:\n",
        "        ans = input(\"Upload audio now? (y to upload) [y/N]: \").strip().lower()\n",
        "        if ans == 'y':\n",
        "            audio_path = colab_upload_one()\n",
        "\n",
        "    labels_csv = args.labels_csv\n",
        "    if labels_csv is None and args.ship_indices is None:\n",
        "        labels_csv = auto_fetch_labels_csv() or None\n",
        "        if labels_csv is None:\n",
        "            print(\"[Colab] Could not obtain labels CSV; use --ship_indices.\")\n",
        "\n",
        "    return audio_path, labels_csv\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    audio_path = args.audio\n",
        "    labels_csv = args.labels_csv\n",
        "\n",
        "    if in_colab() and not audio_path:\n",
        "        audio_path, labels_csv = colab_wizard(args)\n",
        "\n",
        "    if not audio_path:\n",
        "        raise SystemExit(\"Provide an audio file path.\")\n",
        "\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "    # Load model\n",
        "    model = _load_yamnet(args.yamnet_savedmodel)\n",
        "\n",
        "    # Labels / indices\n",
        "    labels = load_labels(labels_csv) if labels_csv else None\n",
        "    if args.ship_indices:\n",
        "        ship_idxs = sorted({int(x) for x in args.ship_indices.split(',') if x.strip()})\n",
        "    else:\n",
        "        kws = [k.strip() for k in (args.ship_keywords or '').split(',') if k.strip()]\n",
        "        ship_idxs = match_ship_indices(labels, kws)\n",
        "    if len(ship_idxs) == 0:\n",
        "        raise SystemExit(\"No ship-related class indices found. Provide --labels_csv and/or --ship_indices.\")\n",
        "\n",
        "    # Load audio\n",
        "    wav, _ = load_audio(audio_path, YAMNET_SR)\n",
        "\n",
        "    # Inference\n",
        "    res = run_yamnet(wav, model, frame_hop_sec=args.frame_hop_sec)\n",
        "    ship_prob = ship_probability(res.scores, ship_idxs)\n",
        "    msp = msp_like(res.scores)\n",
        "    ent_ship = binary_entropy(ship_prob)\n",
        "\n",
        "    # Fig1: Log-mel\n",
        "    fig1 = os.path.join(args.outdir, 'fig1_logmel.png')\n",
        "    plot_log_mel(res.times_sec, res.log_mel, fig1)\n",
        "\n",
        "    # Fig2: Ship prob\n",
        "    fig2 = os.path.join(args.outdir, 'fig2_prob.png')\n",
        "    plot_ship_prob(res.times_sec, ship_prob, fig2)\n",
        "\n",
        "    # Events CSV\n",
        "    events = segment_events(res.times_sec, ship_prob, thr=args.thr, min_dur=args.min_dur,\n",
        "                            smooth_win=args.smooth, hop_sec=args.frame_hop_sec)\n",
        "    if events:\n",
        "        import csv\n",
        "        with open(os.path.join(args.outdir, 'events.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow(['start_sec','end_sec','peak_prob'])\n",
        "            for s, e, p in events:\n",
        "                w.writerow([f\"{s:.3f}\", f\"{e:.3f}\", f\"{p:.4f}\"])\n",
        "\n",
        "    # Reliability CSV/plot (times, ship_prob, msp_like, entropy)\n",
        "    try:\n",
        "        import csv\n",
        "        with open(os.path.join(args.outdir, 'curves.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow(['time_sec','p_ship','msp_like','entropy_ship'])\n",
        "            for t, ps, m, h in zip(res.times_sec, ship_prob, msp, ent_ship):\n",
        "                w.writerow([f\"{t:.3f}\", f\"{ps:.6f}\", f\"{m:.6f}\", f\"{h:.6f}\"])\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not write curves.csv: {e}\")\n",
        "\n",
        "    # XAI: Spectrogram gradient saliency\n",
        "    sal = None\n",
        "    try:\n",
        "        sal = compute_spec_grad_saliency(model, wav, ship_idxs,\n",
        "                                         smooth_samples=args.xai_smooth_samples,\n",
        "                                         smooth_sigma=args.xai_smooth_sigma)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] XAI gradient saliency failed: {e}\")\n",
        "\n",
        "    if sal is not None:\n",
        "        fig5 = os.path.join(args.outdir, 'fig5_spec_xai_overlay.png')\n",
        "        plot_spec_with_saliency(res.times_sec, res.log_mel, sal, fig5)\n",
        "\n",
        "        # Frequency-importance summary with mel→Hz mapping\n",
        "        freq_hz = mel_center_frequencies(sal.shape[1], fmin=125.0, fmax=7500.0)\n",
        "        fig6 = os.path.join(args.outdir, 'fig6_freq_importance.png')\n",
        "        plot_freq_importance(freq_hz, sal, fig6)\n",
        "        # CSV export\n",
        "        try:\n",
        "            import csv\n",
        "            imp = np.mean(sal, axis=0)\n",
        "            with open(os.path.join(args.outdir, 'xai_freq_importance.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                w.writerow(['mel_bin','center_hz','importance_avg'])\n",
        "                for i, (hz, v) in enumerate(zip(freq_hz, imp)):\n",
        "                    w.writerow([i, f\"{hz:.2f}\", f\"{v:.8f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not write xai_freq_importance.csv: {e}\")\n",
        "\n",
        "    # Optional: 1D time-occlusion saliency\n",
        "    if args.do_saliency:\n",
        "        centers, imp = occlusion_time_saliency(wav, model, ship_idxs, base_prob=ship_prob,\n",
        "                                              window_sec=args.sal_win, stride_sec=args.sal_hop,\n",
        "                                              frame_hop_sec=args.frame_hop_sec)\n",
        "        fig4 = os.path.join(args.outdir, 'fig4_time_saliency.png')\n",
        "        plot_time_saliency(centers, imp, fig4)\n",
        "        try:\n",
        "            import csv\n",
        "            with open(os.path.join(args.outdir, 'saliency_time.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                w.writerow(['center_sec','importance_drop'])\n",
        "                for t, v in zip(centers, imp):\n",
        "                    w.writerow([f\"{t:.3f}\", f\"{v:.6f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not write saliency_time.csv: {e}\")\n",
        "\n",
        "    # Optional: 2D patch occlusion (slow)\n",
        "    if args.do_patch_occlusion:\n",
        "        try:\n",
        "            heat, f_edges_hz, t_edges_sec = patch_occlusion_heatmap(\n",
        "                wav, model, ship_idxs,\n",
        "                time_bins=args.po_time_bins,\n",
        "                freq_bins=args.po_freq_bins,\n",
        "                nperseg=args.po_nperseg,\n",
        "                noverlap=args.po_noverlap)\n",
        "            fig7 = os.path.join(args.outdir, 'fig7_patch_occlusion.png')\n",
        "            plot_patch_occlusion(heat, f_edges_hz, t_edges_sec, fig7)\n",
        "            # CSV export (grid)\n",
        "            import csv\n",
        "            with open(os.path.join(args.outdir, 'patch_occlusion.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                # header\n",
        "                w.writerow([\"freq_bin\",\"time_bin\",\"f_start_Hz\",\"f_end_Hz\",\"t_start_s\",\"t_end_s\",\"drop_avg\"])\n",
        "                for fi in range(heat.shape[0]):\n",
        "                    for ti in range(heat.shape[1]):\n",
        "                        w.writerow([\n",
        "                            fi, ti,\n",
        "                            f\"{f_edges_hz[fi]:.2f}\", f\"{f_edges_hz[fi+1]:.2f}\",\n",
        "                            f\"{t_edges_sec[ti]:.3f}\", f\"{t_edges_sec[ti+1]:.3f}\",\n",
        "                            f\"{heat[fi,ti]:.8f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Patch occlusion failed: {e}\")\n",
        "\n",
        "    # Meta\n",
        "    meta = {\n",
        "        'yamnet_savedmodel': bool(args.yamnet_savedmodel),\n",
        "        'labels_csv': bool(labels_csv),\n",
        "        'ship_indices': ship_idxs,\n",
        "        'frame_hop_sec': args.frame_hop_sec,\n",
        "        'thr': args.thr,\n",
        "        'min_dur': args.min_dur,\n",
        "        'smooth': args.smooth,\n",
        "        'audio_file': audio_path,\n",
        "        'xai_smooth_samples': args.xai_smooth_samples,\n",
        "        'xai_smooth_sigma': args.xai_smooth_sigma,\n",
        "        'do_patch_occlusion': args.do_patch_occlusion,\n",
        "        'po_time_bins': args.po_time_bins,\n",
        "        'po_freq_bins': args.po_freq_bins,\n",
        "        'po_nperseg': args.po_nperseg,\n",
        "        'po_noverlap': args.po_noverlap,\n",
        "    }\n",
        "    with open(os.path.join(args.outdir, 'run_meta.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Console summary\n",
        "    prints = [\"fig1_logmel.png\", \"fig2_prob.png\", \"curves.csv\"]\n",
        "    if sal is not None:\n",
        "        prints.extend([\"fig5_spec_xai_overlay.png\", \"fig6_freq_importance.png\", \"xai_freq_importance.csv\"])\n",
        "    if args.do_saliency:\n",
        "        prints.append(\"fig4_time_saliency.png\")\n",
        "    if args.do_patch_occlusion:\n",
        "        prints.append(\"fig7_patch_occlusion.png\")\n",
        "        prints.append(\"patch_occlusion.csv\")\n",
        "\n",
        "    print(\"\n",
        "Done. Outputs in:\", os.path.abspath(args.outdir))\n",
        "    print(\"- \", \", \".join(prints))\n",
        "\n",
        "    # Colab inline\n",
        "    if in_colab():\n",
        "        try:\n",
        "            from IPython.display import display, Audio\n",
        "            print(\"\n",
        "[Colab] Inline preview:\")\n",
        "            _maybe_display_image(fig2)\n",
        "            if sal is not None:\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig5_spec_xai_overlay.png'))\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig6_freq_importance.png'))\n",
        "            if args.do_patch_occlusion:\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig7_patch_occlusion.png'))\n",
        "            # Play audio\n",
        "            mx = np.max(np.abs(wav)) + 1e-9\n",
        "            display(Audio(wav / mx, rate=YAMNET_SR))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-DpG3LNng-z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD 평가 모듈 =================================\n",
        "# 이 블록은 기존 파이프라인에서 학습이 끝난 후에 붙여 실행하세요.\n",
        "# 필요 전역: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(학습된 분류기), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE 경로\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) Git에서 OOD 샘플 오디오 가볍게 수집 ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # 소형 예제/테스트 오디오가 비교적 들어있는 경우가 많음\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] 리포지토리 수집 ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# 오디오 확장자 패턴(넓게 잡되 개수 제한)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # 너무 많은 경우 샘플링\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - 수집된 OOD 원본 파일: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD 세그먼트(5초) 스트리밍 생성 ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load 없이 스트리밍으로 5초 구간을 균일 스트라이드로 최대 cap만 추출\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # 균일 스트라이드로 시작점 후보 생성\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# 너무 많이 뽑지 않도록 전체 cap (예: 800 세그먼트)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - 생성된 OOD 세그먼트: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD 임베딩 ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(정규화 전에)도 저장해 에너지 편향 분석\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD 임베딩 {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] 임베딩 추출 ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"경고: OOD 임베딩이 비었습니다. 리포 소스나 max_total, cap을 조정해보세요.\")\n",
        "\n",
        "# ---------- 4) 임계값 선택(검증셋 TPR=95%) & ID/OOD FPR 비교 ----------\n",
        "# 학습에 사용한 train에서 validation을 분리(간단히 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# 재학습 없이 clf를 재사용하되, val 확률만 새로 추정\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPR이 target에 가장 근접한 점의 threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[임계값] TPR@val≈95% → τ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-테스트 FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@τ={fpr_id:.4f} | OOD FPR@τ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) 시각화: 확률 분포 / ROC-PR / 에너지 편향 ----------\n",
        "# (a) 확률 히스토그램\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"τ={tau:.2f}\")\n",
        "plt.title(\"Ship 확률 분포(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID 기준)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) 에너지 decile 별 FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMS와 확률\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (낮음→높음)\")\n",
        "    plt.ylabel(f\"FPR@τ\"); plt.title(\"에너지 구간별 FPR (낮을수록 좋음)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile 분석을 위한 유효 표본이 부족합니다.\")\n",
        "\n",
        "print(\"\\n[요약]\")\n",
        "print(f\" - 임계값 τ(Val TPR≈95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@τ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@τ: {fpr_ood:.4f} (낮을수록 좋음)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - 그래프: 확률분포/ROC/PR/에너지-디사일 FPR으로, 에너지-편향 여부를 함께 점검\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}