{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR7.ipynb",
      "authorship_tag": "ABX9TyMzYDtbOgXDiPIme7OAXy0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/20251113_%EB%A7%88%EC%A7%80%EB%A7%89%20%ED%85%8C%EC%8A%A4%ED%8A%B8%EB%B2%84%EC%A0%84\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ShipsEar – Part 1 v3 (Colab)\n",
        "# Major upgrades from v2-mini r2:\n",
        "#  - Device/seed/reporting: device summary, deterministic seeds, run signature.\n",
        "#  - Robust YAMNet I/O: dict/list-safe getters, KerasLayer fallback.\n",
        "#  - Toggles: USE_MIXED, NORM_AUDIO, CACHE_RATIO, FAST_SMOKE, SAVE_NPZ_PRED.\n",
        "#  - Data QA: VAD histograms & active/inactive coverage plots.\n",
        "#  - Training: optional FocalLoss, label smoothing, scaler switch for MLP input.\n",
        "#  - XAI: Grad×Input (fixed), SmoothGrad, Integrated Gradients (IG) option.\n",
        "#  - Occlusion: time/freq, double-occlusion (optional), CSV schema unified.\n",
        "#  - Global band: bootstrap CIs, split selection (train/test/all), fine band control.\n",
        "#  - Manifests: npz of preds/probs/indexes; richer JSON with hashes and cfg.\n",
        "#  - Fail-safe: graceful skips (no ship_idx / missing tensors) with logs.\n",
        "\n",
        "print(\"Setup…\")\n",
        "!pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1 librosa==0.10.2.post1 soundfile==0.12.1 scikit-learn==1.5.2 psutil==5.9.8 seaborn==0.13.2 joblib==1.4.2 scipy==1.13.1 tqdm==4.66.5 umap-learn==0.5.6\n",
        "!apt -yq install fonts-nanum >/dev/null\n",
        "\n",
        "import os, re, random as rd, math, time, json, glob, shutil, warnings, hashlib, platform\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import numpy as np, pandas as pd, psutil, soundfile as sf\n",
        "import scipy as sp\n",
        "import scipy.signal as sig\n",
        "import tensorflow as tf, tensorflow_hub as hub, librosa\n",
        "from tensorflow.keras import mixed_precision as mp\n",
        "import matplotlib.pyplot as plt, seaborn as sns, matplotlib.font_manager as fm\n",
        "from sklearn.preprocessing import LabelEncoder as LE, StandardScaler as SS\n",
        "from sklearn.metrics import (confusion_matrix as CM, f1_score as F1, roc_auc_score as AUC,\n",
        "                             average_precision_score as AP, balanced_accuracy_score as BACC,\n",
        "                             top_k_accuracy_score as TOPK, accuracy_score as ACC)\n",
        "from sklearn.model_selection import GroupShuffleSplit as GSS\n",
        "import scipy.signal as sig\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ==== USER TOGGLES ==========================================================\n",
        "USE_MIXED      = True   # mixed_float16 for Keras heads\n",
        "NORM_AUDIO     = True   # RMS norm to -20 dBFS\n",
        "CACHE_RATIO    = 0.15   # fraction of RAM used for waveform cache\n",
        "FAST_SMOKE     = False  # True -> mini run to validate end-to-end\n",
        "SAVE_NPZ_PRED  = True   # save per-version npz (probs/preds/true/index)\n",
        "USE_FOCAL      = False  # focal loss for MLP\n",
        "LABEL_SMOOTH   = 0.0    # label smoothing for MLP [0..0.1]\n",
        "MLP_SCALE_INPUT= False  # StandardScaler before MLP\n",
        "\n",
        "# XAI/Occlusion/global-band options\n",
        "MAKE_XAI       = True\n",
        "XAI_SMOOTH_N   = 4      # SmoothGrad samples\n",
        "XAI_SMOOTH_S   = 0.002  # noise std scale\n",
        "USE_IG         = True   # add Integrated Gradients maps (on log-mel)\n",
        "\n",
        "MAKE_OCC       = True\n",
        "O_TMS, O_THOP  = 80, 20   # time occlusion window/hop (ms)\n",
        "O_FMIN, O_FMAX = 80, 8000 # Hz\n",
        "O_NB,  O_ORD   = 40, 801  # #bands, FIR order\n",
        "O_DUAL         = False    # time x freq double-occlusion heatmap (heavy)\n",
        "\n",
        "G_SUM          = True\n",
        "G_SPLIT        = 'test'   # 'train' | 'test' | 'all'\n",
        "G_MAX          = 300      # samples for global band summary\n",
        "G_BOOT         = 200      # bootstrap for CI (0=off)\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "# ---- Misc helpers ----------------------------------------------------------\n",
        "SEED=42\n",
        "np.random.seed(SEED); rd.seed(SEED); tf.random.set_seed(SEED)\n",
        "def device_summary():\n",
        "    # 전역 tf/hub/librosa가 텐서/딕셔너리로 덮여있어도 안전하게 모듈 다시 로드\n",
        "    import tensorflow as tf_mod\n",
        "    import tensorflow_hub as hub_mod\n",
        "    import librosa as librosa_mod\n",
        "    import scipy as sp_mod\n",
        "    import platform\n",
        "\n",
        "    try:\n",
        "        gpus = tf_mod.config.experimental.list_physical_devices('GPU')\n",
        "        cpus = tf_mod.config.experimental.list_physical_devices('CPU')\n",
        "        for g in gpus:\n",
        "            try:\n",
        "                tf_mod.config.experimental.set_memory_growth(g, True)\n",
        "            except:\n",
        "                pass\n",
        "    except Exception:\n",
        "        gpus, cpus = [], []\n",
        "\n",
        "    return dict(\n",
        "        tf=tf_mod.__version__,\n",
        "        hub=hub_mod.__version__,\n",
        "        librosa=librosa_mod.__version__,\n",
        "        scipy=sp_mod.__version__,   # ★ 더 이상 scipy.signal 안 씀\n",
        "        py=platform.python_version(),\n",
        "        gpus=len(gpus),\n",
        "        cpus=len(cpus),\n",
        "        gpu_names=[getattr(x, 'name', str(x)) for x in gpus]\n",
        "    )\n",
        "\n",
        "\n",
        "if USE_MIXED:\n",
        "    mp.set_global_policy('mixed_float16')\n",
        "if os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):\n",
        "    fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "    plt.rc('font', family='NanumGothic'); plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# ---- Paths/Config ----------------------------------------------------------\n",
        "BASE=\"/content\"; DSH=f\"{BASE}/drive/MyDrive/ShipsEar\"; DROOT=f\"{BASE}/ShipsEar_colab\"\n",
        "os.makedirs(\"results/part1\", exist_ok=True); os.makedirs(\"cache\", exist_ok=True); os.makedirs(\"artifacts\", exist_ok=True); os.makedirs(\"state\", exist_ok=True)\n",
        "try:\n",
        "    from google.colab import drive; drive.mount('/content/drive', force_remount=False); print(\"Drive mounted.\")\n",
        "except Exception as e: print(\"Not Colab or Drive:\", e)\n",
        "\n",
        "SR=16000; BIN=True; POS=\"Ship\"\n",
        "CFG=dict(seg_dur=1.0, ship_overlap=0.2, noise_overlap=0.0, vad_frame_sec=0.5, vad_hop_sec=0.25, vad_top_db=25.0,\n",
        "         test_size=0.2, epochs=40, batch=32, lr=5e-4, max_seg_per_group_per_class=500, noise_jitter_sec=0.5,\n",
        "         topk=1, cache_emb=True)\n",
        "\n",
        "if FAST_SMOKE:\n",
        "    CFG.update(dict(epochs=2, max_seg_per_group_per_class=50))\n",
        "\n",
        "# Cache size set by RAM ratio\n",
        "RAM=psutil.virtual_memory().total\n",
        "_WC=OrderedDict(); _WC_BYTES=0; _WC_MAX=int(RAM*CACHE_RATIO)\n",
        "\n",
        "# ---- Label/Group rules -----------------------------------------------------\n",
        "KW={\"A\":[\"fishing\",\"trawler\",\"trawl\",\"mussel\",\"tug\",\"dredger\",\"dredge\"],\n",
        "    \"B\":[\"motorboat\",\"motor boat\",\"pilot\",\"sailboat\",\"sailing\"],\n",
        "    \"C\":[\"ferry\",\"passenger\"],\n",
        "    \"D\":[\"oceanliner\",\"ro-ro\",\"roro\",\"ro_ro\",\"cargo\",\"containership\",\"container\",\"tanker\",\"bulk\",\"liner\",\"oceangoing\"],\n",
        "    \"E\":[\"background\",\"noise\",\"ambient\",\"no_ship\",\"noship\",\"silence\"]}\n",
        "\n",
        "basename=lambda p:(os.path.basename(os.path.dirname(p))+\" \"+os.path.basename(p)).lower()\n",
        "mem=lambda: f\"{psutil.Process().memory_info().rss/1024**3:.2f} GB\"\n",
        "\n",
        "# ---- Data copy & presence --------------------------------------------------\n",
        "print(\"Data…\")\n",
        "if os.path.exists(DSH):\n",
        "    if not os.path.exists(DROOT) or not os.listdir(DROOT):\n",
        "        shutil.copytree(DSH, DROOT, dirs_exist_ok=True); print(\" - Copied ShipsEar\")\n",
        "    else: print(\" - ShipsEar exists\")\n",
        "else: raise FileNotFoundError(f\"ShipsEar not found: {DSH}\")\n",
        "\n",
        "# ---- Core: labeling, grouping, VAD, segment build -------------------------\n",
        "EPS=1e-12\n",
        "\n",
        "def cls_of(p):\n",
        "    t=basename(p)\n",
        "    for c,k in ((\"E\",KW[\"E\"]),(\"A\",KW[\"A\"]),(\"B\",KW[\"B\"]),(\"C\",KW[\"C\"]),(\"D\",KW[\"D\"])):\n",
        "        if any(s in t for s in k): return c\n",
        "    m=re.search(r'\\bclass[_\\s-]*([abcde])\\b', t); return m.group(1).upper() if m else None\n",
        "\n",
        "def grp_of(p):\n",
        "    s=os.path.splitext(os.path.basename(p))[0]\n",
        "    m=re.search(r'(\\d{8}[_-]?\\d{4})', s) or re.search(r'(\\d{4}[-_]\\d{2}[-_]\\d{2}[_-]?\\d{2}[-_]?\\d{2})', s)\n",
        "    if m: return m.group(1)\n",
        "    par=os.path.basename(os.path.dirname(p)); tok=re.split(r'[_\\-]+', s); pre=\"_\".join(tok[:3]) if len(tok)>=3 else s\n",
        "    return f\"{par}:{pre}\"\n",
        "\n",
        "def act_of(fp, top_db=25.0, fr=0.5, hop=0.25):\n",
        "    try:\n",
        "        with sf.SoundFile(fp) as f:\n",
        "            sr=f.samplerate; n=len(f); F=max(1,int(fr*sr)); H=max(1,int(hop*sr))\n",
        "            mx=-np.inf; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                rms=float(np.sqrt(np.mean(y**2))+EPS); mx=max(mx,20*np.log10(rms+EPS)); pos+=H\n",
        "            if not np.isfinite(mx): return [], []\n",
        "            th=mx-top_db; A=[]; on=False; cur=0.0; pos=0\n",
        "            while pos+F<=n:\n",
        "                f.seek(pos); y=f.read(frames=F, dtype='float32', always_2d=False); y=y.mean(axis=1) if y.ndim>1 else y\n",
        "                db=20*np.log10(float(np.sqrt(np.mean(y**2))+EPS)); t0=pos/sr; t1=(pos+F)/sr\n",
        "                if db>=th:\n",
        "                    if not on: on=True; cur=t0\n",
        "                else:\n",
        "                    if on: on=False; A.append((cur,t1))\n",
        "                pos+=H\n",
        "            if on: A.append((cur,n/sr))\n",
        "            I=[]; last=0.0; dur=n/sr\n",
        "            for s,e in A:\n",
        "                if s>last: I.append((last,s)); last=e\n",
        "            if last<dur: I.append((last,dur))\n",
        "            return A,I\n",
        "    except: return [], []\n",
        "\n",
        "spans=lambda S,d,h:[(float(st),) for s,e in S for st in np.arange(s, e-d+1e-9, h)]\n",
        "\n",
        "def build_segs(root,C):\n",
        "    d=C['seg_dur']; hop_s=d*(1-C['ship_overlap']); hop_n=d*(1-C['noise_overlap'])\n",
        "    jit=C['noise_jitter_sec']; cap=C['max_seg_per_group_per_class']\n",
        "    infos,labels,groups,miss,pc=[],[],[],0,defaultdict(int); summ=defaultdict(int)\n",
        "    paths=sorted(glob.glob(os.path.join(root, \"**\", \"*.wav\"), recursive=True))\n",
        "    for fp in paths:\n",
        "        c=cls_of(fp)\n",
        "        if c is None: miss+=1; continue\n",
        "        try: inf=sf.info(fp)\n",
        "        except: continue\n",
        "        gk=grp_of(fp)\n",
        "        if c in \"ABCD\":\n",
        "            A,_=act_of(fp,C['vad_top_db'],C['vad_frame_sec'],C['vad_hop_sec']); S=spans(A,d,hop_s)\n",
        "        else:\n",
        "            S=spans([(0.0, inf.frames/inf.samplerate)], d, hop_n)\n",
        "        rd.shuffle(S)\n",
        "        for (st,) in S:\n",
        "            if c==\"E\" and jit>0:\n",
        "                j=rd.uniform(-jit, jit); st=max(0.0, min(st+j,(inf.frames/inf.samplerate)-d))\n",
        "            key=(gk,c)\n",
        "            if cap and pc[key]>=cap: continue\n",
        "            infos.append((fp,float(st),inf.samplerate)); labels.append(c); groups.append(gk); pc[key]+=1; summ[c]+=1\n",
        "    return infos,labels,groups,summ,miss\n",
        "\n",
        "# ---- Audio cache/resample --------------------------------------------------\n",
        "\n",
        "def _cget(k):\n",
        "    a=_WC.get(k)\n",
        "    if a is not None: _WC.move_to_end(k)\n",
        "    return a\n",
        "\n",
        "def _cput(k,arr):\n",
        "    global _WC_BYTES; sz=getattr(arr,'nbytes',None) or (getattr(arr,'size',0)*getattr(arr,'itemsize',0)); _WC[k]=arr; _WC.move_to_end(k); _WC_BYTES+=sz\n",
        "    while _WC_BYTES>_WC_MAX and len(_WC)>1:\n",
        "        kk,v=_WC.popitem(last=False)\n",
        "        try: _WC_BYTES-=v.nbytes\n",
        "        except: pass\n",
        "\n",
        "def resamp(y,sr0,sr1):\n",
        "    if sr0==sr1: return y.astype(np.float32)\n",
        "    try:\n",
        "        g=math.gcd(int(sr0),int(sr1)); up=int(sr1)//g; dn=int(sr0)//g\n",
        "        return sig.resample_poly(y, up, dn).astype(np.float32)\n",
        "    except Exception:\n",
        "        try: return librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr1, res_type='fft').astype(np.float32)\n",
        "        except Exception:\n",
        "            n=int(round(len(y)*float(sr1)/float(sr0))); xp=np.arange(len(y)); x=np.linspace(0,len(y),n,endpoint=False); return np.interp(x,xp,y).astype(np.float32)\n",
        "\n",
        "def load_seg(info,dur,tar_sr=SR,norm=NORM_AUDIO):\n",
        "    fp,st,sr0=info\n",
        "    try:\n",
        "        yF=_cget(fp)\n",
        "        if yF is None:\n",
        "            yF,sr=sf.read(fp,dtype='float32',always_2d=False); yF=yF.mean(axis=1) if yF.ndim>1 else yF\n",
        "            if sr!=tar_sr: yF=resamp(yF,sr,tar_sr); _cput(fp,yF)\n",
        "        L=int(dur*tar_sr); s=int(st*tar_sr)\n",
        "        if s>=len(yF): return None\n",
        "        y=yF[s:min(s+L,len(yF))]\n",
        "        if len(y)<L: y=np.pad(y,(0,L-len(y)))\n",
        "        if norm:\n",
        "            rms=float(np.sqrt(np.mean(y**2))+1e-12); y*=(10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(\"ERR load:\",e); return None\n",
        "\n",
        "# ---- YAMNet interface (robust + fallback) ---------------------------------\n",
        "YURL=\"https://tfhub.dev/google/yamnet/1\"\n",
        "\n",
        "# getters for hub outputs\n",
        "\n",
        "def _get_scores(out):\n",
        "    if isinstance(out, (list,tuple)):\n",
        "        return out[0]\n",
        "    if isinstance(out, dict):\n",
        "        return out.get('scores') or out.get('predictions')\n",
        "    return out\n",
        "\n",
        "def _get_spectrogram(out):\n",
        "    if isinstance(out, (list,tuple)):\n",
        "        return out[2] if len(out)>=3 else None\n",
        "    if isinstance(out, dict):\n",
        "        return out.get('spectrogram') or out.get('log_mel') or out.get('mel_spectrogram') or out.get('features')\n",
        "    return None\n",
        "\n",
        "def _emb_out(o):\n",
        "    e=None\n",
        "    if isinstance(o,(list,tuple)) and len(o)>=2: e=o[1]\n",
        "    elif isinstance(o,dict):\n",
        "        e=o.get('embeddings') or o.get('embedding')\n",
        "        if e is None:\n",
        "            for v in o.values():\n",
        "                if isinstance(v,dict):\n",
        "                    e=v.get('embeddings') or v.get('embedding');\n",
        "                    if e is not None: break\n",
        "    if e is None: return None\n",
        "    t=tf.convert_to_tensor(e)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: t=tf.expand_dims(t,0)\n",
        "    return t\n",
        "\n",
        "\n",
        "def mk_infer():\n",
        "    try:\n",
        "        mod=hub.load(YURL)\n",
        "        def f(y): return mod(tf.convert_to_tensor(y,tf.float32))\n",
        "        _=f(np.zeros(SR,np.float32))\n",
        "        print(\"[YAMNet] hub.load OK\")\n",
        "    except Exception as e:\n",
        "        print(\"[YAMNet] hub.load fail → KerasLayer fallback:\", e)\n",
        "        layer=hub.KerasLayer(YURL, trainable=False)\n",
        "        def f(y):\n",
        "            t=tf.convert_to_tensor(y,tf.float32)\n",
        "            try: return layer(t)\n",
        "            except: return layer(tf.expand_dims(t,0))\n",
        "        _=f(np.zeros(SR,np.float32))\n",
        "        mod=layer\n",
        "    # ship indices\n",
        "    shp=[]\n",
        "    try:\n",
        "        path=mod.class_map_path().numpy().decode('utf-8'); df=pd.read_csv(path); col='display_name' if 'display_name' in df.columns else df.columns[-1]\n",
        "        names=df[col].astype(str).str.lower().tolist()\n",
        "        subs=[\"boat\",\"ship\",\"sail\",\"sailing\",\"ferry\",\"cargo\",\"tanker\",\"submarine\",\"motorboat\",\"watercraft\",\"water vehicle\",\"ocean liner\",\"yacht\",\"kayak\",\"canoe\",\"rowboat\",\"row\",\"fishing\",\"harbor\",\"engine\",\"propeller\"]\n",
        "        shp=[i for i,n in enumerate(names) if any(s in n for s in subs)]\n",
        "    except Exception: pass\n",
        "    return f, shp, mod\n",
        "\n",
        "# embedding & scores\n",
        "\n",
        "def emb1(infer,y,pool=\"meanstd\"):\n",
        "    if y is None: return None\n",
        "    try:\n",
        "        t=_emb_out(infer(y))\n",
        "        if t is None or t.shape.rank!=2 or int(t.shape[0])==0: return None\n",
        "        if pool==\"mean\": feat=tf.reduce_mean(t,0)\n",
        "        else: m=tf.reduce_mean(t,0); s=tf.math.reduce_std(t,0); feat=tf.concat([m,s],0)\n",
        "        out=feat.numpy().astype(np.float32)\n",
        "        if USE_MIXED: out=out.astype(np.float32)\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        print(\"ERR emb:\",e); return None\n",
        "\n",
        "def embN(infos,infer,C,pool=\"meanstd\",aug=None,ck=None,show=4000):\n",
        "    path=None\n",
        "    if C['cache_emb'] and ck:\n",
        "        path=f\"cache/emb_{ck}.npz\"\n",
        "        if os.path.exists(path):\n",
        "            z=np.load(path,allow_pickle=True); print(f\" - cache {path} | X:{z['X'].shape} keep:{z['keep'].shape}\"); return z['X'], z['keep']\n",
        "    X=[]; keep=[]\n",
        "    for i,inf in enumerate(infos,1):\n",
        "        y=load_seg(inf,C['seg_dur'],SR,NORM_AUDIO)\n",
        "        if aug:\n",
        "            y=y*(10**(rd.uniform(-3,3)/20)); sh=rd.randint(-int(0.25*SR),int(0.25*SR))\n",
        "            if sh>0: y=np.concatenate([np.zeros(sh,dtype=y.dtype),y[:-sh]])\n",
        "            elif sh<0: y=np.concatenate([y[-sh:],np.zeros(-sh,dtype=y.dtype)])\n",
        "        e=emb1(infer,y,pool);\n",
        "        if e is not None: X.append(e); keep.append(i-1)\n",
        "        if i%show==0: print(f\"  ... {i}/{len(infos)} (mem {mem()})\")\n",
        "    X=np.asarray(X,np.float32); keep=np.array(keep,np.int64)\n",
        "    if path and X.size>0: np.savez_compressed(path,X=X,keep=keep)\n",
        "    if X.size==0: print(\"ERR: no embeddings\")\n",
        "    return X,keep\n",
        "\n",
        "def y_sc(infer,y):\n",
        "    o=infer(y); s=_get_scores(o)\n",
        "    if s is None: return None\n",
        "    t=tf.convert_to_tensor(s)\n",
        "    if t.shape.rank==3 and t.shape[0]==1: t=tf.squeeze(t,0)\n",
        "    if t.shape.rank==1: return t.numpy().astype(np.float32)\n",
        "    return tf.reduce_mean(t,0).numpy().astype(np.float32)\n",
        "\n",
        "# ---- VAD QA plots ----------------------------------------------------------\n",
        "\n",
        "def plot_vad_hist(paths, cfg, out_png):\n",
        "    vals=[]\n",
        "    for fp in rd.sample(paths, min(80, len(paths))):\n",
        "        A,I=act_of(fp, cfg['vad_top_db'], cfg['vad_frame_sec'], cfg['vad_hop_sec'])\n",
        "        for s,e in A: vals.append(e-s)\n",
        "    if not vals: return\n",
        "    plt.figure(figsize=(6,3.2)); sns.histplot(vals, bins=30); plt.title('활성 구간 길이 분포 (샘플)'); plt.xlabel('sec'); plt.tight_layout(); plt.savefig(out_png,dpi=150); plt.close()\n",
        "\n",
        "# ---- Models ---------------------------------------------------------------\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true=tf.cast(y_true, y_pred.dtype)\n",
        "        eps=tf.keras.backend.epsilon()\n",
        "        y_pred=tf.clip_by_value(y_pred, eps, 1.0-eps)\n",
        "        ce=-y_true*tf.math.log(y_pred)\n",
        "        pt=tf.reduce_sum(y_true*y_pred, axis=-1)\n",
        "        fl=tf.pow(1.0-pt, gamma)\n",
        "        return tf.reduce_mean(tf.reduce_sum(alpha*fl*ce, axis=-1))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def build_mlp(in_dim, n_cls, lr):\n",
        "    reg=tf.keras.regularizers.l2(1e-4)\n",
        "    x=tf.keras.Input((in_dim,), dtype='float32')\n",
        "    h=tf.keras.layers.BatchNormalization()(x)\n",
        "    h=tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.5)(h)\n",
        "    h=tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg)(h); h=tf.keras.layers.Dropout(0.4)(h)\n",
        "    y=tf.keras.layers.Dense(n_cls, activation='softmax', dtype='float32')(h)  # force FP32 output\n",
        "    m=tf.keras.Model(x,y)\n",
        "    if USE_FOCAL:\n",
        "        loss=focal_loss()\n",
        "    else:\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTH)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(lr), loss=loss, metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "# ---- Train/Eval -----------------------------------------------------------\n",
        "\n",
        "def fit_eval(v,Xtr,ytr,Xte,yte,cls,cfg,pre):\n",
        "    if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"[emb] empty\")\n",
        "    scaler=None\n",
        "    if v.get('classifier')=='mlp':\n",
        "        Xtr_, Xte_ = Xtr, Xte\n",
        "        if MLP_SCALE_INPUT:\n",
        "            scaler=SS().fit(Xtr); Xtr_=scaler.transform(Xtr); Xte_=scaler.transform(Xte)\n",
        "        m=build_mlp(Xtr_.shape[-1], len(cls), cfg['lr'])\n",
        "        cb=[tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss'),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5, min_lr=1e-6)]\n",
        "        ytrc=tf.keras.utils.to_categorical(ytr, len(cls)); ytec=tf.keras.utils.to_categorical(yte, len(cls))\n",
        "        cw={c:len(ytr)/(len(np.unique(ytr))*cnt) for c,cnt in Counter(ytr).items()}\n",
        "        t0=time.time(); hist=m.fit(Xtr_,ytrc,validation_data=(Xte_,ytec),epochs=cfg['epochs'],batch_size=cfg['batch'],verbose=0,class_weight=cw,callbacks=cb)\n",
        "        P=m.predict(Xte_, verbose=0).astype(np.float32); pr=P.argmax(1); path=f\"artifacts/{v['name']}_mlp.keras\"; m.save(path); T=time.time()-t0\n",
        "        # plot history\n",
        "        plt.figure(figsize=(8,3.2))\n",
        "        if 'loss' in hist.history: plt.plot(hist.epoch, hist.history['loss'], label='loss')\n",
        "        if 'val_loss' in hist.history: plt.plot(hist.epoch, hist.history['val_loss'], label='val_loss')\n",
        "        if 'accuracy' in hist.history: plt.plot(hist.epoch, hist.history['accuracy'], label='acc')\n",
        "        if 'val_accuracy' in hist.history: plt.plot(hist.epoch, hist.history['val_accuracy'], label='val_acc')\n",
        "        plt.legend(); plt.title('Training'); plt.tight_layout(); plt.savefig(pre+\"_history.png\", dpi=150); plt.close()\n",
        "        if scaler is not None:\n",
        "            import joblib; joblib.dump(scaler, f\"artifacts/{v['name']}_mlp_scaler.joblib\")\n",
        "    else:\n",
        "        from sklearn.linear_model import LogisticRegression as LR; from sklearn.svm import SVC; import joblib\n",
        "        sc=SS().fit(Xtr); Xtr_=sc.transform(Xtr); Xte_=sc.transform(Xte); t0=time.time()\n",
        "        if v.get('classifier')=='logreg': clf=LR(max_iter=2000,class_weight='balanced',n_jobs=-1).fit(Xtr_,ytr); P=clf.predict_proba(Xte_); pr=P.argmax(1)\n",
        "        else: clf=SVC(C=2.0,kernel='rbf',probability=True,class_weight='balanced').fit(Xtr_,ytr); P=clf.predict_proba(Xte_); pr=P.argmax(1)\n",
        "        T=time.time()-t0; joblib.dump(clf,f\"artifacts/{v['name']}_{v.get('classifier','svm')}.joblib\"); joblib.dump(sc,f\"artifacts/{v['name']}_scaler.joblib\"); path=\"artifacts/*\"\n",
        "    yt=yte; res=dict(artifact=path,time_sec=T, acc=ACC(yt,pr), bal_acc=BACC(yt,pr), macroF1=F1(yt,pr,average='macro'))\n",
        "    try: res['macroROC']=AUC(tf.keras.utils.to_categorical(yt,len(cls)), P, average='macro', multi_class='ovr')\n",
        "    except: res['macroROC']=np.nan\n",
        "    try: res['topk']=TOPK(yt,P,k=CFG['topk'],labels=range(len(cls)))\n",
        "    except: res['topk']=np.nan\n",
        "    ap={};\n",
        "    for i,lb in enumerate(cls):\n",
        "        yb=(yt==i).astype(int); ap[lb]=float(AP(yb,P[:,i])) if 0<yb.sum()<len(yb) else float('nan')\n",
        "    res['ap_per_class']=ap; cm=CM(yt,pr); res['cm']=cm; res['probs']=P; res['pred']=pr; res['y_true']=yt\n",
        "    return res\n",
        "\n",
        "# ---- Plots utilities ------------------------------------------------------\n",
        "\n",
        "def p_cm(cm,cls,p,title='CM'):\n",
        "    plt.figure(figsize=(5.2,4.5)); sns.heatmap(cm,annot=True,fmt='d',cmap='Blues',xticklabels=cls,yticklabels=cls)\n",
        "    plt.xlabel('Pred'); plt.ylabel('True'); plt.title(title); plt.tight_layout(); plt.savefig(p,dpi=150); plt.close()\n",
        "\n",
        "# ---- XAI: Grad×Input, SmoothGrad, IG --------------------------------------\n",
        "\n",
        "# ship probability aggregator (fixed scalar bug)\n",
        "ship_p=lambda sc,idx: (float(1.0-np.prod(1.0 - np.asarray(sc)[idx])) if (sc is not None and len(idx)>0) else float('nan'))\n",
        "pship=lambda f,y,idx: ship_p(y_sc(f,y), idx)\n",
        "\n",
        "\n",
        "def grad_sal(mod,w,ship_idx,sm_n=0,sm_s=0.001):\n",
        "    def one(z):\n",
        "        with tf.GradientTape() as t:\n",
        "            zt=tf.convert_to_tensor(z,tf.float32)\n",
        "            out=mod(zt)\n",
        "            sc=_get_scores(out)\n",
        "            sp=_get_spectrogram(out)\n",
        "            if sp is None: raise RuntimeError('YAMNet spectrogram not available')\n",
        "            t.watch(sp)\n",
        "            p=tf.gather(sc,ship_idx,axis=1)\n",
        "            agg=1.0-tf.reduce_prod(1.0-p,axis=1)\n",
        "            tgt=tf.reduce_mean(agg)\n",
        "        g=t.gradient(tgt,sp)\n",
        "        if g is None: raise RuntimeError('grad is None; cannot compute saliency')\n",
        "        s=tf.nn.relu(g*sp)\n",
        "        return s.numpy()\n",
        "    if sm_n>0:\n",
        "        rms=float(np.sqrt(np.mean(w*w))+1e-12); std=sm_s*(rms if rms>0 else 1.0); acc=None\n",
        "        for _ in range(sm_n):\n",
        "            wn=w+np.random.normal(0.0,std,size=w.shape).astype(np.float32); s=one(wn); acc=s if acc is None else acc+s\n",
        "        sal=acc/float(sm_n)\n",
        "    else: sal=one(w)\n",
        "    return np.maximum(0.0, sal)\n",
        "\n",
        "# Integrated Gradients (simple path on spectrogram activation)\n",
        "\n",
        "def integrated_grads(mod, w, ship_idx, steps=32):\n",
        "    z0=np.zeros_like(w, dtype=np.float32)\n",
        "    alphas=np.linspace(0,1,steps,endpoint=True).astype(np.float32)\n",
        "    acc=None\n",
        "    for a in alphas:\n",
        "        z=z0*(1-a)+w*a\n",
        "        with tf.GradientTape() as t:\n",
        "            zt=tf.convert_to_tensor(z,tf.float32)\n",
        "            out=mod(zt)\n",
        "            sc=_get_scores(out); sp=_get_spectrogram(out)\n",
        "            if sp is None: return None\n",
        "            t.watch(sp)\n",
        "            p=tf.gather(sc,ship_idx,axis=1)\n",
        "            agg=1.0-tf.reduce_prod(1.0-p,axis=1)\n",
        "            tgt=tf.reduce_mean(agg)\n",
        "        g=t.gradient(tgt,sp)\n",
        "        if g is None: return None\n",
        "        acc = g.numpy() if acc is None else acc + g.numpy()\n",
        "    return np.maximum(0.0, acc/len(alphas))\n",
        "\n",
        "# ---- Occlusion (time, freq, dual) -----------------------------------------\n",
        "\n",
        "def occ_t(f,y,sr,idx,win=80,hop=20):\n",
        "    if len(idx)==0: return None,None,None\n",
        "    base=pship(f,y,idx)\n",
        "    if not np.isfinite(base): return None,None,None\n",
        "    w=int(max(1,round(win/1000.0*sr))); h=int(max(1,round(hop/1000.0*sr))); L=len(y)\n",
        "    ctr,dp=[],[]\n",
        "    for st in range(0,max(1,L-w+1),h):\n",
        "        yy=y.copy(); yy[st:st+w]=0.0; p=pship(f,yy,idx)\n",
        "        dp.append(max(0.0,base-p) if np.isfinite(p) else 0.0); ctr.append((st+min(w//2,L-1))/sr)\n",
        "    return np.array(ctr,np.float32), np.array(dp,np.float32), float(base)\n",
        "\n",
        "\n",
        "def bs_taps(sr,f1,f2,ord=801):\n",
        "    ny=sr/2.0; f1=max(1.0,min(f1,ny*0.999)); f2=max(f1+1.0,min(f2,ny*0.999)); return sig.firwin(ord,[f1,f2],pass_zero='bandstop',fs=sr)\n",
        "\n",
        "def bs_apply(y,sr,f1,f2,ord=801):\n",
        "    taps=bs_taps(sr,f1,f2,ord)\n",
        "    try: return sig.filtfilt(taps,[1.0],y,method='pad').astype(np.float32)\n",
        "    except Exception: return sig.lfilter(taps,[1.0],y).astype(np.float32)\n",
        "\n",
        "\n",
        "def occ_f(f,y,sr,idx,fmin=80,fmax=8000,nb=40,ord=801):\n",
        "    if len(idx)==0: return None,None,None\n",
        "    base=pship(f,y,idx)\n",
        "    if not np.isfinite(base): return None,None,None\n",
        "    ed=np.geomspace(max(1.0,fmin),min(fmax,sr/2*0.999),num=nb+1); cen=np.sqrt(ed[:-1]*ed[1:])\n",
        "    dp=[]\n",
        "    for a,b in zip(ed[:-1],ed[1:]):\n",
        "        yy=bs_apply(y,sr,a,b,ord); p=pship(f,yy,idx); dp.append(max(0.0,base-p) if np.isfinite(p) else 0.0)\n",
        "    return cen.astype(np.float32), np.array(dp,np.float32), float(base)\n",
        "\n",
        "\n",
        "def occ_tf_heatmap(f,y,sr,idx,win=80,hop=40,fmin=80,fmax=8000,nb=32,ord=801):\n",
        "    # heavy: compute time windows × bandstop per band\n",
        "    w=int(max(1,round(win/1000.0*sr))); h=int(max(1,round(hop/1000.0*sr))); L=len(y)\n",
        "    ed=np.geomspace(max(1.0,fmin),min(fmax,sr/2*0.999),num=nb+1); cen=np.sqrt(ed[:-1]*ed[1:])\n",
        "    t_centers=[]; M=[]\n",
        "    base=pship(f,y,idx)\n",
        "    if not np.isfinite(base): return None,None,None, None\n",
        "    for st in range(0,max(1,L-w+1),h):\n",
        "        t_centers.append((st+min(w//2,L-1))/sr)\n",
        "        seg=y.copy(); seg[st:st+w]=0.0\n",
        "        row=[]\n",
        "        for a,b in zip(ed[:-1],ed[1:]):\n",
        "            yy=bs_apply(seg,sr,a,b,ord); p=pship(f,yy,idx); row.append(max(0.0,base-p) if np.isfinite(p) else 0.0)\n",
        "        M.append(row)\n",
        "    return np.array(t_centers,np.float32), cen.astype(np.float32), np.array(M,np.float32), float(base)\n",
        "\n",
        "# ---- Global band summary with bootstrap CI --------------------------------\n",
        "\n",
        "def _edges(sr,fmin,fmax,nb):\n",
        "    ed=np.geomspace(max(1.0,fmin), min(fmax, sr/2*0.999), num=nb+1); return ed, np.sqrt(ed[:-1]*ed[1:])\n",
        "\n",
        "def gb_sum(f, infos, y, pi, sr, fmin,fmax,nb,ord, m=None, seed=SEED, boot=0):\n",
        "    if len(ship_idx)==0: return None\n",
        "    rng=np.random.RandomState(seed); idx=np.arange(len(infos))\n",
        "    if (m is not None) and (m < len(idx)): idx=rng.choice(idx,size=m,replace=False)\n",
        "    ed,cen=_edges(sr,fmin,fmax,nb); S_all=[]; S_s=[]; S_n=[]\n",
        "    for i in idx:\n",
        "        yv=load_seg(infos[i], CFG['seg_dur'], sr, NORM_AUDIO)\n",
        "        if yv is None: continue\n",
        "        base=pship(f,yv,ship_idx)\n",
        "        if not np.isfinite(base): continue\n",
        "        d=[]\n",
        "        for a,b in zip(ed[:-1],ed[1:]):\n",
        "            yy=bs_apply(yv,sr,a,b,ord); p=pship(f,yy,ship_idx); d.append(max(0.0,base-p) if np.isfinite(p) else 0.0)\n",
        "        d=np.asarray(d,np.float64); S_all.append(d)\n",
        "        if y[i]==pi: S_s.append(d)\n",
        "        else: S_n.append(d)\n",
        "    if len(S_all)==0: return None\n",
        "    S_all=np.stack(S_all,0); mean_all=S_all.mean(0)\n",
        "    def _ci(arr):\n",
        "        if boot<=0 or len(arr)==0: return None\n",
        "        rng=np.random.RandomState(seed+123)\n",
        "        B=[]\n",
        "        for _ in range(boot):\n",
        "            idx=rng.randint(0,len(arr),size=len(arr)); B.append(arr[idx].mean(0))\n",
        "        B=np.stack(B,0)\n",
        "        lo=np.percentile(B,2.5,axis=0); hi=np.percentile(B,97.5,axis=0)\n",
        "        return lo,hi\n",
        "    out=dict(centers=cen.astype(np.float32), mean_all=mean_all.astype(np.float32), n_total=S_all.shape[0])\n",
        "    if len(S_s)>0:\n",
        "        S_s=np.stack(S_s,0); out['mean_ship']=S_s.mean(0).astype(np.float32); out['n_ship']=int(S_s.shape[0])\n",
        "        ci=_ci(S_s);\n",
        "        if ci is not None: out['ci_ship_lo']=ci[0].astype(np.float32); out['ci_ship_hi']=ci[1].astype(np.float32)\n",
        "    else: out['n_ship']=0\n",
        "    if len(S_n)>0:\n",
        "        S_n=np.stack(S_n,0); out['mean_noise']=S_n.mean(0).astype(np.float32); out['n_noise']=int(S_n.shape[0])\n",
        "        ci=_ci(S_n)\n",
        "        if ci is not None: out['ci_noise_lo']=ci[0].astype(np.float32); out['ci_noise_hi']=ci[1].astype(np.float32)\n",
        "    else: out['n_noise']=0\n",
        "    return out\n",
        "\n",
        "\n",
        "def gb_fig(d,out):\n",
        "    c=d['centers']/1000.0; plt.figure(figsize=(10,3.0))\n",
        "    plt.plot(c,d['mean_all'],label='Overall', linewidth=2)\n",
        "    if 'mean_ship' in d: plt.plot(c,d['mean_ship'],label='Ship(true)')\n",
        "    if 'mean_noise' in d: plt.plot(c,d['mean_noise'],label='Noise(true)')\n",
        "    if 'ci_ship_lo' in d: plt.fill_between(c,d['ci_ship_lo'],d['ci_ship_hi'],alpha=0.2, label='Ship 95% CI')\n",
        "    if 'ci_noise_lo' in d: plt.fill_between(c,d['ci_noise_lo'],d['ci_noise_hi'],alpha=0.2, label='Noise 95% CI')\n",
        "    plt.xlabel('Freq(kHz)'); plt.ylabel('ΔP(avg)'); plt.title('Dataset-level Band Importance'); plt.legend(); plt.tight_layout(); plt.savefig(out,dpi=150); plt.close()\n",
        "\n",
        "\n",
        "def gb_csv(d,out):\n",
        "    cols={'center_hz':d['centers'],'delta_p_overall':d['mean_all']}\n",
        "    if 'mean_ship' in d:\n",
        "        cols['delta_p_ship_true']=d['mean_ship']\n",
        "        if 'ci_ship_lo' in d: cols['ci_ship_lo']=d['ci_ship_lo']; cols['ci_ship_hi']=d['ci_ship_hi']\n",
        "    if 'mean_noise' in d:\n",
        "        cols['delta_p_noise_true']=d['mean_noise']\n",
        "        if 'ci_noise_lo' in d: cols['ci_noise_lo']=d['ci_noise_lo']; cols['ci_noise_hi']=d['ci_noise_hi']\n",
        "    pd.DataFrame(cols).to_csv(out,index=False)\n",
        "\n",
        "# ---- Split & pipeline -----------------------------------------------------\n",
        "\n",
        "def split(y,g,t=0.2,seed=SEED):\n",
        "    if len(y)<2 or len(set(y))<2:\n",
        "        raise RuntimeError(\"[데이터 부족] 세그먼트 수가 너무 적거나 클래스가 2종 미만\")\n",
        "    s=GSS(n_splits=1, test_size=t, random_state=seed); tr,te=next(s.split(np.arange(len(y)),y,g)); return tr,te\n",
        "\n",
        "VS=[\n",
        "    dict(name=\"v0a_yamnet_zeroshot\", type=\"zero\"),\n",
        "    dict(name=\"v0b_emb_logreg_basic\", type=\"emb\", classifier=\"logreg\", pooling=\"meanstd\", aug=None),\n",
        "    dict(name=\"v5_meanstd_mlp_aug\",  type=\"emb\", classifier=\"mlp\",    pooling=\"meanstd\", aug=\"light\"),\n",
        "    dict(name=\"v6_ft_mean_headonly\", type=\"ft\",  pooling=\"mean\",       aug=\"light\"),\n",
        "    dict(name=\"v7_ft_meanstd_headonly\", type=\"ft\", pooling=\"meanstd\",  aug=\"light\"),\n",
        "    dict(name=\"v8_ft_meanstd_headonly_tinyLR\", type=\"ft\", pooling=\"meanstd\", aug=\"light\"),\n",
        "]\n",
        "\n",
        "# ---- RUN ------------------------------------------------------------------\n",
        "print(\"Build segs…\")\n",
        "INF,LBL,GRP,SUM,MS=build_segs(DROOT,CFG)\n",
        "print(f\" - per-class: {dict(SUM)} | missing: {MS}\")\n",
        "if BIN: LBL=[\"Ship\" if l in \"ABCD\" else \"Noise\" for l in LBL]\n",
        "le=LE(); y=le.fit_transform(LBL); CLS=list(le.classes_); g=np.array(GRP)\n",
        "tr,te=split(y,g,CFG['test_size']); print(f\"[Split] train={len(tr)} test={len(te)} grp {len(set(g[tr]))}/{len(set(g[te]))}\")\n",
        "Xtr_i=[INF[i] for i in tr]; ytr=y[tr]; Xte_i=[INF[i] for i in te]; yte=y[te]\n",
        "\n",
        "# VAD QA\n",
        "plot_vad_hist(sorted(set([p for p,_,_ in INF])), CFG, \"results/part1/vad_active_len_hist.png\")\n",
        "\n",
        "print(\"YAMNet…\", end=\"\")\n",
        "infer, ship_idx, MOD = mk_infer(); print(f\" OK (ship_idx={len(ship_idx)})\")\n",
        "\n",
        "FB={}\n",
        "getF=lambda tag,infos,pool,aug: (FB.setdefault((tag,pool,aug or 'none',CFG['seg_dur'],len(infos)), embN(infos,infer,CFG,pool,aug, ck=f\"{tag}_pool={pool}_aug={(aug or 'none')}_seg={CFG['seg_dur']}s\")))\n",
        "\n",
        "rows=[]; PV={}\n",
        "\n",
        "for v in VS:\n",
        "    print(f\"\\n==== {v['name']} ====\")\n",
        "    pre=f\"results/part1/{v['name']}\"\n",
        "    if v['type'] in ('emb','ft'):\n",
        "        pool=v.get('pooling','meanstd'); aug=v.get('aug',None)\n",
        "        print(\" - emb train…\", end=\"\"); Xtr,kt=getF('train',Xtr_i,pool,aug); ytr_v=ytr[kt]; print(f\" OK {Xtr.shape} (mem {mem()})\")\n",
        "        print(\" - emb test …\", end=\"\"); Xte,ke=getF('test', Xte_i,pool,None); yte_v=yte[ke]; print(f\" OK {Xte.shape} (mem {mem()})\")\n",
        "        if Xtr.size==0 or Xte.size==0: raise RuntimeError(\"emb fail\")\n",
        "        res=fit_eval((dict(v, classifier='mlp') if v['type']=='ft' else v), Xtr,ytr_v,Xte,yte_v,CLS,CFG,pre)\n",
        "\n",
        "    elif v['type']=='zero':\n",
        "\n",
        "        if not ship_idx:\n",
        "            print(\" - skip zero\"); continue\n",
        "        print(\" - zero…\", end=\"\")\n",
        "\n",
        "        # ✅ FIXED: use y_sc() + ship_p() to avoid wrong axis slicing\n",
        "        def scoreL(infos):\n",
        "            s = []\n",
        "            kept = []\n",
        "            for i, inf in enumerate(infos):\n",
        "                yv = load_seg(inf, CFG['seg_dur'], SR, NORM_AUDIO)\n",
        "                if yv is None:\n",
        "                    continue\n",
        "                sc_mean = y_sc(infer, yv)  # 1D [classes], time-averaged\n",
        "                if sc_mean is None or not np.isfinite(sc_mean).any():\n",
        "                    continue\n",
        "                p = ship_p(sc_mean, ship_idx)  # 1 - prod(1 - p_ship_labels)\n",
        "                if np.isfinite(p):\n",
        "                    s.append(float(p)); kept.append(i)\n",
        "            return np.array(s, np.float32), np.array(kept, np.int64)\n",
        "\n",
        "        s_tr, ktr = scoreL(Xtr_i)\n",
        "        s_te, kte = scoreL(Xte_i)\n",
        "        ytr_v = ytr[ktr]; yte_v = yte[kte]\n",
        "        pi = CLS.index(POS) if POS in CLS else 1\n",
        "        ytr_b = (ytr_v == pi).astype(int); yte_b = (yte_v == pi).astype(int)\n",
        "\n",
        "        # threshold sweep on train for best F1\n",
        "        t_best, f_best = 0.5, -1.0\n",
        "        for t in np.linspace(0, 1, 41):\n",
        "            f = F1(ytr_b, (s_tr >= t).astype(int), average='binary', zero_division=0)\n",
        "            if f > f_best:\n",
        "                f_best, t_best = f, float(t)\n",
        "\n",
        "        pr = (s_te >= t_best).astype(int)\n",
        "        res = dict(\n",
        "            artifact=\"\",\n",
        "            time_sec=0.0,\n",
        "            acc=ACC(yte_b, pr),\n",
        "            bal_acc=BACC(yte_b, pr),\n",
        "            macroF1=F1(yte_b, pr, average='macro'),\n",
        "            macroROC=(AUC(yte_b, s_te) if len(np.unique(yte_b)) == 2 else np.nan),\n",
        "            topk=np.nan,\n",
        "            ap_per_class={POS: float(AP(yte_b, s_te))},\n",
        "            cm=CM(yte_b, pr),\n",
        "            probs=np.vstack([1.0 - s_te, s_te]).T,\n",
        "            pred=pr,\n",
        "            y_true=yte_v,\n",
        "        )\n",
        "        print(\" OK\")\n",
        "\n",
        "    else:\n",
        "        print(\" - unknown; skip\"); continue\n",
        "\n",
        "    # plots\n",
        "    p_cm(res['cm'], CLS, pre+\"_cm.png\", title=f\"CM — {v['name']}\")\n",
        "    if BIN and len(CLS)==2:\n",
        "        from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "        pi=CLS.index(POS) if POS in CLS else 1\n",
        "        p_=res['probs'][:,pi]\n",
        "        fpr,tpr,_=roc_curve((res['y_true']==pi).astype(int), p_)\n",
        "        prc,rec,_=precision_recall_curve((res['y_true']==pi).astype(int), p_)\n",
        "        plt.figure(figsize=(4.2,3.6)); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.tight_layout(); plt.savefig(pre+\"_roc.png\",dpi=150); plt.close()\n",
        "        plt.figure(figsize=(4.2,3.6)); plt.plot(rec,prc); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR'); plt.tight_layout(); plt.savefig(pre+\"_pr.png\",dpi=150); plt.close()\n",
        "\n",
        "    # save npz if requested\n",
        "    if SAVE_NPZ_PRED:\n",
        "        np.savez_compressed(f\"state/{v['name']}_test_preds.npz\", probs=res['probs'], pred=res['pred'], y_true=res['y_true'], test_index=(te if v['type']!='zero' else kte))\n",
        "\n",
        "    rows.append((dict(version=v['name'], type=v['type'], pooling=v.get('pooling','-'), classifier=(v.get('classifier','-') if v['type']=='emb' else 'mlp'), aug=(v.get('aug') or 'none'), acc=res['acc'], bal_acc=res['bal_acc'], macroF1=res['macroF1'], macroROC=res['macroROC'], topk=res['topk'], time_sec=res['time_sec'], artifact=res.get('artifact','')), res))\n",
        "    PV[v['name']]={\"cm_png\": pre+\"_cm.png\", \"history_png\": pre+\"_history.png\" if os.path.exists(pre+\"_history.png\") else None, \"roc_png\": pre+\"_roc.png\" if os.path.exists(pre+\"_roc.png\") else None, \"pr_png\": pre+\"_pr.png\" if os.path.exists(pre+\"_pr.png\") else None}\n",
        "\n",
        "# summary\n",
        "if not rows:\n",
        "    raise SystemExit(\"No results. Check data path.\")\n",
        "DF=pd.DataFrame([r[0] for r in rows]).sort_values([\"macroF1\",\"bal_acc\",\"acc\"],ascending=False)\n",
        "DF.to_csv(\"results/part1/summary_part1.csv\", index=False)\n",
        "print(\"\\n[SUMMARY]\"); print(DF.to_string(index=False))\n",
        "\n",
        "# ---- XAI & Occlusion on best version -------------------------------------\n",
        "PER_CAT=2\n",
        "spec_dir=None\n",
        "if (MAKE_XAI or MAKE_OCC) and len(rows)>0:\n",
        "    best=DF.iloc[0]['version']\n",
        "    rres=None\n",
        "    for meta,res in rows:\n",
        "        if meta['version']==best: rres=res; break\n",
        "    if rres is not None:\n",
        "        print(f\"\\n[XAI/SPEC] best: {best}\")\n",
        "        pi=CLS.index(POS) if POS in CLS else 1\n",
        "        P=rres['probs']; yt=rres['y_true']; pr=rres['pred']\n",
        "        pp=P[:,pi] if P.ndim==2 else P; yb=(yt==pi).astype(int)\n",
        "        tp=np.where((yb==1)&(pr==pi))[0]; fn=np.where((yb==1)&(pr!=pi))[0]; bd=np.where((pp>=0.45)&(pp<=0.55))[0]\n",
        "        pick=lambda a: a[:PER_CAT] if len(a)>PER_CAT else a\n",
        "        cats={\"tp\":pick(tp),\"fn\":pick(fn),\"bd\":pick(bd)}\n",
        "        spec_dir=f\"results/part1/xai_spec_v3_{best}\"; os.makedirs(spec_dir, exist_ok=True)\n",
        "        idx_to_info=lambda idx: Xte_i[idx] if idx < len(Xte_i) else Xte_i[-1]\n",
        "\n",
        "        for tag,arr in cats.items():\n",
        "            for j,idx in enumerate(arr):\n",
        "                info=idx_to_info(idx); yv=load_seg(info, CFG['seg_dur'], SR, NORM_AUDIO)\n",
        "                if yv is None: continue\n",
        "                try:\n",
        "                    out=MOD(tf.convert_to_tensor(yv,tf.float32))\n",
        "                    sc=_get_scores(out); sp=_get_spectrogram(out)\n",
        "                    T=int(tf.convert_to_tensor(sc).shape[0]) if tf.convert_to_tensor(sc).shape.rank>=1 else 1\n",
        "                    t=np.arange(T,dtype=np.float32)*0.48\n",
        "                    mel=sp.numpy() if sp is not None else None\n",
        "                    sal=grad_sal(MOD,yv,ship_idx,sm_n=XAI_SMOOTH_N,sm_s=XAI_SMOOTH_S)\n",
        "                    ig = integrated_grads(MOD,yv,ship_idx,steps=32) if USE_IG else None\n",
        "                    v=np.percentile(sal,95.0) if np.isfinite(sal).any() else 1.0; v=1.0 if v<=0 else v; s=np.clip(sal/v,0.0,1.0)\n",
        "                    plt.figure(figsize=(10,3.8))\n",
        "                    if mel is not None:\n",
        "                        plt.imshow(mel.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+(t[1]-t[0] if len(t)>1 else 0.48),0,s.shape[1]])\n",
        "                        plt.imshow(s.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+(t[1]-t[0] if len(t)>1 else 0.48),0,s.shape[1]],alpha=0.6)\n",
        "                    else:\n",
        "                        plt.imshow(s.T,aspect='auto',origin='lower')\n",
        "                    plt.title('Spec + Grad×Input'); plt.xlabel('Time(s)'); plt.ylabel('Mel bin'); plt.colorbar(); plt.tight_layout();\n",
        "                    plt.savefig(os.path.join(spec_dir,f\"{tag}_{j}_spec_grad.png\"),dpi=150); plt.close()\n",
        "                    if ig is not None:\n",
        "                        v=np.percentile(ig,95.0) if np.isfinite(ig).any() else 1.0; v=1.0 if v<=0 else v; g=np.clip(ig/v,0.0,1.0)\n",
        "                        plt.figure(figsize=(10,3.8))\n",
        "                        if mel is not None:\n",
        "                            plt.imshow(mel.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+(t[1]-t[0] if len(t)>1 else 0.48),0,g.shape[1]])\n",
        "                            plt.imshow(g.T,aspect='auto',origin='lower',extent=[t[0],t[-1]+(t[1]-t[0] if len(t)>1 else 0.48),0,g.shape[1]],alpha=0.6)\n",
        "                        else:\n",
        "                            plt.imshow(g.T,aspect='auto',origin='lower')\n",
        "                        plt.title('Spec + Integrated Gradients'); plt.xlabel('Time(s)'); plt.ylabel('Mel bin'); plt.colorbar(); plt.tight_layout();\n",
        "                        plt.savefig(os.path.join(spec_dir,f\"{tag}_{j}_spec_ig.png\"),dpi=150); plt.close()\n",
        "                except Exception as e:\n",
        "                    print(\"[WARN] Grad/IG skip:\",e)\n",
        "                if MAKE_OCC:\n",
        "                    try:\n",
        "                        tt,dp,base=occ_t(infer,yv,SR,ship_idx,O_TMS,O_THOP)\n",
        "                        if tt is not None:\n",
        "                            plt.figure(figsize=(10,3.0)); plt.plot(tt,dp); plt.ylabel('ΔP_ship'); plt.xlabel('Time(s)'); plt.title(f'Time Occlusion (base={base:.3f})'); plt.tight_layout();\n",
        "                            plt.savefig(os.path.join(spec_dir,f\"{tag}_{j}_time_occ.png\"),dpi=150); plt.close()\n",
        "                            pd.DataFrame({'time_sec':tt,'delta_p':dp}).to_csv(os.path.join(spec_dir,f\"{tag}_{j}_time_occ.csv\"), index=False)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] time occ skip:\",e)\n",
        "                    try:\n",
        "                        cen,dfp,base2=occ_f(infer,yv,SR,ship_idx,O_FMIN,O_FMAX,O_NB,O_ORD)\n",
        "                        if cen is not None:\n",
        "                            plt.figure(figsize=(10,3.0)); plt.bar(cen/1000.0,dfp,width=(cen*0.12)/1000.0); plt.xlabel('Freq(kHz)'); plt.ylabel('ΔP_ship'); plt.title(f'Freq Occlusion (base={base2:.3f})'); plt.tight_layout();\n",
        "                            plt.savefig(os.path.join(spec_dir,f\"{tag}_{j}_freq_occ.png\"),dpi=150); plt.close()\n",
        "                            pd.DataFrame({'center_hz':cen,'delta_p':dfp}).to_csv(os.path.join(spec_dir,f\"{tag}_{j}_freq_occ.csv\"), index=False)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] freq occ skip:\",e)\n",
        "                    if O_DUAL:\n",
        "                        try:\n",
        "                            tt, cen, M, base3=occ_tf_heatmap(infer,yv,SR,ship_idx,win=O_TMS,hop=O_THOP,fmin=O_FMIN,fmax=O_FMAX,nb=min(O_NB,24),ord=O_ORD)\n",
        "                            if M is not None:\n",
        "                                plt.figure(figsize=(10,4.0)); plt.imshow(M.T,aspect='auto',origin='lower',extent=[tt[0],tt[-1],cen[0]/1000.0,cen[-1]/1000.0]); plt.xlabel('Time(s)'); plt.ylabel('Freq(kHz)'); plt.title(f'Dual Occlusion ΔP (base={base3:.3f})'); plt.colorbar(); plt.tight_layout();\n",
        "                                plt.savefig(os.path.join(spec_dir,f\"{tag}_{j}_tf_occ.png\"),dpi=150); plt.close()\n",
        "                                # Save as npz to avoid huge CSV\n",
        "                                np.savez_compressed(os.path.join(spec_dir,f\"{tag}_{j}_tf_occ.npz\"), time_sec=tt, center_hz=cen, delta_p=M)\n",
        "                        except Exception as e:\n",
        "                            print(\"[WARN] tf occ skip:\", e)\n",
        "\n",
        "# ---- Global band summary --------------------------------------------------\n",
        "GB=None\n",
        "if MAKE_OCC and G_SUM and len(rows)>0:\n",
        "    print(\"\\n[GLOBAL] Band importance…\")\n",
        "    if G_SPLIT=='train': infos_ref,y_ref=Xtr_i,ytr\n",
        "    elif G_SPLIT=='all': infos_ref,y_ref= Xtr_i+Xte_i, np.concatenate([ytr,yte],0)\n",
        "    else: infos_ref,y_ref=Xte_i,yte\n",
        "    pi=CLS.index(POS) if POS in CLS else 1\n",
        "    GB=gb_sum(infer, infos_ref, y_ref, pi, SR, O_FMIN,O_FMAX,O_NB,O_ORD, m=G_MAX, seed=SEED, boot=G_BOOT)\n",
        "    if GB is not None:\n",
        "        png=\"results/part1/global_band_importance.png\"; csv_=\"results/part1/global_band_importance.csv\"; gb_fig(GB,png); gb_csv(GB,csv_)\n",
        "        print(f\" - saved: {png}, {csv_} | N={GB.get('n_total',0)} (Ship {GB.get('n_ship',0)}, Noise {GB.get('n_noise',0)})\")\n",
        "    else:\n",
        "        print(\" - skipped (no ship_idx or no valid samples)\")\n",
        "\n",
        "# ---- Summary CSV & report.md ---------------------------------------------\n",
        "DF.to_csv(\"results/part1/summary_part1.csv\", index=False)\n",
        "with open(\"results/part1/report_part1.md\",\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# Ship vs Noise — Part 1 v3 요약\\n\\n\")\n",
        "    f.write(\"|version|type|pooling|classifier|aug|acc|bal_acc|macroF1|macroROC|topk|time_sec|artifact|\\n\")\n",
        "    f.write(\"|---|---|---|---|---|---:|---:|---:|---:|---:|---:|---|\\n\")\n",
        "    for _,row in DF.iterrows():\n",
        "        macroROC = np.nan if pd.isna(row['macroROC']) else row['macroROC']\n",
        "        topk = np.nan if pd.isna(row['topk']) else row['topk']\n",
        "        f.write(f\"|{row['version']}|{row['type']}|{row['pooling']}|{row['classifier']}|{row['aug']}|{row['acc']:.4f}|{row['bal_acc']:.4f}|{row['macroF1']:.4f}|{macroROC:.4f}|{topk:.4f}|{row['time_sec']:.1f}|{row['artifact']}|\\n\")\n",
        "    f.write(\"\\n- 혼동행렬/ROC/PR: results/part1/*_{cm,roc,pr}.png\\n\")\n",
        "    f.write(\"- 전역 밴드 중요도 PNG/CSV: results/part1/global_band_importance.*\\n\")\n",
        "    f.write(\"- XAI/오클루전 샘플: results/part1/xai_spec_v3_*\\n\")\n",
        "\n",
        "# ---- Manifest (rich) ------------------------------------------------------\n",
        "H=lambda s: hashlib.sha1(open(s,'rb').read()).hexdigest() if os.path.exists(s) else None\n",
        "mani={\n",
        "    \"signature\": {\n",
        "        \"seed\": SEED,\n",
        "        \"device\": device_summary(),\n",
        "        \"flags\": {\n",
        "            \"USE_MIXED\": USE_MIXED, \"NORM_AUDIO\": NORM_AUDIO, \"CACHE_RATIO\": CACHE_RATIO,\n",
        "            \"FAST_SMOKE\": FAST_SMOKE, \"SAVE_NPZ_PRED\": SAVE_NPZ_PRED,\n",
        "            \"USE_FOCAL\": USE_FOCAL, \"LABEL_SMOOTH\": LABEL_SMOOTH, \"MLP_SCALE_INPUT\": MLP_SCALE_INPUT,\n",
        "            \"MAKE_XAI\": MAKE_XAI, \"USE_IG\": USE_IG, \"MAKE_OCC\": MAKE_OCC, \"O_DUAL\": O_DUAL,\n",
        "            \"G_SUM\": G_SUM, \"G_SPLIT\": G_SPLIT, \"G_MAX\": G_MAX, \"G_BOOT\": G_BOOT\n",
        "        }\n",
        "    },\n",
        "    \"cfg\": CFG,\n",
        "    \"classes\": CLS,\n",
        "    \"ship_idx_len\": len(ship_idx),\n",
        "    \"summary_csv\": \"results/part1/summary_part1.csv\",\n",
        "    \"report_md\": \"results/part1/report_part1.md\",\n",
        "    \"per_version_pngs\": PV,\n",
        "    \"xai_spec_dir\": spec_dir,\n",
        "    \"global_band\": {\n",
        "        \"png\": (\"results/part1/global_band_importance.png\" if (GB is not None) else None),\n",
        "        \"csv\": (\"results/part1/global_band_importance.csv\" if (GB is not None) else None),\n",
        "        \"n_total\": (GB.get('n_total',0) if GB is not None else 0),\n",
        "        \"n_ship\": (GB.get('n_ship',0) if GB is not None else 0),\n",
        "        \"n_noise\": (GB.get('n_noise',0) if GB is not None else 0)\n",
        "    },\n",
        "    \"hashes\": {\n",
        "        \"summary_csv\": H(\"results/part1/summary_part1.csv\"),\n",
        "        \"report_md\": H(\"results/part1/report_part1.md\"),\n",
        "        \"global_band_csv\": H(\"results/part1/global_band_importance.csv\") if (GB is not None) else None\n",
        "    }\n",
        "}\n",
        "with open(\"state/part1_manifest.json\",\"w\",encoding=\"utf-8\") as f: json.dump(mani,f,ensure_ascii=False,indent=2)\n",
        "print(\"\\n[Part 1 v3 완료]\")\n",
        "print(json.dumps(mani, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca3c110-063b-4731-e541-7d1a2a53a7bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup…\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Data…\n",
            " - ShipsEar exists\n",
            "Build segs…\n",
            " - per-class: {'A': 1855, 'B': 3134, 'C': 5085, 'D': 1513, 'E': 1140} | missing: 0\n",
            "[Split] train=10227 test=2500 grp 68/17\n",
            "YAMNet…[YAMNet] hub.load OK\n",
            " OK (ship_idx=21)\n",
            "\n",
            "==== v0a_yamnet_zeroshot ====\n",
            " - zero… OK\n",
            "\n",
            "==== v0b_emb_logreg_basic ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=none_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.06 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.07 GB)\n",
            "\n",
            "==== v5_meanstd_mlp_aug ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.14 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.14 GB)\n",
            "\n",
            "==== v6_ft_mean_headonly ====\n",
            " - emb train… - cache cache/emb_train_pool=mean_aug=light_seg=1.0s.npz | X:(10227, 1024) keep:(10227,)\n",
            " OK (10227, 1024) (mem 4.23 GB)\n",
            " - emb test … - cache cache/emb_test_pool=mean_aug=none_seg=1.0s.npz | X:(2500, 1024) keep:(2500,)\n",
            " OK (2500, 1024) (mem 4.23 GB)\n",
            "\n",
            "==== v7_ft_meanstd_headonly ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.31 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.31 GB)\n",
            "\n",
            "==== v8_ft_meanstd_headonly_tinyLR ====\n",
            " - emb train… - cache cache/emb_train_pool=meanstd_aug=light_seg=1.0s.npz | X:(10227, 2048) keep:(10227,)\n",
            " OK (10227, 2048) (mem 4.33 GB)\n",
            " - emb test … - cache cache/emb_test_pool=meanstd_aug=none_seg=1.0s.npz | X:(2500, 2048) keep:(2500,)\n",
            " OK (2500, 2048) (mem 4.33 GB)\n",
            "\n",
            "[SUMMARY]\n",
            "                      version type pooling classifier   aug    acc  bal_acc  macroF1  macroROC  topk  time_sec                                          artifact\n",
            "v8_ft_meanstd_headonly_tinyLR   ft meanstd        mlp light 0.9944 0.975234 0.979869  0.999225   NaN 21.395949 artifacts/v8_ft_meanstd_headonly_tinyLR_mlp.keras\n",
            "           v5_meanstd_mlp_aug  emb meanstd        mlp light 0.9932 0.979414 0.975966  0.998575   NaN 29.157376            artifacts/v5_meanstd_mlp_aug_mlp.keras\n",
            "       v7_ft_meanstd_headonly   ft meanstd        mlp light 0.9924 0.981397 0.973393  0.998112   NaN 25.771091        artifacts/v7_ft_meanstd_headonly_mlp.keras\n",
            "          v6_ft_mean_headonly   ft    mean        mlp light 0.9924 0.964491 0.972478  0.998817   NaN 22.870530           artifacts/v6_ft_mean_headonly_mlp.keras\n",
            "         v0b_emb_logreg_basic  emb meanstd     logreg  none 0.9892 0.970005 0.962190  0.998571   NaN  3.068972                                       artifacts/*\n",
            "          v0a_yamnet_zeroshot zero       -        mlp  none 0.9240 0.500000 0.480249  0.815917   NaN  0.000000                                                  \n",
            "\n",
            "[XAI/SPEC] best: v8_ft_meanstd_headonly_tinyLR\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "[WARN] Grad/IG skip: grad is None; cannot compute saliency\n",
            "\n",
            "[GLOBAL] Band importance…\n",
            " - saved: results/part1/global_band_importance.png, results/part1/global_band_importance.csv | N=300 (Ship 280, Noise 20)\n",
            "\n",
            "[Part 1 v3 완료]\n",
            "{\n",
            "  \"signature\": {\n",
            "    \"seed\": 42,\n",
            "    \"device\": {\n",
            "      \"tf\": \"2.19.0\",\n",
            "      \"hub\": \"0.16.1\",\n",
            "      \"librosa\": \"0.10.2.post1\",\n",
            "      \"scipy\": \"1.13.1\",\n",
            "      \"py\": \"3.12.12\",\n",
            "      \"gpus\": 1,\n",
            "      \"cpus\": 1,\n",
            "      \"gpu_names\": [\n",
            "        \"/physical_device:GPU:0\"\n",
            "      ]\n",
            "    },\n",
            "    \"flags\": {\n",
            "      \"USE_MIXED\": true,\n",
            "      \"NORM_AUDIO\": true,\n",
            "      \"CACHE_RATIO\": 0.15,\n",
            "      \"FAST_SMOKE\": false,\n",
            "      \"SAVE_NPZ_PRED\": true,\n",
            "      \"USE_FOCAL\": false,\n",
            "      \"LABEL_SMOOTH\": 0.0,\n",
            "      \"MLP_SCALE_INPUT\": false,\n",
            "      \"MAKE_XAI\": true,\n",
            "      \"USE_IG\": true,\n",
            "      \"MAKE_OCC\": true,\n",
            "      \"O_DUAL\": false,\n",
            "      \"G_SUM\": true,\n",
            "      \"G_SPLIT\": \"test\",\n",
            "      \"G_MAX\": 300,\n",
            "      \"G_BOOT\": 200\n",
            "    }\n",
            "  },\n",
            "  \"cfg\": {\n",
            "    \"seg_dur\": 1.0,\n",
            "    \"ship_overlap\": 0.2,\n",
            "    \"noise_overlap\": 0.0,\n",
            "    \"vad_frame_sec\": 0.5,\n",
            "    \"vad_hop_sec\": 0.25,\n",
            "    \"vad_top_db\": 25.0,\n",
            "    \"test_size\": 0.2,\n",
            "    \"epochs\": 40,\n",
            "    \"batch\": 32,\n",
            "    \"lr\": 0.0005,\n",
            "    \"max_seg_per_group_per_class\": 500,\n",
            "    \"noise_jitter_sec\": 0.5,\n",
            "    \"topk\": 1,\n",
            "    \"cache_emb\": true\n",
            "  },\n",
            "  \"classes\": [\n",
            "    \"Noise\",\n",
            "    \"Ship\"\n",
            "  ],\n",
            "  \"ship_idx_len\": 21,\n",
            "  \"summary_csv\": \"results/part1/summary_part1.csv\",\n",
            "  \"report_md\": \"results/part1/report_part1.md\",\n",
            "  \"per_version_pngs\": {\n",
            "    \"v0a_yamnet_zeroshot\": {\n",
            "      \"cm_png\": \"results/part1/v0a_yamnet_zeroshot_cm.png\",\n",
            "      \"history_png\": null,\n",
            "      \"roc_png\": \"results/part1/v0a_yamnet_zeroshot_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v0a_yamnet_zeroshot_pr.png\"\n",
            "    },\n",
            "    \"v0b_emb_logreg_basic\": {\n",
            "      \"cm_png\": \"results/part1/v0b_emb_logreg_basic_cm.png\",\n",
            "      \"history_png\": null,\n",
            "      \"roc_png\": \"results/part1/v0b_emb_logreg_basic_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v0b_emb_logreg_basic_pr.png\"\n",
            "    },\n",
            "    \"v5_meanstd_mlp_aug\": {\n",
            "      \"cm_png\": \"results/part1/v5_meanstd_mlp_aug_cm.png\",\n",
            "      \"history_png\": \"results/part1/v5_meanstd_mlp_aug_history.png\",\n",
            "      \"roc_png\": \"results/part1/v5_meanstd_mlp_aug_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v5_meanstd_mlp_aug_pr.png\"\n",
            "    },\n",
            "    \"v6_ft_mean_headonly\": {\n",
            "      \"cm_png\": \"results/part1/v6_ft_mean_headonly_cm.png\",\n",
            "      \"history_png\": \"results/part1/v6_ft_mean_headonly_history.png\",\n",
            "      \"roc_png\": \"results/part1/v6_ft_mean_headonly_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v6_ft_mean_headonly_pr.png\"\n",
            "    },\n",
            "    \"v7_ft_meanstd_headonly\": {\n",
            "      \"cm_png\": \"results/part1/v7_ft_meanstd_headonly_cm.png\",\n",
            "      \"history_png\": \"results/part1/v7_ft_meanstd_headonly_history.png\",\n",
            "      \"roc_png\": \"results/part1/v7_ft_meanstd_headonly_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v7_ft_meanstd_headonly_pr.png\"\n",
            "    },\n",
            "    \"v8_ft_meanstd_headonly_tinyLR\": {\n",
            "      \"cm_png\": \"results/part1/v8_ft_meanstd_headonly_tinyLR_cm.png\",\n",
            "      \"history_png\": \"results/part1/v8_ft_meanstd_headonly_tinyLR_history.png\",\n",
            "      \"roc_png\": \"results/part1/v8_ft_meanstd_headonly_tinyLR_roc.png\",\n",
            "      \"pr_png\": \"results/part1/v8_ft_meanstd_headonly_tinyLR_pr.png\"\n",
            "    }\n",
            "  },\n",
            "  \"xai_spec_dir\": \"results/part1/xai_spec_v3_v8_ft_meanstd_headonly_tinyLR\",\n",
            "  \"global_band\": {\n",
            "    \"png\": \"results/part1/global_band_importance.png\",\n",
            "    \"csv\": \"results/part1/global_band_importance.csv\",\n",
            "    \"n_total\": 300,\n",
            "    \"n_ship\": 280,\n",
            "    \"n_noise\": 20\n",
            "  },\n",
            "  \"hashes\": {\n",
            "    \"summary_csv\": \"ea38ba455ae47637aa2d1ac36445480c268232e9\",\n",
            "    \"report_md\": \"6afa39d484bb2e4f0f4ae33b50622af3afe0d62c\",\n",
            "    \"global_band_csv\": \"3c3f156ed020a8485eb75f3b0b3281f9ca1336df\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ShipsEar — Part 2 v1.1 (Colab, hardened)\n",
        "# Purpose: Aggregate & visualize results produced by Part 1 v3.1\n",
        "# Patches included: [P1]~[P7] (deps, file guards, calibration bins, exemplar mapping, UMAP alignment, YAMNet spec extraction, report robustness)\n",
        "\n",
        "print(\"Setup…\")\n",
        "# [P1] Dependencies\n",
        "!pip -q install umap-learn==0.5.6 seaborn==0.13.2 pandas==2.2.2 scikit-learn==1.5.2 tqdm==4.67.1 tabulate==0.9.0\n",
        "try:\n",
        "    import tensorflow as tf, tensorflow_hub as hub\n",
        "except Exception:\n",
        "    !pip -q install \"tensorflow==2.19.0\" tensorflow_hub==0.16.1\n",
        "    import tensorflow as tf, tensorflow_hub as hub\n",
        "\n",
        "import os, json, glob, math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.metrics import (accuracy_score as ACC, balanced_accuracy_score as BACC,\n",
        "                             f1_score as F1, precision_recall_curve, roc_curve, auc)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# [P2] File presence checks\n",
        "# ----------------------------------------------------------------------\n",
        "assert Path(\"state/part1_manifest.json\").exists(), \"Missing state/part1_manifest.json (run Part 1 first)\"\n",
        "assert Path(\"results/part1/summary_part1.csv\").exists(), \"Missing results/part1/summary_part1.csv (run Part 1 first)\"\n",
        "\n",
        "# IO dirs\n",
        "R2 = Path(\"results/part2\"); R2.mkdir(parents=True, exist_ok=True)\n",
        "EX = R2 / \"exemplars\"; EX.mkdir(exist_ok=True)\n",
        "AR = R2 / \"archetype_spec\"; AR.mkdir(exist_ok=True)\n",
        "\n",
        "# Load manifest/summary\n",
        "MAN = json.load(open(\"state/part1_manifest.json\", \"r\", encoding=\"utf-8\"))\n",
        "VERS = MAN.get(\"versions\", [])\n",
        "SUM1 = pd.read_csv(\"results/part1/summary_part1.csv\")\n",
        "\n",
        "# Load test infos for index mapping\n",
        "# We reconstruct TEST_INFOS by re-reading Part 1's split indices if stored; else we cannot rebuild.\n",
        "# Part 1 saved per-version npz with 'test_index' which are indices into Part 1's Xte_i list order.\n",
        "# In Part 2 we only need to map those indices back to file paths/start_sec if available.\n",
        "# For portability, we also store a fallback CSV if available in state; check both.\n",
        "TEST_INFOS_F = Path(\"state/test_infos.csv\")\n",
        "TEST_INFOS = None\n",
        "if TEST_INFOS_F.exists():\n",
        "    df_infos = pd.read_csv(TEST_INFOS_F)\n",
        "    TEST_INFOS = [(r[\"file_path\"], float(r[\"start_sec\"]), int(r.get(\"sr\", 16000))) for _, r in df_infos.iterrows()]\n",
        "else:\n",
        "    # Graceful: we can still run most of Part 2 without paths; exemplars will omit file_path/start_sec\n",
        "    TEST_INFOS = []\n",
        "\n",
        "# Helper for safe mapping from test index → path/start\n",
        "\n",
        "def idx2path(i):  # [P4]\n",
        "    try:\n",
        "        ii = int(i)\n",
        "    except Exception:\n",
        "        return None, np.nan\n",
        "    if 0 <= ii < len(TEST_INFOS):\n",
        "        fp, st, _ = TEST_INFOS[ii]\n",
        "        return fp, float(st)\n",
        "    return None, np.nan\n",
        "\n",
        "# Collect prediction npz files from state/\n",
        "npz_files = sorted([p for p in Path(\"state\").glob(\"*_test_preds.npz\")])\n",
        "assert len(npz_files) > 0, \"No predictions found under state/*_test_preds.npz\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Leaderboard + per-version plots\n",
        "# ----------------------------------------------------------------------\n",
        "rows = []\n",
        "for npzf in npz_files:\n",
        "    ver = npzf.stem.replace(\"_test_preds\", \"\")\n",
        "    D = np.load(npzf, allow_pickle=True)\n",
        "    probs = D[\"probs\"]\n",
        "    pred  = D[\"pred\"]\n",
        "    y_true = D[\"y_true\"]\n",
        "    te_idx = D[\"test_index\"]  # indices into Part 1's Xte_i order\n",
        "\n",
        "    # Binary assumption (Ship vs Noise)\n",
        "    assert probs.shape[1] == 2, f\"Expected probs[:,2] for binary; got {probs.shape} in {ver}\"\n",
        "    p1 = probs[:, 1]\n",
        "\n",
        "    acc = ACC(y_true, pred)\n",
        "    ba  = BACC(y_true, pred)\n",
        "    f1  = F1(y_true, pred, average='macro')\n",
        "\n",
        "    rows.append(dict(version=ver, acc=acc, bal_acc=ba, macroF1=f1, n=len(y_true)))\n",
        "\n",
        "DF2 = pd.DataFrame(rows).sort_values([\"macroF1\", \"bal_acc\", \"acc\"], ascending=False)\n",
        "DF2.to_csv(R2/\"leaderboard_part2.csv\", index=False)\n",
        "print(\"[Part2] Leaderboard saved →\", R2/\"leaderboard_part2.csv\")\n",
        "\n",
        "# Choose best by same key used in Part 1\n",
        "best_ver = DF2.iloc[0][\"version\"]\n",
        "print(\"[Part2] Best version:\", best_ver)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Plots: Calibration / PR / ROC + Threshold sweep per version\n",
        "# ----------------------------------------------------------------------\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "for npzf in npz_files:\n",
        "    ver = npzf.stem.replace(\"_test_preds\", \"\")\n",
        "    D = np.load(npzf, allow_pickle=True)\n",
        "    probs = D[\"probs\"]; y_true = D[\"y_true\"]\n",
        "    p1 = probs[:, 1].astype(float)\n",
        "    yb = (y_true == 1).astype(int)  # assume class 1 is Ship as in Part 1\n",
        "\n",
        "    # Calibration [P3]\n",
        "    pos_n = int(yb.sum()); bins = max(5, min(15, pos_n//3 if pos_n>0 else 5))\n",
        "    try:\n",
        "        frac_pos, mean_pred = calibration_curve(yb, p1, n_bins=bins, strategy='quantile')\n",
        "        plt.figure(figsize=(4.2, 3.8))\n",
        "        plt.plot([0,1],[0,1],'--',linewidth=1)\n",
        "        plt.plot(mean_pred, frac_pos, marker='o')\n",
        "        plt.xlabel('Mean predicted probability')\n",
        "        plt.ylabel('Fraction of positives')\n",
        "        plt.title(f'Calibration – {ver} (pos={pos_n}, bins={bins})')\n",
        "        plt.tight_layout(); plt.savefig(R2/f\"{ver}_calibration.png\", dpi=150); plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] calibration skip for {ver}:\", e)\n",
        "\n",
        "    # PR Curve\n",
        "    try:\n",
        "        pr, rc, th = precision_recall_curve(yb, p1)\n",
        "        ap = auc(rc, pr)\n",
        "        plt.figure(figsize=(4.2,3.8))\n",
        "        plt.plot(rc, pr)\n",
        "        plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "        plt.title(f'PR – {ver} (AUC≈{ap:.3f})')\n",
        "        plt.tight_layout(); plt.savefig(R2/f\"{ver}_pr.png\", dpi=150); plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] PR curve skip for {ver}:\", e)\n",
        "\n",
        "    # ROC Curve\n",
        "    try:\n",
        "        fpr, tpr, th = roc_curve(yb, p1)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.figure(figsize=(4.2,3.8))\n",
        "        plt.plot([0,1],[0,1],'--',linewidth=1)\n",
        "        plt.plot(fpr, tpr)\n",
        "        plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "        plt.title(f'ROC – {ver} (AUC≈{roc_auc:.3f})')\n",
        "        plt.tight_layout(); plt.savefig(R2/f\"{ver}_roc.png\", dpi=150); plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] ROC curve skip for {ver}:\", e)\n",
        "\n",
        "    # Threshold sweep (+ BA best flag)\n",
        "    try:\n",
        "        T = np.linspace(0,1,201); F1s=[]; Accs=[]; BAs=[]\n",
        "        for t in T:\n",
        "            pr = (p1 >= t).astype(int)\n",
        "            F1s.append(F1(yb, pr, average='macro'))\n",
        "            Accs.append(ACC(yb, pr))\n",
        "            BAs.append(BACC(yb, pr))\n",
        "        best_idx = int(np.argmax(BAs)); t_star=float(T[best_idx])\n",
        "        out = pd.DataFrame({\"thr\":T, \"f1\":F1s, \"acc\":Accs, \"bal_acc\":BAs})\n",
        "        out[\"best_ba\"] = 0; out.loc[best_idx,\"best_ba\"] = 1\n",
        "        out.to_csv(R2/f\"{ver}_threshold_sweep.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] threshold sweep skip for {ver}:\", e)\n",
        "\n",
        "print(\"[Part2] Curves and sweeps done.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Exemplars (TP/FP/FN/TN) with robust mapping [P4]\n",
        "# ----------------------------------------------------------------------\n",
        "for npzf in npz_files:\n",
        "    ver = npzf.stem.replace(\"_test_preds\", \"\")\n",
        "    D = np.load(npzf, allow_pickle=True)\n",
        "    probs = D[\"probs\"]; pred = D[\"pred\"]; y_true = D[\"y_true\"]; te_idx = D[\"test_index\"]\n",
        "    p1 = probs[:,1]\n",
        "    yb = (y_true==1).astype(int)\n",
        "\n",
        "    TP = (pred==1) & (yb==1); FP = (pred==1) & (yb==0)\n",
        "    FN = (pred==0) & (yb==1); TN = (pred==0) & (yb==0)\n",
        "\n",
        "    df = pd.DataFrame({\"idx\": te_idx, \"y_true\": y_true, \"pred\": pred, \"p1\": p1, \"tp\": TP, \"fp\": FP, \"fn\": FN, \"tn\": TN})\n",
        "\n",
        "    ex_dir = EX / ver; ex_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for tag, asc in [(\"tp\", False), (\"fp\", False), (\"fn\", True), (\"tn\", True)]:\n",
        "        sub = df[df[tag]].copy()\n",
        "        paths, starts = [], []\n",
        "        for i in sub[\"idx\"].tolist():\n",
        "            fp, st = idx2path(i)\n",
        "            paths.append(fp); starts.append(st)\n",
        "        sub[\"file_path\"] = paths\n",
        "        sub[\"start_sec\"] = starts\n",
        "        sub = sub[sub[\"file_path\"].notna()]  # drop if mapping failed\n",
        "        # sort: for tp/fp, high prob first; for fn/tn, low prob first\n",
        "        sub.sort_values(\"p1\", ascending=asc, inplace=True)\n",
        "        sub.to_csv(ex_dir/f\"{tag}.csv\", index=False)\n",
        "\n",
        "print(\"[Part2] Exemplars saved →\", EX)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Archetype spectrograms for best version (mean log-mel per bucket)\n",
        "# ----------------------------------------------------------------------\n",
        "# We need YAMNet to (re)extract spectrograms from raw audio; fallback to per-file paths\n",
        "# If TEST_INFOS is missing we will skip this section gracefully\n",
        "\n",
        "YAM_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
        "module = hub.load(YAM_URL)\n",
        "\n",
        "def yam_infer(y):\n",
        "    return module(tf.convert_to_tensor(y, tf.float32))\n",
        "\n",
        "# Robust spectrogram access [P6]\n",
        "\n",
        "def _get_spec(o):\n",
        "    # tuple/list: try 3rd element, else dict at [0]\n",
        "    if isinstance(o, (list, tuple)):\n",
        "        if len(o) >= 3:\n",
        "            return o[2]\n",
        "        if len(o) >= 1 and isinstance(o[0], dict):\n",
        "            dd = o[0]\n",
        "            return (dd.get(\"spectrogram\") or dd.get(\"log_mel\") or dd.get(\"mel_spectrogram\") or dd.get(\"features\"))\n",
        "        return None\n",
        "    if isinstance(o, dict):\n",
        "        return (o.get(\"spectrogram\") or o.get(\"log_mel\") or o.get(\"mel_spectrogram\") or o.get(\"features\"))\n",
        "    return None\n",
        "\n",
        "# simple loader (must match Part 1's SR/norm for consistency)\n",
        "import soundfile as sf\n",
        "\n",
        "def load_wave(fp, start_sec, dur_sec, target_sr=16000, rms_norm=True):\n",
        "    try:\n",
        "        y, sr = sf.read(fp, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        # slice\n",
        "        i0 = int(start_sec*sr); L = int(dur_sec*sr)\n",
        "        y = y[i0:i0+L]\n",
        "        if len(y) < L:\n",
        "            y = np.pad(y, (0, L-len(y)))\n",
        "        # resample if needed\n",
        "        if sr != target_sr:\n",
        "            import scipy.signal as sig\n",
        "            g = math.gcd(int(sr), int(target_sr)); up=int(target_sr)//g; down=int(sr)//g\n",
        "            y = sig.resample_poly(y, up, down).astype(np.float32)\n",
        "        if rms_norm:\n",
        "            rms = float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "            y *= (10**(-20/20))/rms\n",
        "        return y.astype(np.float32)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Build buckets for best version\n",
        "best_npz = [p for p in npz_files if p.stem.replace(\"_test_preds\",\"\") == best_ver][0]\n",
        "D = np.load(best_npz, allow_pickle=True)\n",
        "probs = D[\"probs\"]; pred = D[\"pred\"]; y_true = D[\"y_true\"]; te_idx = D[\"test_index\"]\n",
        "\n",
        "if len(TEST_INFOS) == 0:\n",
        "    print(\"[archetype] TEST_INFOS not available → skip archetype spectrograms\")\n",
        "else:\n",
        "    print(\"[archetype] building mean log-mel per bucket…\")\n",
        "    buckets = {\n",
        "        \"tp_ship\": ((pred==1) & (y_true==1)),\n",
        "        \"fn_ship\": ((pred==0) & (y_true==1)),\n",
        "        \"tn_noise\":((pred==0) & (y_true==0)),\n",
        "        \"fp_noise\":((pred==1) & (y_true==0)),\n",
        "    }\n",
        "    SEG_DUR = 1.0  # must match Part 1 cfg\n",
        "    for name, mask in buckets.items():\n",
        "        idxs = np.where(mask)[0].tolist()\n",
        "        S_acc = None; n_eff = 0\n",
        "        for j in tqdm(idxs[:400], desc=f\"{name}\"):\n",
        "            # map to path\n",
        "            fp, st = idx2path(te_idx[j])\n",
        "            if fp is None: continue\n",
        "            y = load_wave(fp, st, SEG_DUR, 16000, True)\n",
        "            if y is None: continue\n",
        "            out = yam_infer(y)\n",
        "            S = _get_spec(out)\n",
        "            if S is None: continue\n",
        "            S = np.array(S)\n",
        "            # squeeze [1,T,F] → [T,F]\n",
        "            if S.ndim == 3 and S.shape[0] == 1:\n",
        "                S = S[0]\n",
        "            if S.ndim != 2:\n",
        "                continue\n",
        "            if S_acc is None:\n",
        "                S_acc = np.zeros_like(S, dtype=np.float32)\n",
        "            if S.shape != S_acc.shape:\n",
        "                # resize time axis by simple linear resampling to match first shape\n",
        "                T_ref, F_ref = S_acc.shape\n",
        "                T = S.shape[0]\n",
        "                x = np.linspace(0, 1, T)\n",
        "                xr = np.linspace(0, 1, T_ref)\n",
        "                S = np.stack([np.interp(xr, x, S[:,k]) for k in range(S.shape[1])], axis=1)\n",
        "            S_acc += S; n_eff += 1\n",
        "        if S_acc is None or n_eff == 0:\n",
        "            print(f\"[archetype] skip {name}: no spectrograms decoded\")\n",
        "            continue\n",
        "        MEAN = (S_acc / max(1,n_eff))\n",
        "        # time axis extent\n",
        "        T = MEAN.shape[0]; dt = SEG_DUR / max(1, T)\n",
        "        extent = [0, T*dt, 0, MEAN.shape[1]]\n",
        "        plt.figure(figsize=(5.5, 2.8))\n",
        "        plt.imshow(MEAN.T, aspect='auto', origin='lower', extent=extent)\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.xlabel('Time (s)'); plt.ylabel('Mel bin')\n",
        "        plt.title(f\"Mean log-mel – {name} (n={n_eff})\")\n",
        "        plt.tight_layout(); plt.savefig(AR/f\"mean_logmel_{name}.png\", dpi=150); plt.close()\n",
        "    print(\"[archetype] saved →\", AR)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# UMAP from cached embeddings (aligned) [P5]\n",
        "# ----------------------------------------------------------------------\n",
        "# Try to locate a test-embedding cache from Part 1: cache/emb_test_pool=*_aug=none_seg=*.npz\n",
        "CANDS = sorted(Path(\"cache\").glob(\"emb_test_pool=*aug=none_*.npz\"))\n",
        "if len(CANDS) and len(TEST_INFOS):\n",
        "    try:\n",
        "        Z = np.load(CANDS[0], allow_pickle=True)\n",
        "        X = Z[\"X\"]; keep = Z[\"keep\"]  # keep maps from original test list index → row in X\n",
        "        # Build alignment between Part 2 te_idx and keep\n",
        "        idx_map = {int(v): j for j, v in enumerate(keep.tolist())}\n",
        "        sel = [idx_map[i] for i in D[\"test_index\"].tolist() if int(i) in idx_map]\n",
        "        if len(sel) >= 10:\n",
        "            X_aligned = X[np.array(sel)]\n",
        "            import umap\n",
        "            reducer = umap.UMAP(n_neighbors=30, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
        "            U = reducer.fit_transform(X_aligned)\n",
        "            m = min(len(U), len(D[\"y_true\"]))\n",
        "            yy = D[\"y_true\"][:m]; prd = D[\"pred\"][:m]\n",
        "            # truth color\n",
        "            plt.figure(figsize=(4.2,3.8))\n",
        "            plt.scatter(U[:m,0], U[:m,1], c=yy, s=8, alpha=0.8)\n",
        "            plt.title(\"UMAP (truth)\"); plt.tight_layout(); plt.savefig(R2/\"umap_truth.png\", dpi=150); plt.close()\n",
        "            # pred color\n",
        "            plt.figure(figsize=(4.2,3.8))\n",
        "            plt.scatter(U[:m,0], U[:m,1], c=prd, s=8, alpha=0.8)\n",
        "            plt.title(\"UMAP (pred)\"); plt.tight_layout(); plt.savefig(R2/\"umap_pred.png\", dpi=150); plt.close()\n",
        "            print(\"[UMAP] saved →\", R2/\"umap_truth.png\", R2/\"umap_pred.png\")\n",
        "        else:\n",
        "            print(\"[UMAP] insufficient overlap between cache keep and te_idx; skipping\")\n",
        "    except Exception as e:\n",
        "        print(\"[UMAP] skipped:\", e)\n",
        "else:\n",
        "    print(\"[UMAP] cache or TEST_INFOS missing; skipping\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Global band overlay (optional): reuse Part 1 output if present\n",
        "# ----------------------------------------------------------------------\n",
        "GB_CSV = Path(\"results/part1/global_band_importance.csv\")\n",
        "if GB_CSV.exists():\n",
        "    gb = pd.read_csv(GB_CSV)\n",
        "    plt.figure(figsize=(7.0,3.0))\n",
        "    x = gb['center_hz'].values/1000.0\n",
        "    plt.plot(x, gb['delta_p_overall'].values, label='Overall', linewidth=2)\n",
        "    if 'delta_p_ship_true' in gb.columns:\n",
        "        plt.plot(x, gb['delta_p_ship_true'].values, label='Ship(true)')\n",
        "    if 'delta_p_noise_true' in gb.columns:\n",
        "        plt.plot(x, gb['delta_p_noise_true'].values, label='Noise(true)')\n",
        "    plt.xlabel('Freq (kHz)'); plt.ylabel('ΔP(avg)'); plt.title('Global Band Importance (from Part 1)'); plt.legend()\n",
        "    plt.tight_layout(); plt.savefig(R2/\"global_band_overlay.png\", dpi=150); plt.close()\n",
        "    print(\"[overlay] saved →\", R2/\"global_band_overlay.png\")\n",
        "else:\n",
        "    print(\"[overlay] Part 1 global band csv not found; skipping overlay\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Report (markdown-lite) [P7]\n",
        "# ----------------------------------------------------------------------\n",
        "with open(R2/\"report_part2.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# Part 2 – Results Aggregation & Analytics\\n\\n\")\n",
        "    f.write(\"- Leaderboard CSV: results/part2/leaderboard_part2.csv\\n\")\n",
        "    f.write(\"- Calibration/PR/ROC images: results/part2/*_{calibration,pr,roc}.png\\n\")\n",
        "    f.write(\"- Threshold sweeps: results/part2/*_threshold_sweep.csv\\n\")\n",
        "    f.write(\"- Exemplars CSV: results/part2/exemplars/<version>/{tp,fp,fn,tn}.csv\\n\")\n",
        "    f.write(\"- Archetype spectrograms (best): results/part2/archetype_spec/mean_logmel_*.png\\n\")\n",
        "    f.write(\"- UMAP (if available): results/part2/umap_*.png\\n\")\n",
        "    f.write(\"- Global band overlay: results/part2/global_band_overlay.png\\n\")\n",
        "\n",
        "print(\"\\n[Part2] All done. See results/part2/* and report_part2.md\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tMLJKgpqHym",
        "outputId": "a553128a-1986-46e7-b687-a5f753612fdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup…\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m[Part2] Leaderboard saved → results/part2/leaderboard_part2.csv\n",
            "[Part2] Best version: v8_ft_meanstd_headonly_tinyLR\n",
            "[Part2] Curves and sweeps done.\n",
            "[Part2] Exemplars saved → results/part2/exemplars\n",
            "[archetype] TEST_INFOS not available → skip archetype spectrograms\n",
            "[UMAP] cache or TEST_INFOS missing; skipping\n",
            "[overlay] saved → results/part2/global_band_overlay.png\n",
            "\n",
            "[Part2] All done. See results/part2/* and report_part2.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "YAMNet Ship Detection – Visualization Toolkit\n",
        "Version: v3.1-colab-xai-solo (2025-11-11)\n",
        "\n",
        "What this script does (single-audio, no-beam)\n",
        "--------------------------------------------\n",
        "1) Runs YAMNet on a mono audio file (16 kHz assumed; auto-resample).\n",
        "2) Aggregates ship-related probabilities (labels CSV keyword match or explicit indices).\n",
        "3) Visuals & reports:\n",
        "   - Fig1: YAMNet Log-Mel Spectrogram\n",
        "   - Fig2: Ship probability over time\n",
        "   - Fig5: Spectrogram + gradient saliency overlay (Grad×Input on log-mel)\n",
        "   - Fig6: Frequency-band importance (time-averaged saliency, Hz labels) + CSV\n",
        "   - Fig7: (optional) Time–Frequency Patch Occlusion heatmap + CSV (model-agnostic)\n",
        "   - Fig4: (optional) Time occlusion saliency (1D) + CSV\n",
        "   - Curves CSV: times, P_ship, MSP-like, entropy(ship)\n",
        "\n",
        "Colab-friendly\n",
        "--------------\n",
        "- Interactive wizard (file upload) when no args are provided in Colab.\n",
        "- Auto-install lightweight deps (tensorflow_hub, soundfile). Uses Colab's TensorFlow.\n",
        "- Auto-fetches `yamnet_class_map.csv` if missing.\n",
        "\n",
        "Removed\n",
        "-------\n",
        "- **All beam/DOA features removed** (no multi-file, no beam heatmap, no angles).\n",
        "\n",
        "CHANGELOG\n",
        "---------\n",
        "2025-11-11 v3.1-colab-xai-solo:\n",
        "- FIX: Added missing `occlusion_time_saliency` function (1D time occlusion).\n",
        "- FIX: Restored proper mel→Hz mapping for frequency-importance; CSV now includes Hz.\n",
        "- FIX: More robust gradient path (explicitly watch waveform tensor); clear error if grad None.\n",
        "- Minor: Cleaned labels, comments, ensured consistent plotting extents.\n",
        "\n",
        "2025-11-11 v3-colab-xai-solo:\n",
        "- Removed all beam/DOA options and code paths (project-wide clean).\n",
        "- Added reliability tracks: MSP-like & binary entropy of P_ship.\n",
        "- Added optional time–frequency patch occlusion heatmap (+ CSV export).\n",
        "- Kept IG-style saliency on spectrogram + frequency-importance CSV.\n",
        "\n",
        "2025-11-11 v2-colab-xai:\n",
        "- Gradient-based spectrogram XAI (saliency on YAMNet log-mel), SmoothGrad option\n",
        "- Overlay figure, frequency-importance bar + CSV\n",
        "\n",
        "2025-11-11 v1-colab:\n",
        "- Colab interactive mode, auto deps/labels, inline display\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Sequence\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Audio I/O & DSP\n",
        "try:\n",
        "    import soundfile as sf\n",
        "    _HAVE_SF = True\n",
        "except Exception:\n",
        "    _HAVE_SF = False\n",
        "\n",
        "from scipy.signal import resample_poly, stft, istft\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "\n",
        "# ML\n",
        "import tensorflow as tf\n",
        "try:\n",
        "    import tensorflow_hub as hub\n",
        "    _HAVE_TFHUB = True\n",
        "except Exception:\n",
        "    _HAVE_TFHUB = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
        "YAMNET_SR = 16000\n",
        "DEFAULT_HOP_SEC = 0.48\n",
        "DEFAULT_WIN_SEC = 0.96\n",
        "\n",
        "LABELS_DEFAULT_URL = \"https://storage.googleapis.com/audioset/yamnet/yamnet_class_map.csv\"\n",
        "\n",
        "# ------------------------------ Colab helpers -------------------------------\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def _pip_install_if_missing(mod_name: str, pip_pkg: str):\n",
        "    try:\n",
        "        __import__(mod_name)\n",
        "    except Exception:\n",
        "        try:\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_pkg])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to install {pip_pkg}: {e}\")\n",
        "\n",
        "\n",
        "def ensure_colab_deps():\n",
        "    _pip_install_if_missing(\"tensorflow_hub\", \"tensorflow_hub\")\n",
        "    _pip_install_if_missing(\"soundfile\", \"soundfile\")\n",
        "\n",
        "\n",
        "def colab_upload_one() -> Optional[str]:\n",
        "    from google.colab import files  # type: ignore\n",
        "    print(\"\n",
        "[Colab] Select ONE audio file to upload…\")\n",
        "    up = files.upload()\n",
        "    for name, data in up.items():\n",
        "        path = f\"/content/{name}\"\n",
        "        with open(path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        print(f\"[Colab] Saved: {path}\")\n",
        "        return path\n",
        "    print(\"[Colab] No file uploaded.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def maybe_colab_mount_drive():\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        ans = input(\"Mount Google Drive? [y/N]: \").strip().lower()\n",
        "        if ans == 'y':\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"[Colab] Drive mounted at /content/drive\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def auto_fetch_labels_csv(dest_dir: str = \"/content\") -> Optional[str]:\n",
        "    dest = os.path.join(dest_dir, \"yamnet_class_map.csv\")\n",
        "    try:\n",
        "        import urllib.request\n",
        "        print(f\"[Info] Downloading label CSV → {dest}\")\n",
        "        urllib.request.urlretrieve(LABELS_DEFAULT_URL, dest)\n",
        "        return dest\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not download labels CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "# --------------------------------- Core -------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class InferenceResult:\n",
        "    times_sec: np.ndarray            # [T]\n",
        "    scores: np.ndarray               # [T, 521]\n",
        "    embeddings: Optional[np.ndarray] # [T, 1024]\n",
        "    log_mel: Optional[np.ndarray]    # [T, 64]\n",
        "\n",
        "\n",
        "def load_audio(path: str, target_sr: int = YAMNET_SR) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Load mono audio at target_sr. Returns (waveform_float32, sr).\"\"\"\n",
        "    if _HAVE_SF:\n",
        "        data, sr = sf.read(path, always_2d=False)\n",
        "        if data.ndim == 2:\n",
        "            data = np.mean(data, axis=1)\n",
        "    else:\n",
        "        from scipy.io import wavfile\n",
        "        sr, data = wavfile.read(path)\n",
        "        if data.dtype.kind in (\"i\", \"u\"):\n",
        "            peak = np.iinfo(data.dtype).max\n",
        "            data = data.astype(np.float32) / peak\n",
        "        elif data.dtype.kind == \"f\":\n",
        "            data = data.astype(np.float32)\n",
        "        if data.ndim == 2:\n",
        "            data = np.mean(data, axis=1)\n",
        "    if sr != target_sr:\n",
        "        g = math.gcd(sr, target_sr)\n",
        "        up = target_sr // g\n",
        "        down = sr // g\n",
        "        data = resample_poly(data, up, down).astype(np.float32)\n",
        "        sr = target_sr\n",
        "    if not np.isfinite(data).all():\n",
        "        data = np.nan_to_num(data).astype(np.float32)\n",
        "    return data.astype(np.float32), sr\n",
        "\n",
        "\n",
        "def _load_yamnet(yamnet_savedmodel: Optional[str] = None):\n",
        "    if yamnet_savedmodel:\n",
        "        model = tf.saved_model.load(yamnet_savedmodel)\n",
        "    else:\n",
        "        if not _HAVE_TFHUB:\n",
        "            raise RuntimeError(\"tensorflow_hub not available. Provide --yamnet_savedmodel for offline use.\")\n",
        "        model = hub.load(YAMNET_HANDLE)\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_yamnet(wav: np.ndarray, model, frame_hop_sec: float = DEFAULT_HOP_SEC) -> InferenceResult:\n",
        "    wav_tf = tf.convert_to_tensor(wav, dtype=tf.float32)\n",
        "    scores, embeddings, spectrogram = model(wav_tf)\n",
        "    scores_np = scores.numpy()\n",
        "    T = scores_np.shape[0]\n",
        "    times = np.arange(T, dtype=np.float32) * float(frame_hop_sec)\n",
        "    log_mel = spectrogram.numpy() if spectrogram is not None else None\n",
        "    embeds = embeddings.numpy() if embeddings is not None else None\n",
        "    return InferenceResult(times, scores_np, embeds, log_mel)\n",
        "\n",
        "\n",
        "def load_labels(labels_csv: Optional[str]) -> Optional[List[str]]:\n",
        "    if labels_csv is None:\n",
        "        return None\n",
        "    labels: List[str] = []\n",
        "    with open(labels_csv, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0 and (\",\" in line and (\"display_name\" in line or \"label\" in line)):\n",
        "                pass\n",
        "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
        "            if not parts:\n",
        "                continue\n",
        "            label = parts[-1]\n",
        "            labels.append(label)\n",
        "    labels = [l.strip().strip('\"') for l in labels if l.strip()]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def match_ship_indices(labels: Optional[List[str]], keywords: Sequence[str]) -> List[int]:\n",
        "    if not labels:\n",
        "        return []\n",
        "    lw = [l.lower() for l in labels]\n",
        "    idxs: List[int] = []\n",
        "    for i, l in enumerate(lw):\n",
        "        if any(kw.lower() in l for kw in keywords):\n",
        "            idxs.append(i)\n",
        "    return sorted(set(idxs))\n",
        "\n",
        "\n",
        "def ship_probability(scores: np.ndarray, ship_indices: Sequence[int]) -> np.ndarray:\n",
        "    if len(ship_indices) == 0:\n",
        "        raise ValueError(\"No ship_indices provided. Use --labels_csv + --ship_keywords or --ship_indices.\")\n",
        "    p = np.clip(scores[:, ship_indices], 0.0, 1.0)\n",
        "    agg = 1.0 - np.prod(1.0 - p, axis=1)\n",
        "    return agg\n",
        "\n",
        "\n",
        "def msp_like(scores: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Max over 521 sigmoid-like outputs (confidence proxy).\"\"\"\n",
        "    return np.max(np.clip(scores, 0.0, 1.0), axis=1)\n",
        "\n",
        "\n",
        "def binary_entropy(p: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
        "    p = np.clip(p, eps, 1.0 - eps)\n",
        "    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n",
        "\n",
        "# ----------------------------- Occlusion (1D) --------------------------------\n",
        "\n",
        "def occlusion_time_saliency(wav: np.ndarray,\n",
        "                            model,\n",
        "                            ship_indices: Sequence[int],\n",
        "                            base_prob: Optional[np.ndarray] = None,\n",
        "                            window_sec: float = 0.48,\n",
        "                            stride_sec: float = 0.24,\n",
        "                            frame_hop_sec: float = DEFAULT_HOP_SEC) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Time-wise occlusion: zero-out windows and observe drop in P_ship.\n",
        "    Returns (centers_sec, importance) where importance >= 0.\n",
        "    \"\"\"\n",
        "    if base_prob is None:\n",
        "        base_scores = run_yamnet(wav, model, frame_hop_sec).scores\n",
        "        base_prob = ship_probability(base_scores, ship_indices)\n",
        "    L = len(wav)\n",
        "    win = int(round(window_sec * YAMNET_SR))\n",
        "    hop = int(round(stride_sec * YAMNET_SR))\n",
        "    if win <= 0 or hop <= 0:\n",
        "        raise ValueError(\"Invalid occlusion window/stride.\")\n",
        "    centers = []\n",
        "    importances = []\n",
        "    for start in range(0, max(1, L - win), hop):\n",
        "        w = wav.copy()\n",
        "        w[start:start+win] = 0.0\n",
        "        sc = run_yamnet(w, model, frame_hop_sec).scores\n",
        "        p = ship_probability(sc, ship_indices)\n",
        "        M = min(len(base_prob), len(p))\n",
        "        drop = np.maximum(0.0, base_prob[:M] - p[:M])\n",
        "        centers.append((start + win/2) / float(YAMNET_SR))\n",
        "        importances.append(float(np.mean(drop)))\n",
        "    return np.array(centers, dtype=np.float32), np.array(importances, dtype=np.float32)\n",
        "\n",
        "# ----------------------- XAI: Spectrogram Gradients --------------------------\n",
        "\n",
        "def _hz_to_mel(hz: np.ndarray) -> np.ndarray:\n",
        "    return 2595.0 * np.log10(1.0 + hz / 700.0)\n",
        "\n",
        "def _mel_to_hz(mel: np.ndarray) -> np.ndarray:\n",
        "    return 700.0 * (10**(mel / 2595.0) - 1.0)\n",
        "\n",
        "def mel_center_frequencies(n_mels: int, fmin: float = 125.0, fmax: float = 7500.0) -> np.ndarray:\n",
        "    mel_min = _hz_to_mel(np.array([fmin]))[0]\n",
        "    mel_max = _hz_to_mel(np.array([fmax]))[0]\n",
        "    mels = np.linspace(mel_min, mel_max, num=n_mels)\n",
        "    return _mel_to_hz(mels)\n",
        "\n",
        "\n",
        "def compute_spec_grad_saliency(model,\n",
        "                               wav: np.ndarray,\n",
        "                               ship_indices: Sequence[int],\n",
        "                               smooth_samples: int = 0,\n",
        "                               smooth_sigma: float = 0.001) -> np.ndarray:\n",
        "    \"\"\"Return saliency map d target / d spectrogram (T×64), using ReLU(grad * spec).\n",
        "    - smooth_samples: number of noisy repetitions (0=off)\n",
        "    - smooth_sigma: noise std as fraction of waveform RMS\n",
        "    \"\"\"\n",
        "    def _single(w: np.ndarray) -> np.ndarray:\n",
        "        with tf.GradientTape() as tape:\n",
        "            wtf = tf.convert_to_tensor(w, dtype=tf.float32)\n",
        "            tape.watch(wtf)  # ensure path tracked\n",
        "            scores, embeddings, spectrogram = model(wtf)\n",
        "            # Compute target from scores\n",
        "            p = tf.gather(scores, ship_indices, axis=1)     # [T, K]\n",
        "            agg = 1.0 - tf.reduce_prod(1.0 - p, axis=1)     # [T]\n",
        "            target = tf.reduce_mean(agg)                    # scalar\n",
        "        # Grad w.r.t spectrogram output\n",
        "        grad = tape.gradient(target, spectrogram)\n",
        "        if grad is None:\n",
        "            raise RuntimeError(\"Gradient w.r.t spectrogram is None. Try --xai_smooth_samples > 0 or use --do_patch_occlusion.\")\n",
        "        sal = tf.nn.relu(grad * spectrogram)                # Grad×Input, positive\n",
        "        return sal.numpy()\n",
        "\n",
        "    if smooth_samples and smooth_samples > 0:\n",
        "        rms = float(np.sqrt(np.mean(np.square(wav)) + 1e-12))\n",
        "        std = smooth_sigma * (rms if rms > 0 else 1.0)\n",
        "        acc = None\n",
        "        for _ in range(smooth_samples):\n",
        "            wn = wav + np.random.normal(0.0, std, size=wav.shape).astype(np.float32)\n",
        "            s = _single(wn)\n",
        "            acc = s if acc is None else (acc + s)\n",
        "        sal = acc / float(smooth_samples)\n",
        "    else:\n",
        "        sal = _single(wav)\n",
        "    sal = np.maximum(0.0, sal)\n",
        "    return sal  # [T, 64]\n",
        "\n",
        "# -------------------- XAI: Patch Occlusion (Time–Frequency) ------------------\n",
        "\n",
        "def patch_occlusion_heatmap(wav: np.ndarray,\n",
        "                            model,\n",
        "                            ship_indices: Sequence[int],\n",
        "                            time_bins: int = 24,\n",
        "                            freq_bins: int = 16,\n",
        "                            nperseg: int = 1024,\n",
        "                            noverlap: int = 512) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Coarse 2D occlusion by zeroing STFT patches and measuring drop in P_ship.\n",
        "    Returns (heat[f_bins, t_bins], f_edges_Hz[f_bins+1], t_edges_sec[t_bins+1]).\n",
        "    Note: This is approximate and compute-heavy; use small grids.\n",
        "    \"\"\"\n",
        "    # Baseline\n",
        "    base_scores = run_yamnet(wav, model).scores\n",
        "    base_ship = ship_probability(base_scores, ship_indices)\n",
        "\n",
        "    # STFT\n",
        "    f, t, Z = stft(wav, fs=YAMNET_SR, nperseg=nperseg, noverlap=noverlap, window='hann', boundary='zeros', padded=True)\n",
        "    F, Tt = Z.shape\n",
        "\n",
        "    # Grid edges\n",
        "    t_edges = np.linspace(0, Tt, num=time_bins+1, dtype=int)\n",
        "    f_edges = np.linspace(0, F, num=freq_bins+1, dtype=int)\n",
        "\n",
        "    heat = np.zeros((freq_bins, time_bins), dtype=np.float32)\n",
        "\n",
        "    for ti in range(time_bins):\n",
        "        t0, t1 = t_edges[ti], t_edges[ti+1]\n",
        "        if t1 <= t0: continue\n",
        "        for fi in range(freq_bins):\n",
        "            f0, f1 = f_edges[fi], f_edges[fi+1]\n",
        "            if f1 <= f0: continue\n",
        "            Zm = Z.copy()\n",
        "            Zm[f0:f1, t0:t1] = 0.0\n",
        "            _, x = istft(Zm, fs=YAMNET_SR, nperseg=nperseg, noverlap=noverlap, window='hann')\n",
        "            x = x.astype(np.float32)\n",
        "            sc = run_yamnet(x, model).scores\n",
        "            shp = ship_probability(sc, ship_indices)\n",
        "            M = min(len(base_ship), len(shp))\n",
        "            drop = np.maximum(0.0, base_ship[:M] - shp[:M])\n",
        "            heat[fi, ti] = float(np.mean(drop))\n",
        "\n",
        "    # Convert STFT edges to Hz/sec\n",
        "    f_edges_hz = np.interp(f_edges, np.arange(F), f)\n",
        "    t_edges_sec = (t_edges / float(Tt)) * (len(wav) / float(YAMNET_SR))\n",
        "\n",
        "    return heat, f_edges_hz, t_edges_sec\n",
        "\n",
        "# ------------------------------ Plot helpers ---------------------------------\n",
        "\n",
        "def _maybe_display_image(path: str):\n",
        "    if in_colab():\n",
        "        try:\n",
        "            from IPython.display import Image, display\n",
        "            display(Image(filename=path))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def plot_log_mel(times: np.ndarray, log_mel: Optional[np.ndarray], outpath: str):\n",
        "    if log_mel is None:\n",
        "        return\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    dt = times[1]-times[0] if len(times) > 1 else DEFAULT_HOP_SEC\n",
        "    plt.imshow(log_mel.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, log_mel.shape[1]])\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Mel bin'); plt.title('YAMNet Log-Mel Spectrogram')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_ship_prob(times: np.ndarray, prob: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.plot(times, prob)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('P(ship)'); plt.title('Ship Probability over Time')\n",
        "    plt.grid(True); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_time_saliency(centers_sec: np.ndarray, importance: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.stem(centers_sec, importance, use_line_collection=True)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Importance (ΔP)'); plt.title('Time Occlusion Saliency')\n",
        "    plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_spec_with_saliency(times: np.ndarray,\n",
        "                            log_mel: Optional[np.ndarray],\n",
        "                            saliency: np.ndarray,\n",
        "                            outpath: str,\n",
        "                            alpha: float = 0.6,\n",
        "                            clip_percentile: float = 95.0):\n",
        "    sal = saliency.copy()\n",
        "    vmax = np.percentile(sal, clip_percentile) if np.isfinite(sal).any() else 1.0\n",
        "    if vmax <= 0:\n",
        "        vmax = 1.0\n",
        "    sal = np.clip(sal / vmax, 0.0, 1.0)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    dt = times[1]-times[0] if len(times) > 1 else DEFAULT_HOP_SEC\n",
        "    if log_mel is not None:\n",
        "        plt.imshow(log_mel.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]])\n",
        "        plt.imshow(sal.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]], alpha=alpha)\n",
        "        plt.title('Spectrogram + XAI Saliency Overlay')\n",
        "    else:\n",
        "        plt.imshow(sal.T, aspect='auto', origin='lower', extent=[times[0], times[-1] + dt, 0, sal.shape[1]])\n",
        "        plt.title('XAI Saliency (no spectrogram available)')\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Mel bin')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_freq_importance(freq_hz: np.ndarray, saliency: np.ndarray, outpath: str):\n",
        "    imp = np.mean(saliency, axis=0)  # [64]\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.bar(np.arange(len(imp)), imp)\n",
        "    # Sparse tick labels in kHz\n",
        "    idxs = np.linspace(0, len(imp)-1, num=8).astype(int)\n",
        "    labels = [f\"{freq_hz[i]/1000:.1f}k\" for i in idxs]\n",
        "    plt.xticks(idxs, labels)\n",
        "    plt.xlabel('Frequency (Hz)'); plt.ylabel('Importance (avg)')\n",
        "    plt.title('Frequency-band Importance (from saliency)')\n",
        "    plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "\n",
        "def plot_patch_occlusion(heat: np.ndarray, f_edges_hz: np.ndarray, t_edges_sec: np.ndarray, outpath: str):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    extent = [t_edges_sec[0], t_edges_sec[-1], f_edges_hz[0], f_edges_hz[-1]]\n",
        "    plt.imshow(heat, aspect='auto', origin='lower', extent=extent)\n",
        "    plt.xlabel('Time (s)'); plt.ylabel('Frequency (Hz)'); plt.title('Patch Occlusion Heatmap (ΔP_ship)')\n",
        "    plt.colorbar(); plt.tight_layout(); plt.savefig(outpath, dpi=150); plt.close()\n",
        "    _maybe_display_image(outpath)\n",
        "\n",
        "# ---------------------------------- CLI --------------------------------------\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description='YAMNet Ship Detection – Visualization Toolkit (Colab-ready, single-audio, XAI)')\n",
        "    p.add_argument('--audio', type=str, help='Audio file path (single). If omitted in Colab, an upload wizard appears.')\n",
        "    p.add_argument('--yamnet_savedmodel', type=str, default=os.getenv('YAMNET_SAVEDMODEL'), help='Local SavedModel dir for offline use.')\n",
        "    p.add_argument('--labels_csv', type=str, default=None, help='Path to yamnet_class_map.csv (auto-fetched in Colab if missing).')\n",
        "    p.add_argument('--ship_keywords', type=str, default='boat,ship,water vehicle,marine engine,ferry', help='Comma-separated keywords for label matching.')\n",
        "    p.add_argument('--ship_indices', type=str, default=None, help='Comma-separated explicit class indices (overrides keywords).')\n",
        "    p.add_argument('--frame_hop_sec', type=float, default=DEFAULT_HOP_SEC, help='Frame hop seconds for timeline.')\n",
        "    p.add_argument('--thr', type=float, default=0.5, help='Event threshold on aggregated ship prob.')\n",
        "    p.add_argument('--min_dur', type=float, default=1.0, help='Minimum event duration (s).')\n",
        "    p.add_argument('--smooth', type=float, default=1.0, help='Smoothing window (s) before thresholding.')\n",
        "    p.add_argument('--outdir', type=str, default='viz_out', help='Output directory for images & CSV.')\n",
        "    p.add_argument('--do_saliency', action='store_true', help='Compute time-occlusion saliency (1D)')\n",
        "    p.add_argument('--sal_win', type=float, default=0.48, help='Saliency occlusion window (s).')\n",
        "    p.add_argument('--sal_hop', type=float, default=0.24, help='Saliency occlusion stride (s).')\n",
        "    # XAI over spectrogram\n",
        "    p.add_argument('--xai_smooth_samples', type=int, default=0, help='SmoothGrad samples (0=off).')\n",
        "    p.add_argument('--xai_smooth_sigma', type=float, default=0.001, help='SmoothGrad noise std as fraction of waveform RMS.')\n",
        "    # Patch occlusion (2D)\n",
        "    p.add_argument('--do_patch_occlusion', action='store_true', help='Compute 2D time–frequency patch occlusion (slow).')\n",
        "    p.add_argument('--po_time_bins', type=int, default=24, help='Occlusion grid time bins.')\n",
        "    p.add_argument('--po_freq_bins', type=int, default=16, help='Occlusion grid freq bins.')\n",
        "    p.add_argument('--po_nperseg', type=int, default=1024, help='STFT nperseg for occlusion.')\n",
        "    p.add_argument('--po_noverlap', type=int, default=512, help='STFT noverlap for occlusion.')\n",
        "    return p.parse_args()\n",
        "\n",
        "# ------------------------------ Main / Entrypoint ----------------------------\n",
        "\n",
        "def colab_wizard(args):\n",
        "    print(\"\n",
        "[Colab] Interactive mode enabled.\")\n",
        "    ensure_colab_deps()\n",
        "    maybe_colab_mount_drive()\n",
        "\n",
        "    audio_path = args.audio\n",
        "    if not audio_path:\n",
        "        ans = input(\"Upload audio now? (y to upload) [y/N]: \").strip().lower()\n",
        "        if ans == 'y':\n",
        "            audio_path = colab_upload_one()\n",
        "\n",
        "    labels_csv = args.labels_csv\n",
        "    if labels_csv is None and args.ship_indices is None:\n",
        "        labels_csv = auto_fetch_labels_csv() or None\n",
        "        if labels_csv is None:\n",
        "            print(\"[Colab] Could not obtain labels CSV; use --ship_indices.\")\n",
        "\n",
        "    return audio_path, labels_csv\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    audio_path = args.audio\n",
        "    labels_csv = args.labels_csv\n",
        "\n",
        "    if in_colab() and not audio_path:\n",
        "        audio_path, labels_csv = colab_wizard(args)\n",
        "\n",
        "    if not audio_path:\n",
        "        raise SystemExit(\"Provide an audio file path.\")\n",
        "\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "    # Load model\n",
        "    model = _load_yamnet(args.yamnet_savedmodel)\n",
        "\n",
        "    # Labels / indices\n",
        "    labels = load_labels(labels_csv) if labels_csv else None\n",
        "    if args.ship_indices:\n",
        "        ship_idxs = sorted({int(x) for x in args.ship_indices.split(',') if x.strip()})\n",
        "    else:\n",
        "        kws = [k.strip() for k in (args.ship_keywords or '').split(',') if k.strip()]\n",
        "        ship_idxs = match_ship_indices(labels, kws)\n",
        "    if len(ship_idxs) == 0:\n",
        "        raise SystemExit(\"No ship-related class indices found. Provide --labels_csv and/or --ship_indices.\")\n",
        "\n",
        "    # Load audio\n",
        "    wav, _ = load_audio(audio_path, YAMNET_SR)\n",
        "\n",
        "    # Inference\n",
        "    res = run_yamnet(wav, model, frame_hop_sec=args.frame_hop_sec)\n",
        "    ship_prob = ship_probability(res.scores, ship_idxs)\n",
        "    msp = msp_like(res.scores)\n",
        "    ent_ship = binary_entropy(ship_prob)\n",
        "\n",
        "    # Fig1: Log-mel\n",
        "    fig1 = os.path.join(args.outdir, 'fig1_logmel.png')\n",
        "    plot_log_mel(res.times_sec, res.log_mel, fig1)\n",
        "\n",
        "    # Fig2: Ship prob\n",
        "    fig2 = os.path.join(args.outdir, 'fig2_prob.png')\n",
        "    plot_ship_prob(res.times_sec, ship_prob, fig2)\n",
        "\n",
        "    # Events CSV\n",
        "    events = segment_events(res.times_sec, ship_prob, thr=args.thr, min_dur=args.min_dur,\n",
        "                            smooth_win=args.smooth, hop_sec=args.frame_hop_sec)\n",
        "    if events:\n",
        "        import csv\n",
        "        with open(os.path.join(args.outdir, 'events.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow(['start_sec','end_sec','peak_prob'])\n",
        "            for s, e, p in events:\n",
        "                w.writerow([f\"{s:.3f}\", f\"{e:.3f}\", f\"{p:.4f}\"])\n",
        "\n",
        "    # Reliability CSV/plot (times, ship_prob, msp_like, entropy)\n",
        "    try:\n",
        "        import csv\n",
        "        with open(os.path.join(args.outdir, 'curves.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow(['time_sec','p_ship','msp_like','entropy_ship'])\n",
        "            for t, ps, m, h in zip(res.times_sec, ship_prob, msp, ent_ship):\n",
        "                w.writerow([f\"{t:.3f}\", f\"{ps:.6f}\", f\"{m:.6f}\", f\"{h:.6f}\"])\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not write curves.csv: {e}\")\n",
        "\n",
        "    # XAI: Spectrogram gradient saliency\n",
        "    sal = None\n",
        "    try:\n",
        "        sal = compute_spec_grad_saliency(model, wav, ship_idxs,\n",
        "                                         smooth_samples=args.xai_smooth_samples,\n",
        "                                         smooth_sigma=args.xai_smooth_sigma)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] XAI gradient saliency failed: {e}\")\n",
        "\n",
        "    if sal is not None:\n",
        "        fig5 = os.path.join(args.outdir, 'fig5_spec_xai_overlay.png')\n",
        "        plot_spec_with_saliency(res.times_sec, res.log_mel, sal, fig5)\n",
        "\n",
        "        # Frequency-importance summary with mel→Hz mapping\n",
        "        freq_hz = mel_center_frequencies(sal.shape[1], fmin=125.0, fmax=7500.0)\n",
        "        fig6 = os.path.join(args.outdir, 'fig6_freq_importance.png')\n",
        "        plot_freq_importance(freq_hz, sal, fig6)\n",
        "        # CSV export\n",
        "        try:\n",
        "            import csv\n",
        "            imp = np.mean(sal, axis=0)\n",
        "            with open(os.path.join(args.outdir, 'xai_freq_importance.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                w.writerow(['mel_bin','center_hz','importance_avg'])\n",
        "                for i, (hz, v) in enumerate(zip(freq_hz, imp)):\n",
        "                    w.writerow([i, f\"{hz:.2f}\", f\"{v:.8f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not write xai_freq_importance.csv: {e}\")\n",
        "\n",
        "    # Optional: 1D time-occlusion saliency\n",
        "    if args.do_saliency:\n",
        "        centers, imp = occlusion_time_saliency(wav, model, ship_idxs, base_prob=ship_prob,\n",
        "                                              window_sec=args.sal_win, stride_sec=args.sal_hop,\n",
        "                                              frame_hop_sec=args.frame_hop_sec)\n",
        "        fig4 = os.path.join(args.outdir, 'fig4_time_saliency.png')\n",
        "        plot_time_saliency(centers, imp, fig4)\n",
        "        try:\n",
        "            import csv\n",
        "            with open(os.path.join(args.outdir, 'saliency_time.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                w.writerow(['center_sec','importance_drop'])\n",
        "                for t, v in zip(centers, imp):\n",
        "                    w.writerow([f\"{t:.3f}\", f\"{v:.6f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not write saliency_time.csv: {e}\")\n",
        "\n",
        "    # Optional: 2D patch occlusion (slow)\n",
        "    if args.do_patch_occlusion:\n",
        "        try:\n",
        "            heat, f_edges_hz, t_edges_sec = patch_occlusion_heatmap(\n",
        "                wav, model, ship_idxs,\n",
        "                time_bins=args.po_time_bins,\n",
        "                freq_bins=args.po_freq_bins,\n",
        "                nperseg=args.po_nperseg,\n",
        "                noverlap=args.po_noverlap)\n",
        "            fig7 = os.path.join(args.outdir, 'fig7_patch_occlusion.png')\n",
        "            plot_patch_occlusion(heat, f_edges_hz, t_edges_sec, fig7)\n",
        "            # CSV export (grid)\n",
        "            import csv\n",
        "            with open(os.path.join(args.outdir, 'patch_occlusion.csv'), 'w', newline='', encoding='utf-8') as f:\n",
        "                w = csv.writer(f)\n",
        "                # header\n",
        "                w.writerow([\"freq_bin\",\"time_bin\",\"f_start_Hz\",\"f_end_Hz\",\"t_start_s\",\"t_end_s\",\"drop_avg\"])\n",
        "                for fi in range(heat.shape[0]):\n",
        "                    for ti in range(heat.shape[1]):\n",
        "                        w.writerow([\n",
        "                            fi, ti,\n",
        "                            f\"{f_edges_hz[fi]:.2f}\", f\"{f_edges_hz[fi+1]:.2f}\",\n",
        "                            f\"{t_edges_sec[ti]:.3f}\", f\"{t_edges_sec[ti+1]:.3f}\",\n",
        "                            f\"{heat[fi,ti]:.8f}\"])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Patch occlusion failed: {e}\")\n",
        "\n",
        "    # Meta\n",
        "    meta = {\n",
        "        'yamnet_savedmodel': bool(args.yamnet_savedmodel),\n",
        "        'labels_csv': bool(labels_csv),\n",
        "        'ship_indices': ship_idxs,\n",
        "        'frame_hop_sec': args.frame_hop_sec,\n",
        "        'thr': args.thr,\n",
        "        'min_dur': args.min_dur,\n",
        "        'smooth': args.smooth,\n",
        "        'audio_file': audio_path,\n",
        "        'xai_smooth_samples': args.xai_smooth_samples,\n",
        "        'xai_smooth_sigma': args.xai_smooth_sigma,\n",
        "        'do_patch_occlusion': args.do_patch_occlusion,\n",
        "        'po_time_bins': args.po_time_bins,\n",
        "        'po_freq_bins': args.po_freq_bins,\n",
        "        'po_nperseg': args.po_nperseg,\n",
        "        'po_noverlap': args.po_noverlap,\n",
        "    }\n",
        "    with open(os.path.join(args.outdir, 'run_meta.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Console summary\n",
        "    prints = [\"fig1_logmel.png\", \"fig2_prob.png\", \"curves.csv\"]\n",
        "    if sal is not None:\n",
        "        prints.extend([\"fig5_spec_xai_overlay.png\", \"fig6_freq_importance.png\", \"xai_freq_importance.csv\"])\n",
        "    if args.do_saliency:\n",
        "        prints.append(\"fig4_time_saliency.png\")\n",
        "    if args.do_patch_occlusion:\n",
        "        prints.append(\"fig7_patch_occlusion.png\")\n",
        "        prints.append(\"patch_occlusion.csv\")\n",
        "\n",
        "    print(\"\n",
        "Done. Outputs in:\", os.path.abspath(args.outdir))\n",
        "    print(\"- \", \", \".join(prints))\n",
        "\n",
        "    # Colab inline\n",
        "    if in_colab():\n",
        "        try:\n",
        "            from IPython.display import display, Audio\n",
        "            print(\"\n",
        "[Colab] Inline preview:\")\n",
        "            _maybe_display_image(fig2)\n",
        "            if sal is not None:\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig5_spec_xai_overlay.png'))\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig6_freq_importance.png'))\n",
        "            if args.do_patch_occlusion:\n",
        "                _maybe_display_image(os.path.join(args.outdir, 'fig7_patch_occlusion.png'))\n",
        "            # Play audio\n",
        "            mx = np.max(np.abs(wav)) + 1e-9\n",
        "            display(Audio(wav / mx, rate=YAMNET_SR))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-DpG3LNng-z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ OOD 평가 모듈 =================================\n",
        "# 이 블록은 기존 파이프라인에서 학습이 끝난 후에 붙여 실행하세요.\n",
        "# 필요 전역: YAMNET_SAMPLE_RATE, CONFIG, yamnet, clf(학습된 분류기), le,\n",
        "#            Xtr/ytr, Xte/yte, Xtr_info/Xte_info (option), BASE 경로\n",
        "# ==============================================================================\n",
        "\n",
        "import os, subprocess, random, math, gc, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa, librosa.display\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
        "\n",
        "# ---------- 1) Git에서 OOD 샘플 오디오 가볍게 수집 ----------\n",
        "OOD_ROOT = f\"{BASE}/ood_audio_corpus\"\n",
        "os.makedirs(OOD_ROOT, exist_ok=True)\n",
        "\n",
        "OOD_REPOS = [\n",
        "    # 소형 예제/테스트 오디오가 비교적 들어있는 경우가 많음\n",
        "    (\"https://github.com/openai/whisper.git\",          \"whisper\"),\n",
        "    (\"https://github.com/pytorch/audio.git\",           \"torchaudio\"),\n",
        "    (\"https://github.com/iver56/audiomentations.git\",  \"audiomentations\"),\n",
        "    (\"https://github.com/huggingface/transformers.git\",\"transformers\"),\n",
        "]\n",
        "\n",
        "def clone_if_needed(url, name):\n",
        "    dst = os.path.join(OOD_ROOT, name)\n",
        "    if not os.path.exists(dst):\n",
        "        try:\n",
        "            subprocess.run([\"git\",\"clone\",\"--depth\",\"1\",url,dst], check=True, capture_output=True)\n",
        "            print(f\" - OK: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - FAIL: {url} ({e})\")\n",
        "    else:\n",
        "        print(f\" - already exists: {url}\")\n",
        "    return dst\n",
        "\n",
        "print(\"\\n[OOD] 리포지토리 수집 ...\")\n",
        "repo_dirs = [clone_if_needed(u,n) for (u,n) in OOD_REPOS]\n",
        "\n",
        "# 오디오 확장자 패턴(넓게 잡되 개수 제한)\n",
        "EXTS = (\".wav\",\".flac\",\".ogg\",\".mp3\",\".m4a\",\".aac\",\".wma\",\".aiff\",\".aif\",\".aifc\",\".au\",\".mp2\",\".opus\")\n",
        "def find_audio_files(roots, max_total=200):\n",
        "    all_files=[]\n",
        "    for r in roots:\n",
        "        for ext in EXTS:\n",
        "            all_files += glob.glob(os.path.join(r, \"**\", f\"*{ext}\"), recursive=True)\n",
        "    # 너무 많은 경우 샘플링\n",
        "    if len(all_files) > max_total:\n",
        "        random.shuffle(all_files)\n",
        "        all_files = all_files[:max_total]\n",
        "    return all_files\n",
        "\n",
        "ood_files = find_audio_files(repo_dirs, max_total=250)\n",
        "print(f\" - 수집된 OOD 원본 파일: {len(ood_files)}\")\n",
        "\n",
        "# ---------- 2) OOD 세그먼트(5초) 스트리밍 생성 ----------\n",
        "def stream_segments_for_ood(file_path, seg_dur=5.0, stride=5.0, cap_per_file=6):\n",
        "    \"\"\"librosa.load 없이 스트리밍으로 5초 구간을 균일 스트라이드로 최대 cap만 추출\"\"\"\n",
        "    segs=[]\n",
        "    try:\n",
        "        info = sf.info(file_path)\n",
        "        total = info.frames\n",
        "        sr    = info.samplerate\n",
        "        if info.duration < seg_dur: return segs\n",
        "\n",
        "        # 균일 스트라이드로 시작점 후보 생성\n",
        "        starts = np.arange(0, info.duration - seg_dur + 1e-9, stride)\n",
        "        random.shuffle(starts)\n",
        "        for st in starts[:cap_per_file]:\n",
        "            segs.append((file_path, float(st), sr))\n",
        "    except:\n",
        "        pass\n",
        "    return segs\n",
        "\n",
        "# 너무 많이 뽑지 않도록 전체 cap (예: 800 세그먼트)\n",
        "OOD_GLOBAL_CAP = 800\n",
        "ood_segments=[]\n",
        "for f in ood_files:\n",
        "    segs = stream_segments_for_ood(f, seg_dur=CONFIG[\"segment_duration\"], stride=CONFIG[\"segment_duration\"], cap_per_file=6)\n",
        "    ood_segments.extend(segs)\n",
        "    if len(ood_segments) >= OOD_GLOBAL_CAP: break\n",
        "print(f\" - 생성된 OOD 세그먼트: {len(ood_segments)}\")\n",
        "\n",
        "# ---------- 3) OOD 임베딩 ----------\n",
        "def load_and_process_segment(info, duration, target_sr, rms_norm=True):\n",
        "    file_path, start_time, orig_sr = info\n",
        "    try:\n",
        "        start = int(start_time*orig_sr); num = int(duration*orig_sr)\n",
        "        y, _ = sf.read(file_path, start=start, stop=start+num, dtype='float32', always_2d=False)\n",
        "        if y.ndim>1: y = y.mean(axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_fast\")\n",
        "        if rms_norm:\n",
        "            rms = np.sqrt(np.mean(y**2))+1e-12\n",
        "            y = y * ((10**(-20/20))/rms)\n",
        "        return y\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def yamnet_embed_batch(infos, seg_dur=5.0, batch=128):\n",
        "    X=[]; rms_list=[]; kept=[]\n",
        "    for i,info in enumerate(infos):\n",
        "        y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=True)\n",
        "        if y is None: continue\n",
        "        # RMS(정규화 전에)도 저장해 에너지 편향 분석\n",
        "        y_raw = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "        rms_list.append(float(np.sqrt(np.mean(y_raw**2))+1e-12) if y_raw is not None else np.nan)\n",
        "        try:\n",
        "            _, emb, _ = yamnet(y)\n",
        "            if emb.shape[0] == 0: continue\n",
        "            X.append(tf.reduce_mean(emb, axis=0).numpy())\n",
        "            kept.append(info)\n",
        "        except:\n",
        "            continue\n",
        "        if (i+1)%500==0:\n",
        "            print(f\"  OOD 임베딩 {i+1}/{len(infos)}...\")\n",
        "    return np.asarray(X, dtype=np.float32), np.asarray(rms_list), kept\n",
        "\n",
        "print(\"\\n[OOD] 임베딩 추출 ...\")\n",
        "Xood, rms_ood, kept_ood = yamnet_embed_batch(ood_segments, seg_dur=CONFIG[\"segment_duration\"])\n",
        "print(f\" - Xood:{Xood.shape}\")\n",
        "\n",
        "if Xood.shape[0] == 0:\n",
        "    print(\"경고: OOD 임베딩이 비었습니다. 리포 소스나 max_total, cap을 조정해보세요.\")\n",
        "\n",
        "# ---------- 4) 임계값 선택(검증셋 TPR=95%) & ID/OOD FPR 비교 ----------\n",
        "# 학습에 사용한 train에서 validation을 분리(간단히 10% hold-out)\n",
        "def split_val_from_train(Xtr, ytr_onehot, val_ratio=0.1, seed=42):\n",
        "    n = len(Xtr)\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(idx)\n",
        "    k = max(1, int(round(n*val_ratio)))\n",
        "    val_idx = idx[:k]; tr_idx = idx[k:]\n",
        "    return Xtr[tr_idx], ytr_onehot[tr_idx], Xtr[val_idx], ytr_onehot[val_idx]\n",
        "\n",
        "Xtr_fit, ytr_fit, Xval, yval = split_val_from_train(Xtr, ytr, val_ratio=0.1, seed=SEED)\n",
        "\n",
        "# 재학습 없이 clf를 재사용하되, val 확률만 새로 추정\n",
        "p_val = clf.predict(Xval, verbose=0)\n",
        "p_te  = clf.predict(Xte,  verbose=0)\n",
        "\n",
        "ship_idx = list(le.classes_).index('ship')\n",
        "yval_bin = (yval.argmax(1)==ship_idx).astype(int)\n",
        "yte_bin  = (yte.argmax(1)==ship_idx).astype(int)\n",
        "\n",
        "def select_threshold_by_tpr(y_true_bin, y_score, target_tpr=0.95):\n",
        "    fpr, tpr, thr = roc_curve(y_true_bin, y_score)\n",
        "    # TPR이 target에 가장 근접한 점의 threshold\n",
        "    j = np.argmin(np.abs(tpr - target_tpr))\n",
        "    return float(thr[j]), float(tpr[j]), float(fpr[j])\n",
        "\n",
        "tau, tpr_at_tau, fpr_at_tau = select_threshold_by_tpr(yval_bin, p_val[:,ship_idx], target_tpr=0.95)\n",
        "print(f\"\\n[임계값] TPR@val≈95% → τ={tau:.4f} (val TPR={tpr_at_tau:.3f}, val FPR={fpr_at_tau:.3f})\")\n",
        "\n",
        "# ID-테스트 FPR / OOD FPR\n",
        "fpr_id  = float(((p_te[:,ship_idx] >= tau) & (yte_bin==0)).mean()) if len(yte_bin)>0 else float('nan')\n",
        "\n",
        "p_ood = clf.predict(Xood, verbose=0) if Xood.shape[0]>0 else np.zeros((0,len(le.classes_)),dtype=np.float32)\n",
        "fpr_ood = float((p_ood[:,ship_idx] >= tau).mean()) if p_ood.shape[0]>0 else float('nan')\n",
        "\n",
        "print(f\"[FPR] ID(Test) FPR@τ={fpr_id:.4f} | OOD FPR@τ={fpr_ood:.4f}\")\n",
        "\n",
        "# ---------- 5) 시각화: 확률 분포 / ROC-PR / 에너지 편향 ----------\n",
        "# (a) 확률 히스토그램\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.kdeplot(p_te[yte_bin==1, ship_idx], label=\"ID: ship\", fill=True, alpha=0.3)\n",
        "sns.kdeplot(p_te[yte_bin==0, ship_idx], label=\"ID: noise\", fill=True, alpha=0.3)\n",
        "if p_ood.shape[0]>0:\n",
        "    sns.kdeplot(p_ood[:, ship_idx], label=\"OOD (others)\", fill=True, alpha=0.3)\n",
        "plt.axvline(tau, color='k', ls='--', label=f\"τ={tau:.2f}\")\n",
        "plt.title(\"Ship 확률 분포(ID vs OOD)\"); plt.xlabel(\"P(ship)\"); plt.legend(); plt.grid(True, alpha=0.3); plt.show()\n",
        "\n",
        "# (b) ROC/PR (ID 기준)\n",
        "fpr_id_curve, tpr_id_curve, _ = roc_curve(yte_bin, p_te[:,ship_idx])\n",
        "roc_auc_id = auc(fpr_id_curve, tpr_id_curve)\n",
        "prec, rec, _ = precision_recall_curve(yte_bin, p_te[:,ship_idx])\n",
        "auprc = average_precision_score(yte_bin, p_te[:,ship_idx])\n",
        "\n",
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr_id_curve, tpr_id_curve, lw=2, label=f\"AUC={roc_auc_id:.3f}\")\n",
        "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (ID Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(rec, prec, lw=2)\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (ID Test), AUPRC={auprc:.3f}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# (c) 에너지 decile 별 FPR (ID-Noise vs OOD)\n",
        "def segment_rms(info, seg_dur=5.0):\n",
        "    y = load_and_process_segment(info, seg_dur, YAMNET_SAMPLE_RATE, rms_norm=False)\n",
        "    if y is None: return np.nan\n",
        "    return float(np.sqrt(np.mean(y**2))+1e-12)\n",
        "\n",
        "# ID-Noise RMS와 확률\n",
        "id_noise_idx = np.where(yte_bin==0)[0]\n",
        "rms_id_noise = np.array([segment_rms(Xte_info[i], CONFIG[\"segment_duration\"]) if 'Xte_info' in globals() else np.nan\n",
        "                         for i in id_noise_idx])\n",
        "prob_id_noise = p_te[id_noise_idx, ship_idx]\n",
        "\n",
        "def fpr_by_rms_decile(rms_arr, prob_arr, tau, n_bins=10):\n",
        "    valid = np.isfinite(rms_arr)\n",
        "    rms_arr, prob_arr = rms_arr[valid], prob_arr[valid]\n",
        "    if len(rms_arr) < 10:\n",
        "        return None\n",
        "    qs = np.quantile(rms_arr, np.linspace(0,1,n_bins+1))\n",
        "    bins = np.digitize(rms_arr, qs[1:-1], right=True)\n",
        "    out=[]\n",
        "    for b in range(n_bins):\n",
        "        m = (bins==b)\n",
        "        if m.sum()==0: out.append(np.nan)\n",
        "        else: out.append(float((prob_arr[m] >= tau).mean()))\n",
        "    return out, qs\n",
        "\n",
        "ood_rms = np.zeros(0);\n",
        "if len(kept_ood)>0:\n",
        "    ood_rms = np.array([segment_rms(info, CONFIG[\"segment_duration\"]) for info in kept_ood])\n",
        "\n",
        "res_id = fpr_by_rms_decile(rms_id_noise, prob_id_noise, tau, n_bins=10)\n",
        "res_ood = (None, None)\n",
        "if len(ood_rms)>0:\n",
        "    res_ood = fpr_by_rms_decile(ood_rms, p_ood[:,ship_idx], tau, n_bins=10)\n",
        "\n",
        "if res_id is not None:\n",
        "    fpr_bins_id, qs_id = res_id\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(range(1,11), fpr_bins_id, marker='o', label='ID-Noise')\n",
        "    if isinstance(res_ood[0], list):\n",
        "        plt.plot(range(1,11), res_ood[0], marker='o', label='OOD')\n",
        "    plt.xticks(range(1,11)); plt.xlabel(\"RMS decile (낮음→높음)\")\n",
        "    plt.ylabel(f\"FPR@τ\"); plt.title(\"에너지 구간별 FPR (낮을수록 좋음)\")\n",
        "    plt.grid(True, alpha=0.3); plt.legend(); plt.show()\n",
        "else:\n",
        "    print(\"RMS decile 분석을 위한 유효 표본이 부족합니다.\")\n",
        "\n",
        "print(\"\\n[요약]\")\n",
        "print(f\" - 임계값 τ(Val TPR≈95%): {tau:.3f}\")\n",
        "print(f\" - FPR(ID-noise)@τ: {fpr_id:.4f}\")\n",
        "print(f\" - FPR(OOD)@τ: {fpr_ood:.4f} (낮을수록 좋음)\")\n",
        "print(f\" - ROC-AUC(ID test): {roc_auc_id:.3f}, AUPRC(ID test): {auprc:.3f}\")\n",
        "print(\" - 그래프: 확률분포/ROC/PR/에너지-디사일 FPR으로, 에너지-편향 여부를 함께 점검\")\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "zLOD2kZhLg6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "f8e3697a",
        "outputId": "63f3be28-2f92-419c-c8b5-659eb3b16b46"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# 압축할 폴더 경로 (여기서는 'results' 폴더)\n",
        "folder_to_archive = 'results'\n",
        "\n",
        "# 생성될 zip 파일의 이름과 경로\n",
        "archive_name = 'results'\n",
        "archive_format = 'zip'\n",
        "\n",
        "# zip 파일 생성\n",
        "print(f\"'{folder_to_archive}' 폴더를 '{archive_name}.{archive_format}'로 압축합니다...\")\n",
        "shutil.make_archive(archive_name, archive_format, folder_to_archive)\n",
        "print(f\"'{archive_name}.{archive_format}' 파일이 성공적으로 생성되었습니다.\")\n",
        "\n",
        "# Colab 환경에서 다운로드 링크 제공\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import files\n",
        "    print(\"다운로드를 시작합니다...\")\n",
        "    files.download(f'{archive_name}.{archive_format}')\n",
        "else:\n",
        "    print(f\"'{os.path.abspath(f'{archive_name}.{archive_format}')}' 경로에서 파일을 찾을 수 있습니다.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'results' 폴더를 'results.zip'로 압축합니다...\n",
            "'results.zip' 파일이 성공적으로 생성되었습니다.\n",
            "다운로드를 시작합니다...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a68383b8-084a-45ce-bfe0-dfeaefa360f3\", \"results.zip\", 2141175)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}