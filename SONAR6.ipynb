{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5xB6k7swDIU7zyNomIY7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEOUL-ABSS/SHIPSHIP/blob/main/SONAR6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =- coding: utf-8 -*-\n",
        "# ==============================================================================\n",
        "#                 DeepShip/MBARI ìˆ˜ì¤‘ ìŒí–¥ ë¶„ë¥˜ í”„ë¡œì íŠ¸ (RAM ìµœì í™” v2 ìµœì¢…ë³¸)\n",
        "# ==============================================================================\n",
        "# ìµœì¢… ì „ë¬¸ê°€ ê²€í†  ì˜ê²¬ì„ ë°˜ì˜í•˜ì—¬, í›„ë³´ íƒìƒ‰ ê³¼ì •ì˜ RAM ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë°\n",
        "# ìµœì†Œ í™(min-heap) ë°©ì‹ì„ ë„ì…í•œ ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.\n",
        "#\n",
        "# [ì£¼ìš” ê°œì„  ì‚¬í•­]\n",
        "# 1. RAM ì‚¬ìš©ëŸ‰ ìµœì†Œí™”: ëª¨ë“  í›„ë³´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ëŠ” ëŒ€ì‹ , Top-Kê°œë§Œ ìœ ì§€í•˜ëŠ” í™ ìë£Œêµ¬ì¡°ë¥¼\n",
        "#                      ì‚¬ìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ MBARI ë°ì´í„°ì…‹ì„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "# 2. ì†ë„ í–¥ìƒ: í›„ë³´ ì„ë² ë”©ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡í•˜ì—¬ íƒìƒ‰ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.\n",
        "# 3. ì œì–´ ê°•í™”: í›„ë³´ íƒìƒ‰ ê°„ê²©, ë°°ì¹˜ í¬ê¸° ë“±ì„ CONFIGì—ì„œ ì‰½ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„°í™”.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ==============================================================================\n",
        "print(\"1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì¤‘...\")\n",
        "\n",
        "# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---\n",
        "!pip install -q tensorflow tensorflow_hub soundfile librosa boto3 noisereduce umap-learn\n",
        "\n",
        "# --- ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---\n",
        "import os, sys, subprocess, random, tempfile, shutil, gc, math, warnings, heapq\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa, librosa.display, soundfile as sf\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import umap.umap_ as umap\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- ì „ì—­ ì‹œë“œ ì„¤ì • (ì¬í˜„ì„± í™•ë³´) ---\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --- Matplotlib í•œê¸€ í°íŠ¸ (ì„ íƒ) ---\n",
        "!sudo apt-get -y install fonts-nanum > /dev/null\n",
        "!sudo fc-cache -fv > /dev/null\n",
        "import matplotlib.font_manager as fm\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "if os.path.exists(font_path):\n",
        "    fm.fontManager.addfont(font_path)\n",
        "    plt.rc('font', family='NanumGothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    print(\"\\nMatplotlib í°íŠ¸ ì„¤ì • ì™„ë£Œ: NanumGothic\")\n",
        "else:\n",
        "    print(\"\\nê²½ê³ : ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# --- ì „ì—­ ìƒìˆ˜ ì •ì˜ ---\n",
        "print(\"\\nì „ì—­ ìƒìˆ˜ ì •ì˜ ì¤‘...\")\n",
        "YAMNET_SAMPLE_RATE = 16000\n",
        "DEEPSHIP_BASE_PATH = '/content/DeepShip'\n",
        "MBARI_NOISE_BASE_DIR = '/content/MBARI_noise_data'\n",
        "CANDIDATE_DIR = '/content/review_candidates'   # í›„ë³´ ì˜¤ë””ì˜¤/ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ\n",
        "VERIFIED_DIR  = '/content/verified_ships'      # ì‚¬ìš©ìê°€ ì§ì ‘ ì˜®ê¸¸ ê²½ë¡œ\n",
        "MODELS_TO_PROCESS = ['YAMNet']\n",
        "print(\"ì „ì—­ ìƒìˆ˜ ì •ì˜ ì™„ë£Œ.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 2. ë°ì´í„° í™•ë³´ (DeepShip í´ë¡  + MBARI ì¼ë¶€ ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ)\n",
        "# ==============================================================================\n",
        "print(\"\\n2. ë°ì´í„° í™•ë³´...\")\n",
        "\n",
        "# DeepShip\n",
        "if not os.path.exists(DEEPSHIP_BASE_PATH):\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            ['git', 'clone', '--depth', '1',\n",
        "             'https://github.com/irfankamboh/DeepShip.git', DEEPSHIP_BASE_PATH],\n",
        "            check=True, capture_output=True\n",
        "        )\n",
        "        print(\"DeepShip í´ë¡  ì™„ë£Œ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜: DeepShip í´ë¡  ì‹¤íŒ¨: {e}\")\n",
        "else:\n",
        "    print(\"DeepShipì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# MBARI (ìƒ˜í”Œ 10ê°œ)\n",
        "os.makedirs(MBARI_NOISE_BASE_DIR, exist_ok=True)\n",
        "if os.listdir(MBARI_NOISE_BASE_DIR):\n",
        "    print(\"MBARI ë…¸ì´ì¦ˆ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(\"MBARI ë…¸ì´ì¦ˆ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘...\")\n",
        "    try:\n",
        "        s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "        pages = s3.get_paginator('list_objects_v2').paginate(\n",
        "            Bucket='pacific-sound-16khz', Prefix='2018/01/'\n",
        "        )\n",
        "        dl_count, MAX_DL = 0, 10\n",
        "        for page in pages:\n",
        "            for obj in page.get('Contents', []):\n",
        "                if obj['Key'].endswith('.wav') and obj.get('Size', 0) > 0:\n",
        "                    local_path = os.path.join(MBARI_NOISE_BASE_DIR, os.path.basename(obj['Key']))\n",
        "                    if not os.path.exists(local_path):\n",
        "                        s3.download_file('pacific-sound-16khz', obj['Key'], local_path)\n",
        "                        dl_count += 1\n",
        "                if dl_count >= MAX_DL:\n",
        "                    break\n",
        "            if dl_count >= MAX_DL:\n",
        "                break\n",
        "        print(f\"MBARI ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. (íŒŒì¼ ìˆ˜: {dl_count})\")\n",
        "    except Exception as e:\n",
        "        print(f\"ì˜¤ë¥˜: MBARI ë…¸ì´ì¦ˆ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "print(\"2. ë°ì´í„° í™•ë³´ ë‹¨ê³„ ì™„ë£Œ.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 3. ì„¸ê·¸ë¨¼í…Œì´ì…˜ (í™œë™/ë¹„í™œë™ ì´ˆ ë‹¨ìœ„) + ë°ì´í„° ë¡œë”\n",
        "# ==============================================================================\n",
        "\n",
        "def get_activity_intervals(file_path, target_sr, top_db=25):\n",
        "    \"\"\"í™œë™/ë¹„í™œë™ êµ¬ê°„ì„ 'ì´ˆ ë‹¨ìœ„' íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=target_sr)\n",
        "        intervals_samples = librosa.effects.split(\n",
        "            y, top_db=top_db, frame_length=2048, hop_length=512\n",
        "        )\n",
        "        active = [(s / target_sr, e / target_sr) for s, e in intervals_samples]\n",
        "        inactive = []\n",
        "        last_end, duration = 0.0, len(y) / target_sr\n",
        "        for s_sec, e_sec in active:\n",
        "            if s_sec > last_end:\n",
        "                inactive.append((last_end, s_sec))\n",
        "            last_end = e_sec\n",
        "        if last_end < duration:\n",
        "            inactive.append((last_end, duration))\n",
        "        return active, inactive\n",
        "    except Exception:\n",
        "        return [], []\n",
        "\n",
        "def load_and_segment_data_final(\n",
        "    ship_paths, noise_paths, verified_ship_path,\n",
        "    segment_duration=5.0, segment_overlap=0.5, undersample=True\n",
        "):\n",
        "    \"\"\"DeepShip + (ì„ íƒ)ê²€ì¦ì„ ë°• + MBARI ë…¸ì´ì¦ˆ â†’ (file,start,sr), label ë¦¬ìŠ¤íŠ¸ ìƒì„±.\"\"\"\n",
        "    hop_length = segment_duration * (1 - segment_overlap)\n",
        "    ship_segments, noise_segments = [], []\n",
        "\n",
        "    # âœ… None-safe ì²˜ë¦¬\n",
        "    all_ship_folders = list(ship_paths)\n",
        "    if verified_ship_path and os.path.exists(verified_ship_path):\n",
        "        all_ship_folders.append(verified_ship_path)\n",
        "\n",
        "    # Ship í´ë”ë“¤: í™œë™ êµ¬ê°„=ship, ë¹„í™œë™ êµ¬ê°„=noise\n",
        "    for folder_path in all_ship_folders:\n",
        "        print(f\"'ship' í´ë˜ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘: {folder_path}\")\n",
        "        if not os.path.exists(folder_path):\n",
        "            continue\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for fn in sorted([f for f in files if f.endswith('.wav')]):\n",
        "                fp = os.path.join(root, fn)\n",
        "                try:\n",
        "                    info = sf.info(fp)\n",
        "                except:\n",
        "                    continue\n",
        "                active, inactive = get_activity_intervals(fp, YAMNET_SAMPLE_RATE)\n",
        "                # í™œë™ êµ¬ê°„ -> ship\n",
        "                for s_sec, e_sec in active:\n",
        "                    if e_sec - s_sec >= segment_duration:\n",
        "                        for seg_start in np.arange(s_sec, e_sec - segment_duration + 1e-9, hop_length):\n",
        "                            ship_segments.append(((fp, float(seg_start), info.samplerate), 'ship'))\n",
        "                # ë¹„í™œë™ êµ¬ê°„ -> noise\n",
        "                for s_sec, e_sec in inactive:\n",
        "                    if e_sec - s_sec >= segment_duration:\n",
        "                        for seg_start in np.arange(s_sec, e_sec - segment_duration + 1e-9, hop_length):\n",
        "                            noise_segments.append(((fp, float(seg_start), info.samplerate), 'noise'))\n",
        "\n",
        "    # Noise í´ë”ë“¤: ì „êµ¬ê°„=noise\n",
        "    for folder_path in noise_paths:\n",
        "        print(f\"'noise' í´ë˜ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘: {folder_path}\")\n",
        "        if not os.path.exists(folder_path):\n",
        "            continue\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for fn in sorted([f for f in files if f.endswith('.wav')]):\n",
        "                fp = os.path.join(root, fn)\n",
        "                try:\n",
        "                    info = sf.info(fp)\n",
        "                except:\n",
        "                    continue\n",
        "                dur = info.duration\n",
        "                if dur < segment_duration:\n",
        "                    continue\n",
        "                for seg_start in np.arange(0, dur - segment_duration + 1e-9, hop_length):\n",
        "                    noise_segments.append(((fp, float(seg_start), info.samplerate), 'noise'))\n",
        "\n",
        "    print(f\"  ì´ 'ship' ì„¸ê·¸ë¨¼íŠ¸: {len(ship_segments)}ê°œ, ì´ 'noise' ì„¸ê·¸ë¨¼íŠ¸: {len(noise_segments)}ê°œ\")\n",
        "    all_data = ship_segments + noise_segments\n",
        "\n",
        "    if undersample and ship_segments and noise_segments:\n",
        "        k = min(len(ship_segments), len(noise_segments))\n",
        "        print(f\"\\ní´ë˜ìŠ¤ ë¶ˆê· í˜• -> ì–¸ë”ìƒ˜í”Œë§({k}ê°œ) ìˆ˜í–‰.\")\n",
        "        all_data = random.sample(ship_segments, k) + random.sample(noise_segments, k)\n",
        "        random.shuffle(all_data)\n",
        "\n",
        "    if not all_data:\n",
        "        return [], [], False\n",
        "    infos, labels = zip(*all_data)\n",
        "    print(\"\\në°ì´í„° ë¡œë“œ ë° ì„¸ê·¸ë¨¼í…Œì´ì…˜ ì™„ë£Œ.\")\n",
        "    return list(infos), list(labels), True\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 4. ì „ì²˜ë¦¬/ì„ë² ë”©/ëª¨ë¸/ìœ í‹¸ í•¨ìˆ˜\n",
        "# ==============================================================================\n",
        "\n",
        "def mix_at_snr(clean, noise, snr_db):\n",
        "    L = min(len(clean), len(noise))\n",
        "    c, n = clean[:L].astype(np.float32), noise[:L].astype(np.float32)\n",
        "    c_power = np.sqrt(np.mean(c**2)); n_power = np.sqrt(np.mean(n**2))\n",
        "    if n_power < 1e-8:\n",
        "        return c\n",
        "    alpha = c_power / (n_power * (10 ** (snr_db / 20)))\n",
        "    return c + alpha * n\n",
        "\n",
        "def load_and_process_segment_efficient(file_info, duration, target_sr, config):\n",
        "    file_path, start_time, orig_sr = file_info\n",
        "    try:\n",
        "        start_frame = int(start_time * orig_sr)\n",
        "        num_frames  = int(duration * orig_sr)\n",
        "        y_segment, _ = sf.read(\n",
        "            file_path, start=start_frame, stop=start_frame + num_frames,\n",
        "            dtype='float32', always_2d=False\n",
        "        )\n",
        "        if y_segment.ndim > 1:\n",
        "            y_segment = np.mean(y_segment, axis=1)\n",
        "        if orig_sr != target_sr:\n",
        "            y_segment = librosa.resample(y=y_segment, orig_sr=orig_sr, target_sr=target_sr)\n",
        "        if config.get(\"apply_noise_reduction\", False):\n",
        "            y_segment = nr.reduce_noise(y=y_segment, sr=target_sr)\n",
        "        if config.get(\"apply_rms_norm\", True):\n",
        "            rms = np.sqrt(np.mean(y_segment ** 2))\n",
        "            if rms > 1e-6:\n",
        "                y_segment = y_segment * (10.0 ** (-20.0 / 20.0) / rms)\n",
        "        return y_segment\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_yamnet_embedding(audio_info, model, config, noise_audio_infos=None, augment=False):\n",
        "    try:\n",
        "        y_segment = load_and_process_segment_efficient(\n",
        "            audio_info, config[\"segment_duration\"], YAMNET_SAMPLE_RATE, config\n",
        "        )\n",
        "        if y_segment is None:\n",
        "            return None\n",
        "        if augment and noise_audio_infos:\n",
        "            noise_info = random.choice(noise_audio_infos)\n",
        "            y_noise = load_and_process_segment_efficient(\n",
        "                noise_info, config[\"segment_duration\"], YAMNET_SAMPLE_RATE,\n",
        "                {\"apply_noise_reduction\": False, \"apply_rms_norm\": True}\n",
        "            )\n",
        "            if y_noise is not None:\n",
        "                if config.get(\"use_snr_augmentation\", False):\n",
        "                    snr_db = random.uniform(config[\"snr_min_db\"], config[\"snr_max_db\"])\n",
        "                    y_segment = mix_at_snr(y_segment, y_noise, snr_db)\n",
        "                else:\n",
        "                    L = min(len(y_segment), len(y_noise))\n",
        "                    y_segment = y_segment[:L] + config.get(\"noise_level\", 0.05) * y_noise[:L]\n",
        "        _, embeddings, _ = model(y_segment)\n",
        "        emb = tf.reduce_mean(embeddings, axis=0).numpy() if embeddings.shape[0] > 0 else None\n",
        "        return emb\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def load_audio_models():\n",
        "    models = {}\n",
        "    print(\"\\nì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "    try:\n",
        "        models['YAMNet'] = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "        print(\"  YAMNet ëª¨ë¸ ë¡œë“œ: ì„±ê³µ\")\n",
        "        return models, True\n",
        "    except Exception as e:\n",
        "        print(f\"  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        return {}, False\n",
        "\n",
        "def build_classifier_model(input_shape, num_classes, learning_rate):\n",
        "    inp = Input(shape=(input_shape,), name='embedding_input')\n",
        "    x = Dense(256, activation='relu')(inp); x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x); x = Dropout(0.5)(x)\n",
        "    out = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def embed_infos(infos, labels, model_hub, config, label_encoder,\n",
        "                noise_infos_for_train=None, augment_ship_only=True):\n",
        "    \"\"\"ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ â†’ (X, y onehot, infos_kept) ì„ë² ë”©.\"\"\"\n",
        "    embs, labs, kept_infos = [], [], []\n",
        "    for info, lab in zip(infos, labels):\n",
        "        is_ship = (label_encoder.inverse_transform([lab])[0] == 'ship')\n",
        "        use_aug = bool(noise_infos_for_train) and (augment_ship_only and is_ship)\n",
        "        noise_src = noise_infos_for_train if use_aug else None\n",
        "        emb = extract_yamnet_embedding(info, model_hub, config,\n",
        "                                       noise_audio_infos=noise_src, augment=use_aug)\n",
        "        if emb is not None:\n",
        "            embs.append(emb); labs.append(lab); kept_infos.append(info)\n",
        "    if not embs:\n",
        "        return np.array([]), np.array([]), []\n",
        "    X = np.asarray(embs, dtype=np.float32)\n",
        "    y = tf.keras.utils.to_categorical(np.asarray(labs), num_classes=len(label_encoder.classes_))\n",
        "    return X, y, kept_infos\n",
        "\n",
        "def summarize_metrics(y_true_onehot, y_prob, label_encoder, title=\"\"):\n",
        "    \"\"\"ì •í™•ë„, Macro-F1, AUC(ship)ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥. AUCëŠ” ê°€ë“œ ì²˜ë¦¬.\"\"\"\n",
        "    y_true = y_true_onehot.argmax(axis=1)\n",
        "    y_pred = y_prob.argmax(axis=1)\n",
        "    acc  = (y_pred == y_true).mean()\n",
        "    f1m  = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    try:\n",
        "        ship_idx = list(label_encoder.classes_).index('ship')\n",
        "        y_true_bin = (y_true == ship_idx).astype(int)\n",
        "        if len(np.unique(y_true_bin)) < 2:\n",
        "            auc_score = float('nan')\n",
        "        else:\n",
        "            auc_score = roc_auc_score(y_true_bin, y_prob[:, ship_idx])\n",
        "    except Exception:\n",
        "        auc_score = float('nan')\n",
        "\n",
        "    print(f\"{title}Acc: {acc:.4f}, Macro-F1: {f1m:.4f}, AUC(ship): {auc_score:.4f}\")\n",
        "    return acc, f1m, auc_score\n",
        "\n",
        "def plot_confusion(y_true_onehot, y_prob, label_encoder, title=\"Confusion Matrix\"):\n",
        "    y_true = y_true_onehot.argmax(axis=1)\n",
        "    y_pred = y_prob.argmax(axis=1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d',\n",
        "                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,\n",
        "                cmap='Blues')\n",
        "    plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”'); plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”'); plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def generate_candidate_report_zip(candidate_dir, candidates, sr=YAMNET_SAMPLE_RATE):\n",
        "    \"\"\"candidates: list of dict({ 'wav':np.array, 'name':str })\"\"\"\n",
        "    os.makedirs(candidate_dir, exist_ok=True)\n",
        "    html_path = os.path.join(candidate_dir, \"candidate_report.html\")\n",
        "    report = [\"<h1>ì„ ë°• ì†Œë¦¬ í›„ë³´ (ê²€ì¦ í•„ìš”)</h1><hr>\"]\n",
        "\n",
        "    for item in candidates:\n",
        "        out_wav_path = os.path.join(candidate_dir, item['name'])\n",
        "        sf.write(out_wav_path, item['wav'], sr)\n",
        "        # HTMLì—ì„œ ìƒëŒ€ê²½ë¡œë¡œ ì ‘ê·¼ (Colab/ë¡œì»¬ ë‹¤ìš´ë¡œë“œ í›„ì—ë„ ì¬ìƒ ê°€ëŠ¥)\n",
        "        report += [\n",
        "            f\"<h3>{item['name']}</h3>\",\n",
        "            f\"<p>Ship Probability: {item['ship_prob']:.2%}</p>\",\n",
        "            f\"<audio controls src='{item['name']}'></audio><br>\"\n",
        "        ]\n",
        "    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(report))\n",
        "\n",
        "    bundle_path = \"/content/review_candidates_bundle\"\n",
        "    shutil.make_archive(bundle_path, 'zip', candidate_dir)\n",
        "    print(f\"\\n[ì‘ì—… ìš”ì²­] '{bundle_path}.zip' íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì••ì¶•ì„ í’€ê³ , HTML ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    print(f\"ê²€ì¦ëœ íŒŒì¼ì„ '{VERIFIED_DIR}' í´ë”ë¡œ ì˜®ê¸°ê±°ë‚˜, ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ íŒŒì¼ëª…ì„ ì§ì ‘ ì…ë ¥í•´ë„ ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "def parse_verified_filenames_to_infos(verified_filenames, base_dir):\n",
        "    \"\"\"í›„ë³´ íŒŒì¼ëª… â†’ (ì›ë³¸ê²½ë¡œ, ì‹œì‘ì´ˆ, sr) ë³µì›.\"\"\"\n",
        "    out_infos = []\n",
        "    for fname in verified_filenames:\n",
        "        try:\n",
        "            base = fname.rsplit('.', 1)[0]\n",
        "            parts = base.split('__')\n",
        "            original_base = parts[0]\n",
        "            start_sec = float(parts[1].split('s')[0])\n",
        "\n",
        "            original_path = None\n",
        "            for root, _, files in os.walk(base_dir):\n",
        "                if f\"{original_base}.wav\" in files:\n",
        "                    original_path = os.path.join(root, f\"{original_base}.wav\")\n",
        "                    break\n",
        "            if original_path:\n",
        "                info = sf.info(original_path)\n",
        "                out_infos.append(((original_path, start_sec, info.samplerate), 'ship'))\n",
        "            else:\n",
        "                print(f\"[ê²½ê³ ] ì›ë³¸ íŒŒì¼ íƒìƒ‰ ì‹¤íŒ¨: {original_base}.wav\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ê²½ê³ ] ê²€ì¦ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {fname} ({e})\")\n",
        "    return out_infos\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 5. ë©”ì¸ íŒŒì´í”„ë¼ì¸ 1ë‹¨ê³„: ìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸ í•™ìŠµ & í›„ë³´ íƒìƒ‰\n",
        "# ==============================================================================\n",
        "CONFIG_SCOUT = {\"segment_duration\": 5.0, \"segment_overlap\": 0.5, \"undersample\": True, \"apply_noise_reduction\": False, \"apply_rms_norm\": True, \"use_snr_augmentation\": True, \"snr_min_db\": 0, \"snr_max_db\": 15, \"noise_level\": 0.05, \"test_size\": 0.2, \"epochs\": 20, \"batch_size\": 32, \"learning_rate\": 0.0005, \"top_k_candidates\": 50, \"candidate_stride\": 5.0, \"candidate_batch\": 32, \"max_mbari_files\": 10}\n",
        "\n",
        "print(\"\\n>>> [1ë‹¨ê³„] ìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸ í•™ìŠµ ë° í›„ë³´ íƒìƒ‰ ì‹œì‘...\")\n",
        "\n",
        "scout_infos, scout_labels, scout_ok = load_and_segment_data_final(ship_paths=[DEEPSHIP_BASE_PATH], noise_paths=[MBARI_NOISE_BASE_DIR], verified_ship_path=None, **{k:v for k,v in CONFIG_SCOUT.items() if k in ['segment_duration','segment_overlap','undersample']})\n",
        "scout_model = None\n",
        "if scout_ok:\n",
        "    le_scout = LabelEncoder(); y_all = le_scout.fit_transform(scout_labels); groups = [info[0] for info in scout_infos]\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG_SCOUT[\"test_size\"], random_state=SEED)\n",
        "    tr_idx, te_idx = next(gss.split(scout_infos, y_all, groups))\n",
        "    Xtr_infos = [scout_infos[i] for i in tr_idx]; ytr_enc = y_all[tr_idx]\n",
        "    Xte_infos = [scout_infos[i] for i in te_idx]; yte_enc = y_all[te_idx]\n",
        "\n",
        "    models_hub, ok = load_audio_models()\n",
        "    if ok:\n",
        "        print(\"\\nì´ˆê¸° ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "        noise_tr = [info for info, lab in zip(Xtr_infos, ytr_enc) if le_scout.inverse_transform([lab])[0] == 'noise']\n",
        "        Xtr, ytr, _ = embed_infos(Xtr_infos, ytr_enc, models_hub['YAMNet'], CONFIG_SCOUT, le_scout, noise_infos_for_train=noise_tr)\n",
        "        Xte, yte, _ = embed_infos(Xte_infos, yte_enc, models_hub['YAMNet'], CONFIG_SCOUT, le_scout)\n",
        "        if len(Xtr) == 0 or len(Xte) == 0: print(\"ì˜¤ë¥˜: ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨(í•™ìŠµ/í‰ê°€ ìƒ˜í”Œì´ ì—†ìŒ).\"); scout_model = None\n",
        "        else:\n",
        "            input_dim = Xtr.shape[-1]; num_classes = ytr.shape[-1]\n",
        "            clf = build_classifier_model(input_dim, num_classes, CONFIG_SCOUT[\"learning_rate\"])\n",
        "            hist = clf.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=CONFIG_SCOUT[\"epochs\"], batch_size=CONFIG_SCOUT[\"batch_size\"], callbacks=[EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=3)], verbose=1)\n",
        "            scout_model = clf\n",
        "            prob_te = scout_model.predict(Xte, verbose=0); summarize_metrics(yte, prob_te, le_scout, title=\"[ìŠ¤ì¹´ìš°íŠ¸] \")\n",
        "\n",
        "def stream_topk_candidates(scout_model, yamnet_model, label_encoder, mbari_dir, config):\n",
        "    ship_idx = list(label_encoder.classes_).index('ship'); top_k = int(config.get(\"top_k_candidates\", 50))\n",
        "    stride = float(config.get(\"candidate_stride\", config[\"segment_duration\"])); batch_size = int(config.get(\"candidate_batch\", 32))\n",
        "    max_files = int(config.get(\"max_mbari_files\", 1000000)); heap = []; batch_embs, batch_infos = [], []\n",
        "\n",
        "    def flush_batch():\n",
        "        nonlocal heap\n",
        "        if not batch_embs: return\n",
        "        probs = scout_model.predict(np.asarray(batch_embs, dtype=np.float32), verbose=0)\n",
        "        for p, info in zip(probs, batch_infos):\n",
        "            prob_ship = float(p[ship_idx])\n",
        "            if len(heap) < top_k: heapq.heappush(heap, (prob_ship, info))\n",
        "            elif prob_ship > heap[0][0]: heapq.heapreplace(heap, (prob_ship, info))\n",
        "        batch_embs.clear(); batch_infos.clear(); gc.collect()\n",
        "\n",
        "    mbari_files = sorted([f for f in os.listdir(mbari_dir) if f.endswith('.wav')])[:max_files]\n",
        "    print(f\"  ì´ {len(mbari_files)}ê°œì˜ MBARI íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
        "    for fi, fn in enumerate(mbari_files, 1):\n",
        "        fp = os.path.join(mbari_dir, fn)\n",
        "        try: info = sf.info(fp)\n",
        "        except: continue\n",
        "\n",
        "        starts = np.arange(0, info.duration - config[\"segment_duration\"] + 1e-9, stride)\n",
        "\n",
        "        for seg_start in starts:\n",
        "            seg_info = (fp, float(seg_start), info.samplerate)\n",
        "            emb = extract_yamnet_embedding(seg_info, yamnet_model, config, augment=False)\n",
        "            if emb is None: continue\n",
        "            batch_embs.append(emb); batch_infos.append(seg_info)\n",
        "            if len(batch_embs) >= batch_size: flush_batch()\n",
        "        flush_batch()\n",
        "        if fi % 5 == 0: print(f\"  ì§„í–‰ ìƒí™©: {fi}/{len(mbari_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œâ€¦\"); gc.collect()\n",
        "\n",
        "    heap.sort(reverse=True)\n",
        "    return heap\n",
        "\n",
        "if scout_model:\n",
        "    print(\"\\n>>> MBARI í›„ë³´ íƒìƒ‰ (RAM ì ˆì•½ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)â€¦\")\n",
        "    topk_heap = stream_topk_candidates(scout_model, models_hub['YAMNet'], le_scout, MBARI_NOISE_BASE_DIR, CONFIG_SCOUT)\n",
        "    print(f\"  Top-{len(topk_heap)} í›„ë³´ í™•ë³´.\")\n",
        "    cand_items = []\n",
        "    for prob, info in topk_heap:\n",
        "        yseg = load_and_process_segment_efficient(info, CONFIG_SCOUT[\"segment_duration\"], YAMNET_SAMPLE_RATE, CONFIG_SCOUT)\n",
        "        if yseg is None: continue\n",
        "        base_name = f\"{os.path.basename(info[0]).replace('.wav','')}__{info[1]:.2f}s_prob{prob:.2f}.wav\"\n",
        "        cand_items.append({'name': base_name, 'wav': yseg, 'ship_prob': prob})\n",
        "    generate_candidate_report_zip(CANDIDATE_DIR, cand_items, sr=YAMNET_SAMPLE_RATE)\n",
        "    os.makedirs(VERIFIED_DIR, exist_ok=True)\n",
        "else:\n",
        "    print(\"\\nìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸ì´ ì—†ì–´ í›„ë³´ íƒìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ## 6. ë©”ì¸ íŒŒì´í”„ë¼ì¸ 2ë‹¨ê³„: ê²€ì¦ëœ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"## 6. ìµœì¢… ëª¨ë¸ í•™ìŠµ ë‹¨ê³„\")\n",
        "print(\"  - 1ë‹¨ê³„ì—ì„œ ìƒì„±ëœ 'review_candidates_bundle.zip' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "print(f\"  - ì„ ë°• ì†ŒìŒì´ í™•ì‹¤í•œ WAV íŒŒì¼ë“¤ì„ Colabì˜ '{VERIFIED_DIR}' í´ë”ë¡œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
        "print(\"  - ì¤€ë¹„ê°€ ë˜ë©´ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not os.path.exists(VERIFIED_DIR) or not any(f.endswith('.wav') for f in os.listdir(VERIFIED_DIR)):\n",
        "    print(f\"\\nê²½ê³ : '{VERIFIED_DIR}' í´ë”ì— ê²€ì¦ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ì¦ ë°ì´í„° ì—†ì´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "CONFIG_FINAL = CONFIG_SCOUT.copy(); CONFIG_FINAL['epochs'] = 50\n",
        "print(\"\\n>>> [2ë‹¨ê³„] ìµœì¢… ë°ì´í„°ì…‹ ë¡œë“œ...\")\n",
        "final_infos, final_labels, ok = load_and_segment_data_final(\n",
        "    ship_paths=[DEEPSHIP_BASE_PATH],\n",
        "    noise_paths=[MBARI_NOISE_BASE_DIR],\n",
        "    verified_ship_path=VERIFIED_DIR,\n",
        "    **{k: v for k, v in CONFIG_FINAL.items() if k in ['segment_duration', 'segment_overlap', 'undersample']}\n",
        ")\n",
        "\n",
        "if ok:\n",
        "    le_final = LabelEncoder(); y_all = le_final.fit_transform(final_labels)\n",
        "    groups = [info[0] for info in final_infos]\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG_FINAL[\"test_size\"], random_state=SEED)\n",
        "    tr_idx, te_idx = next(gss.split(final_infos, y_all, groups))\n",
        "    Xtr_infos = [final_infos[i] for i in tr_idx]; ytr_enc = y_all[tr_idx]\n",
        "    Xte_infos = [final_infos[i] for i in te_idx]; yte_enc = y_all[te_idx]\n",
        "\n",
        "    models_final, ok2 = load_audio_models()\n",
        "    if ok2:\n",
        "        noise_tr = [info for info, lab in zip(Xtr_infos, ytr_enc) if le_final.inverse_transform([lab])[0]=='noise']\n",
        "        Xtr, ytr, _ = embed_infos(Xtr_infos, ytr_enc, models_final['YAMNet'], CONFIG_FINAL, le_final, noise_infos_for_train=noise_tr)\n",
        "        Xte, yte, _ = embed_infos(Xte_infos, yte_enc, models_final['YAMNet'], CONFIG_FINAL, le_final)\n",
        "        if len(Xtr) == 0 or len(Xte) == 0: print(\"ìµœì¢… ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨\")\n",
        "        else:\n",
        "            input_dim = Xtr.shape[-1]; num_classes = ytr.shape[-1]\n",
        "            clf = build_classifier_model(input_dim, num_classes, CONFIG_FINAL[\"learning_rate\"])\n",
        "            hist = clf.fit(Xtr, ytr, validation_data=(Xte, yte), epochs=CONFIG_FINAL[\"epochs\"], batch_size=CONFIG_FINAL[\"batch_size\"],\n",
        "                           callbacks=[EarlyStopping(patience=8, restore_best_weights=True), ReduceLROnPlateau(patience=4)], verbose=1)\n",
        "            probs = clf.predict(Xte, verbose=0)\n",
        "            summarize_metrics(yte, probs, le_final, title=\"[ìµœì¢…] \")\n",
        "            plot_confusion(yte, probs, le_final, title=\"ìµœì¢… ëª¨ë¸ í˜¼ë™ í–‰ë ¬\")\n",
        "    else: print(\"ìµœì¢… ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\")\n",
        "else: print(\"ìµœì¢… ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨\")\n",
        "\n",
        "print(\"\\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "id": "1-ZIkHcXGlzx",
        "collapsed": true,
        "outputId": "c63b44c8-0436-4eb9-e6f2-7ac49493db57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì¤‘...\n",
            "\n",
            "Matplotlib í°íŠ¸ ì„¤ì • ì™„ë£Œ: NanumGothic\n",
            "\n",
            "ì „ì—­ ìƒìˆ˜ ì •ì˜ ì¤‘...\n",
            "ì „ì—­ ìƒìˆ˜ ì •ì˜ ì™„ë£Œ.\n",
            "\n",
            "2. ë°ì´í„° í™•ë³´...\n",
            "DeepShipì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
            "MBARI ë…¸ì´ì¦ˆ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
            "2. ë°ì´í„° í™•ë³´ ë‹¨ê³„ ì™„ë£Œ.\n",
            "\n",
            ">>> [1ë‹¨ê³„] ìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸ í•™ìŠµ ë° í›„ë³´ íƒìƒ‰ ì‹œì‘...\n",
            "'ship' í´ë˜ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘: /content/DeepShip\n",
            "'noise' í´ë˜ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘: /content/MBARI_noise_data\n",
            "  ì´ 'ship' ì„¸ê·¸ë¨¼íŠ¸: 2201ê°œ, ì´ 'noise' ì„¸ê·¸ë¨¼íŠ¸: 345590ê°œ\n",
            "\n",
            "í´ë˜ìŠ¤ ë¶ˆê· í˜• -> ì–¸ë”ìƒ˜í”Œë§(2201ê°œ) ìˆ˜í–‰.\n",
            "\n",
            "ë°ì´í„° ë¡œë“œ ë° ì„¸ê·¸ë¨¼í…Œì´ì…˜ ì™„ë£Œ.\n",
            "\n",
            "ì˜¤ë””ì˜¤ ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
            "  YAMNet ëª¨ë¸ ë¡œë“œ: ì„±ê³µ\n",
            "\n",
            "ì´ˆê¸° ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n"
          ]
        }
      ]
    }
  ]
}